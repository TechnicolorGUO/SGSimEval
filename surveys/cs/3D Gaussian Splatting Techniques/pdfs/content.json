[
    "# A Survey on 3D Gaussian Splatting  \n\nGuikun Chen, and Wenguan Wang, Senior Member, IEEE  \n\nAbstract—3D Gaussian splatting (GS) has emerged as a transformative technique in explicit radiance field and computer graphics. This innovative approach, characterized by the use of millions of learnable 3D Gaussians, represents a significant departure from mainstream neural radiance field approaches, which predominantly use implicit, coordinate-based models to map spatial coordinates to pixel values. 3D GS, with its explicit scene representation and differentiable rendering algorithm, not only promises real-time rendering capability but also introduces unprecedented levels of editability. This positions 3D GS as a potential game-changer for the next generation of 3D reconstruction and representation. In the present paper, we provide the first systematic overview of the recent developments and critical contributions in the domain of 3D GS. We begin with a detailed exploration of the underlying principles and the driving forces behind the emergence of 3D GS, laying the groundwork for understanding its significance. A focal point of our discussion is the practical applicability of 3D GS. By enabling unprecedented rendering speed, 3D GS opens up a plethora of applications, ranging from virtual reality to interactive media and beyond. This is complemented by a comparative analysis of leading 3D GS models, evaluated across various benchmark tasks to highlight their performance and practical utility. The survey concludes by identifying current challenges and suggesting potential avenues for future research. Through this survey, we aim to provide a valuable resource for both newcomers and seasoned researchers, fostering further exploration and advancement in explicit radiance field.  \n\nIndex Terms—3D Gaussian Splatting, Explicit Radiance Field, Real-time Rendering, Scene Understanding  \n\n# 1 INTRODUCTION  \n\nis to convert a collection of views or videos capturing a scene into a digital 3D model that can be computationally processed, analyzed, and manipulated. This hard and long-standing problem is fundamental for machines to comprehend the complexity of real-world environments, facilitating a wide array of applications such as 3D modeling and animation, robot navigation, historical preservation, augmented/virtual reality, and autonomous driving.  \n\nThe journey of 3D scene reconstruction began long before the surge of deep learning, with early endeavors focusing on light fields and basic scene reconstruction methods [1]–[3]. These early attempts, however, were limited by their reliance on dense sampling and structured capture, leading to significant challenges in handling complex scenes and lighting conditions. The emergence of structure-frommotion [4] and subsequent advancements in multi-view stereo [5] algorithms provided a more robust framework for 3D scene reconstruction. Despite these advancements, such methods struggled with novel-view synthesis and texture loss. NeRF represents a quantum leap in this progression. By leveraging deep neural networks, NeRF enabled the direct mapping of spatial coordinates to color and density. The success of NeRF hinged on its ability to create continuous, volumetric scene functions, producing results with unprecedented fidelity. However, as with any burgeoning technology, this implementation came at a cost: i) Computational Intensity. NeRF based methods are computationally intensive [6]–[9], often requiring extensive training times and substantial resources for rendering, especially for highresolution outputs. ii) Editability. Manipulating scenes represented implicitly is challenging, since direct modifications to the neural network’s weights are not intuitively related to changes in geometric or appearance properties of the scene.  \n\n![](images/0a1f3199314e3adf3d31d868ce4e053d6c6d004b7c4816b9a27cd4fbd45fb2ee.jpg)  \nFig. 1. The number of published papers and official GitHub stars on 3D GS. The set of statistics is sourced from # Papers and # GitHub Stars.  \n\nIt is in this context that 3D Gaussian splatting (GS) [10] emerges, not merely as an incremental improvement but as a paradigm-shifting approach that redefines the boundaries of scene representation and rendering. While NeRF excelled in creating photorealistic images, the need for faster, more efficient rendering methods was becoming increasingly apparent, especially for applications (e.g., virtual reality and autonomous driving) that are highly sensitive to latency. 3D GS addressed this need by introducing an advanced, explicit scene representation that models a scene using millions of learnable 3D Gaussians in space. Unlike the implicit, coordinate-based models [11], [12], 3D GS employs an explicit representation and highly parallelized workflows, facilitating more efficient computation and rendering. The innovation of 3D GS lies in its unique blend of the benefits of differentiable pipelines and point-based rendering techniques [13]–[17]. By representing scenes with learnable 3D Gaussians, it preserves the strong fitting capability of continuous volumetric radiance fields, essential for highquality image synthesis, while simultaneously avoiding the computational overhead associated with NeRF based methods (e.g., computationally expensive ray-marching, and unnecessary calculations in empty space).  \n\nThe introduction of 3D GS is not just a technical advancement; it represents a fundamental shift in how we approach scene representation and rendering in computer vision and graphics. By enabling real-time rendering capabilities without compromising on visual quality, 3D GS opens up a plethora of possibilities for applications ranging from virtual reality and augmented reality to real-time cinematic rendering and beyond [18]–[21]. This technology holds the promise of not only enhancing existing applications but also enabling new ones that were previously unfeasible due to computational constraints. Furthermore, 3D GS’s explicit scene representation offers unprecedented flexibility to control the objects and scene dynamics, a crucial factor in complex scenarios involving intricate geometries and varying lighting conditions [22]–[24]. This level of editability, combined with the efficiency of the training and rendering process, positions 3D GS as a transformative force in shaping future developments in relevant fields.  \n\nIn an effort to assist readers in keeping pace with the swift evolution of 3D GS, we provide the first survey on 3D GS, which presents a systematic and timely collection of the most significant literature on the topic. Given that 3D GS is a very recent innovation (Fig. 1), this survey focuses in particular on its principles, and the diverse developments and contributions that have emerged since its introduction. The selected follow-up works are primarily sourced from top-tier conferences, to provide a thorough and up-to-date (Dec. 2024) analysis of the theoretical foundations, remarkable developments, and burgeoning applications of 3D GS. Acknowledging the nascent yet rapidly evolving nature of 3D GS, this survey is inevitably a biased view, but we strive to offer a balanced perspective that reflects both the current state and the future potential of this field. Our aim is to encapsulate the primary research trends and serve as a valuable resource for both researchers and practitioners eager to understand and contribute to this rapidly evolving domain. The distinctions of this survey from existing literature [25]– [28] are evident in the following aspects:  \n\nWe provide the first systematic and comprehensive review that examines 3D GS from a macro-level perspective by establishing clear taxonomies and frameworks. This highlevel systematization helps researchers identify trends and potential directions that might not be apparent from paperspecific reviews. Our organizational structure serves as a roadmap for understanding how different approaches relate to and build upon each other within the 3D GS ecosystem.  \n\nThis paper is the first and only survey to thoroughly delve into the theoretical background and fundamental principles of 3D GS. The comprehensive coverage makes the field more approachable for newcomers while providing valuable insights for experienced researchers.  \n\n![](images/a18511a0ce11d4408be07986aa5dcaf8636b50238c7f0144df28043eff3d6e9d.jpg)  \nFig. 2. Structure of the overall review.  \n\n• To ensure our survey remains relevant and offer longterm value in this rapidly evolving field, we maintain two dynamic GitHub repositories: one that follows our survey’s organizational structure and another that includes comprehensive performance comparisons with analysis data.  \n\nA summary of the structure of this article can be found in Fig. 2, which is presented as follows: Sec. 2 provides a brief background on problem formulation, terminology, and related research domains. Sec. 3 introduces the essential insights of 3D GS, encompassing the rendering process with learned 3D Gaussians and the optimization details (i.e., how to learn 3D Gaussians) of 3D GS. Sec. 4 presents several fruitful directions that aim to improve the capabilities of the original 3D GS. Sec. 5 unveils the diverse application areas and tasks where 3D GS has made significant impacts, showcasing its versatility. Sec. 6 conducts performance comparison and analysis. Finally, Sec. 7 and 8 highlight the open questions for further research and conclude the survey.  \n\n# 2 BACKGROUND  \n\nIn this section, we first provide a brief formulation of radiance fields (Sec. 2.1), including both implicit and explicit ones. Sec. 2.2 further establishes linkages with relevant rendering algorithms and terminologies. For a comprehensive overview of radiance fields, scene reconstruction and representation, and rendering methods, please see the excellent surveys [29]–[33] for more insights.  \n\n# 2.1 Radiance Field  \n\nImplicit Radiance Field. An implicit radiance field represents light distribution in a scene without explicitly defining the geometry of the scene. In the deep learning era, neural networks are often used to learn a continuous volumetric scene representation [34], [35]. The most prominent example is NeRF [12]. In NeRF (Fig. 3a), one or more MLPs are used to map a set of spatial coordinates $( x , y , z )$ and viewing directions $( \\theta , \\phi )$ to color $c$ and volume density $\\sigma$ :  \n\n$$\n( c , \\sigma ) \\gets \\mathrm { M L P } ( x , y , z , \\theta , \\phi ) .\n$$  \n\nThis format allows for a differentiable and compact representation of complex scenes, albeit often at the cost of high computational load due to volumetric ray marching. Note that typically, the color $c$ is direction-dependent, whereas the volume density $\\sigma$ is not [12].  \n\n• Explicit Radiance Field. An explicit radiance field directly represents the distribution of light in a discrete spatial structure, such as a voxel grid or a set of points [36], [37]. Each element in this structure stores the radiance information for its respective location. This allows for direct and often faster access to radiance data but at the cost of higher memory usage and potentially lower resolution. Similar to the implicit radiance field, the explicit one is written as:  \n\n$$\n( c , \\sigma ) \\gets \\mathrm { D a t a S t r u c t u r e } ( x , y , z , \\theta , \\phi ) ,\n$$  \n\nwhere DataStructure could be in the format of volumes, point clouds, etc. DataStructure encodes directional color in two main ways. One is encoding high-dimensional features that are subsequently decoded by a lightweight MLP. Another one is directly storing coefficients of directional basis functions, such as spherical harmonics or spherical Gaussians, where the final color is computed as a function of these coefficients and the viewing direction.  \n\n• 3D Gaussian Splatting: Best-of-Both Worlds. 3D GS [10] is an explicit radiance field with the advantages of implicit radiance fields. Concretely, it leverages the strengths of both paradigms by utilizing learnable 3D Gaussians as the basis elements of DataStructure. Note that 3D GS encodes the opacity $\\alpha$ directly for each Gaussian, as opposed to approaches of first establishing density $\\sigma$ and then computing opacity based on that density. As in previous reconstruction work, 3D Gaussians are optimized under the supervision of multi-view images to represent the scene. Such a 3D Gaussian based differentiable pipeline combines the benefits of neural network based optimization and explicit, structured data storage. This hybrid approach aims to achieve realtime, high-quality rendering and requires less training time, particularly for complex scenes and high-resolution outputs.  \n\n# 2.2 Context and Terminology  \n\nVolumetric rendering aims to transform a 3D volumetric representation into an image by integrating radiance along camera rays. A camera ray $\\mathbf { \\boldsymbol { r } } ( t )$ can be parameterized as: ${ \\pmb r } ( t ) = { \\pmb o } + t { \\pmb d }$ , $t \\in [ t _ { \\mathrm { n e a r } } , t _ { \\mathrm { f a r } } ]$ , where $\\textbf { \\em o }$ represents the ray origin (camera center), $\\mathbf { \\Delta } _ { d }$ is the ray direction, and $t$ indicates the distance along the ray between near and far clipping planes. The pixel color $C ( \\pmb { r } )$ is computed through a line integral along the ray $\\mathbf { } r ( t ) .$ , mathematically expressed as [12]:  \n\n$$\nC ( \\pmb { r } ) = \\int _ { t _ { \\mathrm { n e a r } } } ^ { t _ { \\mathrm { f a r } } } T ( t ) \\sigma ( \\pmb { r } ( t ) ) c ( \\pmb { r } ( t ) , \\pmb { d } ) d t ,\n$$  \n\nwhere $\\sigma ( \\boldsymbol { r } ( t ) )$ is the volume density at point $\\boldsymbol { r } ( t ) , \\boldsymbol { c } ( \\boldsymbol { r } ( t ) , d )$ is the color at that point, and $T ( t )$ is the transmittance. Raymarching directly approximates the volumetric rendering integral by systematically “stepping” along a ray and sampling the scene’s properties at discrete intervals. NeRF [12] shares the same spirit of ray-marching and introduces importance sampling and positional encoding to improve the quality of synthesized images. While providing highquality results, ray-marching is computationally expensive, especially for high-resolution images.  \n\nPoint-based rendering represents another class of rendering algorithms, of which 3D GS introduces a notable implementation. Its simplest form [38] rasterizes point clouds with a fixed size, which introduces drawbacks such as holes and rendering artifacts. Seminal works addressed these limitations through various methods, including: i) splatting point primitives with a spatial extent [14], [15], [39], [40], and ii) more recently, embedding neural features directly into points for subsequent network-based rendering [41], [42]. 3D GS uses 3D Gaussian as the point primitive that contains explicit attributes (e.g., color and opacity) instead of implicit neural features. The rendering approach, i.e., pointbased $\\alpha$ -blending (exemplified in Eq. 5), shares the same image formation model as NeRF-style volumetric rendering (Eq. 3) [10], but demonstrates substantial speed advantages. This advantage originates from fundamental algorithmic differences. NeRFs approximate a line integral along a ray for each pixel, requiring expensive sampling. Point-based methods render point clouds using rasterization, which inherently benefits from parallel computational strategies [43].  \n\n# 3 3D GAUSSIAN SPLATTING: PRINCIPLES  \n\n3D GS offers a breakthrough in real-time, high-resolution image rendering, without relying on deep neural networks. This section aims to provide essential insights of 3D GS. We first elaborate on how 3D GS synthesizes an image given well-constructed 3D Gaussians in Sec. 3.1, i.e., the forward process of 3D GS. Then, we introduce how to obtain wellconstructed 3D Gaussians for a given scene in Sec. 3.2, i.e., the optimization process of 3D GS.  \n\n# 3.1 Rendering with Learned 3D Gaussians  \n\nConsider a scene represented by (millions of) optimized 3D Gaussians. The objective is to generate an image from a specified camera pose. Recall that NeRFs approach this task through computationally demanding volumetric raymarching, sampling 3D space points per pixel. Such a paradigm struggles with high-resolution image synthesis, failing to achieve real-time rendering, especially for platforms with limited computing resources [10]. By contrast, 3D GS begins by projecting these 3D Gaussians onto a pixelbased image plane, a process termed “splatting” [39], [40] (see Fig. 3b). Afterwards, 3D GS sorts these Gaussians and computes the value for each pixel. As shown in Fig. 3, the rendering of NeRFs and 3D GS can be viewed as an inverse process of each other. In what follows, we begin with the definition of a 3D Gaussian, which is the minimal element of the scene representation in 3D GS. Next, we describe how these 3D Gaussians can be used for differentiable rendering. Finally, we introduce the acceleration technique used in 3D GS, which is the key to fast rendering.  \n\n![](images/23962df8b59399ebd293a5705b3de1bfc22ab657b3921b4966a685e7abc741cb.jpg)  \nFig. 3. NeRFs vs. 3D GS. (a) NeRF samples along the ray and then queries the MLP to obtain corresponding colors and densities, which can be seen as a backward mapping (ray tracing). (b) In contrast, 3D GS projects all 3D Gaussians into the image space (i.e., splatting) and then performs parallel rendering, which can be viewed as a forward mapping (rasterization). Best viewed in color.  \n\nProperties of 3D Gaussian. A 3D Gaussian is characterized by its center (position) $\\mu ,$ opacity $\\alpha ,$ , 3D covariance matrix $\\Sigma ,$ and color c. c is represented by spherical harmonics for view-dependent appearance. All the properties are learnable and optimized through back-propagation.  \n\n• Frustum Culling. Given a specified camera pose, this step determines which 3D Gaussians are outside the camera’s frustum. By doing so, 3D Gaussians outside the given view will not be involved in the subsequent computation.  \n\nSplatting. In this step, 3D Gaussians (ellipsoids) in 3D space are projected into 2D image space (ellipses). The projection proceeds through two transformations: first, transforming 3D Gaussians from world coordinates to camera coordinates using the viewing transformation, and subsequently splatting these Gaussians into 2D image space via an approximation of the projective transformation. Mathematically, given the 3D covariance matrix $\\pmb { \\Sigma }$ describing a 3D Gaussian’s spatial distribution, and the viewing transformation matrix $W$ , the 2D covariance matrix $\\Sigma ^ { \\prime }$ characterizing the projected 2D Gaussian is computed through:  \n\n$$\n\\pmb { \\Sigma } ^ { \\prime } = \\pmb { J } \\pmb { W } \\pmb { \\Sigma } \\pmb { W } ^ { \\top } \\pmb { J } ^ { \\top } ,\n$$  \n\nwhere $J$ is the Jacobian of the affine approximation of the projective transformation [10], [39]. One might wonder why the standard camera intrinsics based projective transformation is not used here. This is because its mappings are not affine and therefore cannot directly project $\\pmb { \\Sigma }$ . 3D GS adopts an affine one proposed in [39] which approximates the projective transformation using the first two terms (including $J$ ) of the Taylor expansion (see Sec. 4.4 in [39]).  \n\nRendering by Pixels. Before delving into the final version of 3D GS which utilizes several techniques to boost parallel computation, we first elaborate on its simpler form to offer insights into its basic working mechanism. Given the position of a pixel $\\scriptstyle \\mathbf { { x } } ,$ its distance to all overlapping Gaussians, i.e., the depths of these Gaussians, can be computed through the viewing transformation matrix $W$ , forming a sorted list of Gaussians $\\mathcal { N }$ . Then, $\\alpha$ -blending is adopted to compute the final color of this pixel:  \n\n$$\nC = \\sum _ { n = 1 } ^ { | \\mathcal { N } | } c _ { n } \\alpha _ { n } ^ { \\prime } \\prod _ { j = 1 } ^ { n - 1 } \\left( 1 - \\alpha _ { j } ^ { \\prime } \\right) ,\n$$  \n\nwhere $c _ { n }$ is the learned color. The final opacity $\\alpha _ { n } ^ { \\prime }$ is the multiplication result of the learned opacity $\\alpha _ { n }$ and the Gaussian, defined as follows:  \n\n$$\n\\alpha _ { n } ^ { \\prime } = \\alpha _ { n } \\times \\exp \\big ( - \\frac { 1 } { 2 } ( { \\pmb x } ^ { \\prime } - { \\pmb \\mu } _ { n } ^ { \\prime } ) ^ { \\top } { \\pmb \\Sigma } _ { n } ^ { \\prime - 1 } ( { \\pmb x } ^ { \\prime } - { \\pmb \\mu } _ { n } ^ { \\prime } ) \\big ) ,\n$$  \n\nwhere $\\mathbf { { x } ^ { \\prime } }$ and $\\pmb { \\mu } _ { n } ^ { \\prime }$ are coordinates in the projected space. It is a reasonable concern that the rendering process described could be slower compared to NeRFs, given that generating the required sorted list is hard to parallelize. Indeed, this concern is justified; rendering speeds can be significantly impacted when utilizing such a simplistic, pixel-by-pixel approach. To achieve real-time rendering, 3D GS makes several concessions to accommodate parallel computation.  \n\nTiles (Patches). To avoid the cost computation of deriving Gaussians for each pixel, 3D GS shifts the precision from pixel-level to patch-level detail, which is inspired by tilebased rasterization [43]. Concretely, 3D GS initially divides the image into multiple non-overlapping patches (tiles). Fig. 4b provides an illustration of tiles. Each tile comprises $1 6 \\times 1 6$ pixels as suggested in [10]. 3D GS further determines which tiles intersect with these projected Gaussians. Given that a projected Gaussian may cover several tiles, a logical method involves replicating the Gaussian, assigning each copy an identifier (i.e., a tile ID) for the relevant tile.  \n\n• Parallel Rendering. After replication, 3D GS combines the respective tile ID with the depth value obtained from the view transformation for each Gaussian. This results in an unsorted list of bytes where the upper bits represent the tile ID and the lower bits signify depth. By doing so, the sorted list can be directly utilized for rendering (i.e., alpha compositing). Fig. 4c and Fig. 4d provide the visual demonstration of such concepts. It’s worth highlighting that rendering each tile and pixel occurs independently, making this process highly suitable for parallel computations. An additional benefit is that each tile’s pixels can access a common shared memory and maintain an uniform read sequence (Fig. 5), enabling parallel execution of alpha compositing with increased efficiency. In the official implementation of the original paper [10], the framework regards the processing of tiles and pixels as analogous to the blocks and threads, respectively, in CUDA programming architecture.  \n\nIn a nutshell, 3D GS introduces several approximations during rendering to enhance computational efficiency while maintaining a high standard of image synthesis quality.  \n\n# 3.2 Optimization of 3D Gaussian Splatting  \n\nAt the heart of 3D GS lies an optimization procedure devised to construct a copious collection of 3D Gaussians that accurately captures the scene’s essence, thereby facilitating free-viewpoint rendering. On the one hand, the properties of 3D Gaussians should be optimized via differentiable rasterization to fit the textures of a given scene. On the other hand, the number of 3D Gaussians that can represent a given scene well is unknown in advance. We will introduce how to optimize the properties of each Gaussian in Sec. 3.2.1 and how to adaptively control the density of the Gaussians in Sec. 3.2.2. The two procedures are interleaved within the optimization workflow. Since there are many manually set hyperparameters in the optimization process, we omit the notations of most hyperparameters for clarity.  \n\n![](images/fa02df5aa4917ada35ffb85a12c8ab32ab0e1c7074e64f96319b9139dcc94912.jpg)  \nFig. 4. An illustration of the forward process of 3D GS (see Sec. 3.1). (a) The splatting step projects 3D Gaussians into image space. (b) 3D GS divides the image into multiple non-overlapping patches, i.e., tiles. (c) 3D GS replicates the Gaussians which cover several tiles, assigning each copy an identifier, i.e., a tile ID. (d) By rendering the sorted Gaussians, we can obtain all pixels within the tile. Note that the computational workflows for pixels and tiles are independent and can be done in parallel. Best viewed in color.  \n\n![](images/a5487d94173ffd3c94766fda7d629516eb9badb8ef0d86f3e9ca6560eb84bffc.jpg)  \nFig. 5. An illustration of the tile based parallel (at the pixel-level) rendering. All the pixels within a tile (Tile1 here) access the same ordered Gaussian list stored in a shared memory for rendering. As the system processes each Gaussian sequentially, every pixel in the tile evaluates the Gaussian’s contribution according to the distance (i.e., the exp term in Eq. 6). Therefore, the rendering for a tile can be completed by iterating through the list of Gaussians just once. The computation for the red Gaussian follows a similar way and is omitted here for simplicity.  \n\n# 3.2.1 Parameter Optimization  \n\nLoss Function. Once the synthesis of the image is completed, the difference between the rendered image and ground truth can be measured. All the learnable parameters are optimized by stochastic gradient descent using the $\\ell _ { 1 }$ and D-SSIM loss functions:  \n\n$$\n\\begin{array} { r } { \\mathcal { L } = ( 1 - \\lambda ) \\mathcal { L } _ { 1 } + \\lambda \\mathcal { L } _ { \\mathrm { D - S S I M } } , } \\end{array}\n$$  \n\nwhere $\\lambda \\in [ 0 , 1 ]$ is a weighting factor.  \n\nParameter Update. Most properties of a 3D Gaussian can be optimized directly through back-propagation. It is essential to note that directly optimizing the covariance matrix $\\pmb { \\Sigma }$ can result in a non-positive semi-definite matrix, which would not adhere to the physical interpretation typically associated with covariance matrices. To circumvent this issue, 3D GS chooses to optimize a quaternion $\\pmb q$ and a 3D vector $s$ . Here $\\pmb q$ and $s$ represent rotation and scale, respectively. This approach allows the covariance matrix $\\pmb { \\Sigma }$ to be reconstructed as follows:  \n\n$$\n\\begin{array} { r } { \\Sigma = R S S ^ { \\top } R ^ { \\top } , } \\end{array}\n$$  \n\nwhere $\\scriptstyle { R }$ is the rotation matrix derived from the quaternion $\\scriptstyle q ,$ and $s$ is the scaling matrix given by $\\operatorname { d i a g } ( s )$ . As seen, there is a complex computational graph to obtain the opacity $\\alpha ,$ i.e., $\\pmb q$ and $s \\mapsto \\Sigma$ , $\\Sigma \\mapsto \\Sigma ^ { \\prime }$ , and $\\Sigma ^ { \\prime } \\mapsto \\alpha$ . To avoid the cost of automatic differentiation, 3D GS derives the gradients for $\\pmb q$ and $s$ so as to compute them directly during optimization.  \n\n# 3.2.2 Density Control  \n\nInitialization. 3D GS starts with the initial set of sparse points from SfM or random initialization. Note that a good initialization is essential to convergence and reconstruction quality [44]. Afterwards, point densification and pruning are adopted to control the density of 3D Gaussians.  \n\nPoint Densification. In the point densification phase, 3D GS adaptively increases the density of Gaussians to better capture the details of a scene. This process focuses on areas with missing geometric features or regions where Gaussians are too spread out. The densification procedure will be performed at regular intervals (i.e., after a certain number of training iterations), focusing on those Gaussians with large view-space positional gradients (i.e., above a specific threshold). It involves either cloning small Gaussians in underreconstructed areas or splitting large Gaussians in overreconstructed regions. For cloning, a copy of the Gaussian is created and moved towards the positional gradient. For splitting, a large Gaussian is replaced with two smaller ones, reducing their scale by a specific factor. This step seeks an optimal distribution and representation of Gaussians in 3D space, enhancing the overall quality of the reconstruction.  \n\n$\\cdot$ Point Pruning. The point pruning stage involves the removal of superfluous or less impactful Gaussians, which can be viewed as a regularization process. It is executed by eliminating Gaussians that are virtually transparent (with $\\alpha$ below a specified threshold) and those that are excessively large in either world-space or view-space. In addition, to prevent unjustified increases in Gaussian density near input cameras, the alpha value of the Gaussians is set close to zero after a certain number of iterations. This allows for a controlled increase in the density of necessary Gaussians while enabling the culling of redundant ones. The process not only helps in conserving computational resources but also ensures that the Gaussians in the model remain precise and effective for the representation of the scene.  \n\n# 4 3D GAUSSIAN SPLATTING: DIRECTIONS  \n\nThough 3D GS has achieved impressive milestones, significant room for improvement remains, e.g., data and hardware requirement, rendering and optimization algorithm, and applications in downstream tasks. In the subsequent sections, we seek to elaborate on select extended versions. These are: i) 3D GS for Sparse Input [45]–[55] (Sec. 4.1), ii) Memoryefficient 3D GS [56]–[64] (Sec. 4.2), iii) Photorealistic 3D GS [65]–[80] (Sec. 4.3), iv) Improved Optimization Algorithms [22], [77], [81]–[86] (Sec. 4.4), v) 3D Gaussian with More Properties [87]–[93] (Sec. 4.5), vi) Hybrid Representation [94]–[96] (Sec. 4.6), and vii) New Rendering Algorithm (Sec. 4.7). While we have carefully selected several key directions, we acknowledge that it is inevitably a biased view. A more comprehensive collection is given in Github.  \n\n# 4.1 3D GS for Sparse Input  \n\nA notable issue of 3D GS is the emergence of artifacts in areas with insufficient observational data. This challenge is a prevalent limitation in radiance field rendering, where sparse data often leads to inaccuracies in reconstruction. From a practical perspective, reconstructing scenes from limited viewpoints is of significant interest, particularly for the potential to enhance functionality with minimal input.  \n\nExisting methods can be categorized into two primary groups. i) Regularization based methods introduce additional constraints such as depth information to enhance the detail and global consistency [46], [49], [51], [55]. For example, DNGaussian [49] introduced a depth-regularized approach to address the challenge of geometry degradation in sparse input. FSGS [46] devised a Gaussian Unpooling process for initialization and also introduced depth regularization. MVSplat [51] proposed a cost volume representation so as to provide geometry cues. Unfortunately, when dealing with a limited number of views, or even just one, the efficacy of regularization techniques tends to diminish, which leads to ii) generalizability based methods that use learned priors [47], [48], [53], [97]. One approach involves synthesizing additional views through generative models, which can be seamlessly integrated into existing reconstruction pipelines [98]. However, this augmentation strategy is computationally intensive and inherently bounded by the capabilities of the used generative model. Another wellknown paradigm employs feed-forward Gaussian model to directly generates the properties of a set of 3D Gaussians. This paradigm typically requires multiple views for training but can reconstruct 3D scenes with only one input image. For instance, PixelSplat [47] proposed to sample Gaussians from dense probability distributions. Splatter Image [48] introduced a 2D image-to-image network that maps an input image to a 3D Gaussian per pixel. However, as the generated pixel-aligned Gaussians are distributed nearly evenly in the space, they struggle to represent high-frequency details and smoother regions with an appropriate number of Gaussians.  \n\nThe challenge of 3D GS for sparse inputs centers on the modeling of priors, whether through depth information, generative models, or feed-forward Gaussian models. The fundamental trade-off lies between overfitting to available views and using learned priors for generalization. Future research could explore adaptive mechanisms for controlling this trade-off, potentially through learned confidence measures, context-aware prior selection, user preferences, etc. In addition, while current methods focus on static scenes, extending these approaches to dynamic scenarios presents an exciting frontier for investigation, particularly in handling temporal consistency and motion-induced artifacts.  \n\n# 4.2 Memory-efficient 3D GS  \n\nWhile 3D GS demonstrates remarkable capabilities, its scalability poses significant challenges, particularly when juxtaposed with NeRF-based methods. The latter benefits from the simplicity of storing merely the parameters of a learned MLP. This scalability issue becomes increasingly acute in the context of large-scale scene management, where the computational and memory demands escalate substantially. Consequently, there is an urgent need to optimize memory usage in both model training and storage.  \n\nRecent research has pursued two primary directions to address memory efficiency. First, several approaches focus on reducing the number of 3D Gaussians [58], [62], [63]. These methods either employ strategic pruning of lowimpact Gaussians, such as the volume-based masking [58], or represent neighboring Gaussians using the same properties stored within a “local anchor” obtained by clustering [22], hash-grid [62], etc. Second, researchers have developed methods for compressing Gaussian’s properties [58], [61], [62]. For instance, Niedermayr et al. [61] compressed color and Gaussian parameters into compact codebooks, using sensitivity measures for effective quantization and fine-tuning. HAC [62] predicted the probability of each quantized attribute using Gaussian distributions and then devise an adaptive quantization module. These directions are not mutually exclusive; instead, one framework might use a hybrid approach combining multiple strategies.  \n\nWhile current compression techniques have achieved significant storage reduction ratios (often by factors of 10- $2 0 \\times )$ , several challenges remain. The field particularly needs advances in memory efficiency during the training phase, potentially through quantization-aware training protocols, the development of scene-agnostic, reusable codebooks, etc. Furthermore, optimizing the trade-off between compression efficiency and visual fidelity remains an open problem.  \n\n# 4.3 Photorealistic 3D GS  \n\nThe current rendering pipeline of 3D GS (Sec. 3.1) is straightforward and involves several drawbacks. For instance, the simple visibility algorithm may lead to a drastic switch in the depth/blending order of Gaussians [10]. The visual fidelity of rendered images, including aspects such as aliasing, reflections, and artifacts, can be further optimized.  \n\nRecent research has focused on addressing three main aspects of visual quality, with aliasing being specific to 3D GS’s rendering algorithm, while reflection and blur handling represent broader challenges in 3D reconstruction. i) Aliasing. Due to the discrete sampling paradigm (viewing each pixel as a single point instead of an area), 3D GS is susceptible to aliasing when dealing with varying resolutions, which leads to blurring or jagged edges. Solutions emerged at both training and inference stages. Researchers developed training-time improvements from the sampling rate perspective and introduced schemes such as multi-scale Gaussians [67], 2D Mip filter [65], and conditioned logistic function [78]. Inference-time solutions, such as 2D scaleadaptive filtering [80], offer enhanced fidelity that can be integrated into any existing 3D GS frameworks. ii) Reflection. Achieving realistic rendering of reflective materials is a hard, long-standing problem in 3D scene reconstruction. Recent works have introduced various approaches to model reflective materials [68], [73], [99] and enable relightable Gaussian representation [23], though achieving physically accurate specular effects remains challenging. iii) Blur. While 3D GS excels on carefully curated datasets, realworld captures often suffer from blurs such as motion blur and defocus blur. Recent approaches explicitly incorporated blur modeling during training, employing techniques such as coarse-to-fine kernel optimization [74] and photometric bundle adjustment [75] to address this challenge.  \n\nWhile the approximations made in 3D GS (Sec. 3.1) contribute to its computational efficiency, they also lead to aliasing, difficulties in illumination estimation, etc. Current solutions, though impressive, typically address individual problems rather than providing a universal solution. A practical intermediate approach involves first detecting specific issues (e.g., aliasing, blur) and then applying targeted optimization strategies. The ultimate goal remains developing an advanced reconstruction system that overcomes these limitations, either through fundamental improvements to 3D GS or through brand-new architectures.  \n\n# 4.4 Improved Optimization Algorithms  \n\nThe optimization of 3D GS presents several challenges that affect the quality of reconstruction. These include issues with convergence speed, visual artifacts from improper Gaussians, and the need for better regularization during optimization. The raw optimization method (Sec. 3.2) might lead to overreconstruction in some regions while underrepresenting others, resulting in blur and visual inconsistencies.  \n\nThree main directions stand out for improving the optimization of 3D GS. i) Additional Regularization (e.g., frequency [84] and geometry [22], [77]). Geometry-aware approaches have been particularly successful, preserving scene structure through the incorporation of local anchor points [22], depth and surface constraints [100]–[102], Gaussian volumes [103], etc. ii) Optimization Procedure Enhancement [44], [101], [104]. While the original strategy of density control (Sec. 3.2.2) has proven valuable, considerable room for improvement remains. For example, GaussianPro [44] addresses the challenge of dense initialization in texture-less surfaces and large-scale scenes through an advanced Gaussian densification strategy. iii) Constraint  \n\nRelaxation. Reliance on external tools/algorithms can introduce errors and cap the system’s performance potential. For instance, SfM, commonly used in the initialization process, is error-prone and struggle with complex scenes. Recent works have begun exploring COLMAP-free approaches utilizing stream continuity [81], [105], potentially enabling learning from internet-scale unposed video datasets.  \n\nThough impressive, existing methods primarily concentrate on optimizing Gaussians to accurately reconstruct scenes from scratch, neglecting a challenging yet promising solution which reconstructs scenes in a few-shot manner through established “meta representations”. Such solution could enable adaptive meta-learning strategies that combine scene-specific and general knowledge. See “learning physical priors from large-scale data” in Sec. 7 for further insights.  \n\n# 4.5 3D Gaussian with More Properties  \n\nDespite impressive, the properties of 3D Gaussian (Sec. 3.1) are designed to be used for novel-view synthesis only. By augmenting 3D Gaussian with additional properties, such as linguistic [87]–[89], semantic/instance [90]–[92], and spatialtemporal [93] properties, 3D GS demonstrates its considerable potential to revolutionize various domains.  \n\nHere we list several interesting applications using 3D Gaussians with specially designed properties. i) Language Embedded Scene Representation [87]–[89]. Due to the high computational and memory demands of current languageembedded scene representations, Shi et al. [87] proposed a quantization scheme that augments 3D Gaussian with streamlined language embeddings instead of the original high-dimensional embeddings. This method also mitigated semantic ambiguity and enhanced the precision of open-vocabulary querying by smoothing out semantic features across different views, guided by uncertainty values. ii) Scene Understanding and Editing [90]–[92]. Feature 3DGS [90] integrated 3D GS with feature field distillation from 2D foundation models. By learning a lowerdimensional feature field and applying a lightweight convolutional decoder for upsampling, Feature 3DGS achieved faster training and rendering speeds while enabling highquality feature field distillation, supporting applications like semantic segmentation and language-guided editing. iii) Spatiotemporal Modeling [93], [106]. To capture the complex spatial and temporal dynamics of 3D scenes, Yang et al. [93] conceptualized spacetime as a unified entity and approximates the spatiotemporal volume of dynamic scenes using a collection of 4D Gaussians. The proposed 4D Gaussian representation and corresponding rendering pipeline are capable of modeling arbitrary rotations in space and time and allow for end-to-end training.  \n\n# 4.6 Hybrid Representation  \n\nRather than augmenting 3D Gaussian with additional properties, another promising avenue of adapting to downstream tasks is to introduce structured information (e.g., spatial MLPs and grids) tailored for specific applications.  \n\nNext we showcase various fascinating uses of 3D GS with specially devised structured information. i) Facial Expression Modeling. Considering the challenge of creating high-fidelity 3D head avatars under sparse view conditions, Gaussian Head Avatar [96] introduced controllable 3D Gaussians and an MLP-based deformation field. Concretely, it captured detailed facial expressions and dynamics by optimizing neutral 3D Gaussians alongside the deformation field, thus ensuring both detail fidelity and expression accuracy. ii) Spatiotemporal Modeling. Yang et al. [94] proposed to reconstruct dynamic scenes with deformable 3D Gaussians. The deformable 3D Gaussians are learned in a canonical space, coupled with a deformation field (i.e., a spatial MLP) that models the spatial-temporal dynamics. The proposed method also incorporated an annealing smoothing training mechanism to enhance temporal smoothness without additional computational costs. iii) Style Transfer. Saroha et al. [107] proposed GS in style, an advanced approach for real-time neural scene stylization. To maintain a cohesive stylized appearance across multiple views without compromising on rendering speed, they used pre-trained 3D Gaussians coupled with a multi-resolution hash grid and a small MLP to produce stylized views. In a nutshell, incorporating structured information can serve as a complementary part for adapting to tasks that are incompatible with the sparsity and disorder of 3D Gaussians.  \n\n# 4.7 New Rendering Algorithm for 3D Gaussians  \n\nWhile the rasterization-based pipeline of 3D GS offers impressive real-time performance, it still suffers from the inherent limitations, including inefficient handling of highlydistorted cameras (crucial for robotics), secondary rays (for optical effects like reflections and shadows), and stochastic ray sampling (needed in various existing pipelines). In addition, the assumptions that Gaussians do not overlap and can be sorted accurately using only centers are often violated in practice, leading to temporal artifacts when camera movement changes sorting order.  \n\nRecent works [108]–[110] explored ray tracing based rendering algorithms as an alternative. For instance, GaussianTracer [108] introduced a new ray tracing implementation for Gaussian primitives, and devised several accelerating strategies according to the uneven density and interleaved nature of Gaussians. EVER [109] deivsed a physically accurate, constant density ellipsoid representation that allows for the exact computation of the volume rendering integral, rather than relying on somewhat satisfactory approximations. This advancement eliminates popping artifacts.  \n\nThanks to the fundamental paradigm shift, several exciting possibilities might emerge, including advanced optical effects (reflection, refraction, shadows, global illumination, etc.), support for complex camera models (highly-distorted lenses, rolling shutter effects, etc.), physically accurate rendering with true directional appearance evaluation (vs. tile based approximation), and more. While these capabilities currently come with additional computational costs, they provide essential building blocks for future research in inverse rendering, physical material modeling, relighting, and complex scene reconstruction.  \n\n# 5 APPLICATION AREAS AND TASKS  \n\nBuilding on the rapid advancements in 3D GS, a wide range of innovative applications has emerged across multiple domains (Fig. 6) such as robotics (Sec. 5.1), dynamic scene reconstruction and representation (Sec. 5.2), generation and editing (Sec. 5.3), avatar (Sec. 5.4), medical systems (Sec. 5.5), large-scale scene reconstruction (Sec. 5.6), physics (Sec. 5.7), and even other scientific disciplines [24], [174]–[176]. Here, we highlight key examples that underscore the transformative impact and potential of 3D GS and offer a more comprehensive collection in Github.  \n\n# 5.1 Robotics  \n\nThe evolution of scene representation in robotics has been profoundly shaped by the emergence of NeRF, which revolutionized dense mapping and environmental interaction through implicit neural models. However, NeRF’s computational cost poses a critical bottleneck for real-time robotic applications. The shift from implicit to explicit representation not only accelerates optimization but also unlocks direct access to spatial and structural scene data, making 3D GS a transformative tool for robotics. Its ability to balance high-fidelity reconstruction with computational efficiency positions 3D GS as a cornerstone for advancing robotic perception, manipulation, and navigation in dynamic, realworld environments.  \n\nThe integration of GS into robotic systems has yielded significant advancements across three core domains. In SLAM, GS-based methods [111]–[117], [123], [124], [177]– [182] excel in real-time dense mapping but face inherent trade-offs. Visual SLAM frameworks, particularly RGBD variants [112], [114], [178], leverage depth supervision for geometric fidelity but falter in low-texture or motiondegraded environments. RGB-only approaches [113], [115], [183] circumvent depth sensors but grapple with scale ambiguity and drift. Multi-sensor fusion strategies, such as LiDAR integration [159], [177], [182], enhance robustness in unstructured settings at the cost of calibration complexity. Semantic SLAM [116], [117], [123] extends scene understanding through object-level semantics but struggles with scalability due to lighting sensitivity in color-based methods or computational overhead in feature-based methods. 3D GS based manipulation [118]–[122] bypasses the need for auxiliary pose estimation in NeRF-based methods, enabling rapid single-stage tasks like grasping in static environments via geometric and semantic attributes encoded in Gaussian properties. Multi-stage manipulation [118], [120], where environmental dynamics demand real-time map updates, requires explicit modeling of dynamic adjustments (e.g., object motions and interactions), material compliance, etc.  \n\nThe advancement of 3D GS in robotics faces three pivotal challenges. First, adaptability in dynamic and unstructured environments remains critical: real-world scenes are rarely static, requiring systems to continuously update representations amid motion, occlusions, and sensor noise without sacrificing accuracy. Second, current semantic mapping methods rely on costly, scene-specific optimization processes, limiting generalizability and scalability for real-world deployment. Third, unlike NeRF based systems which can use MLP parameters as input features for downstream decision-making, 3D Gaussians’ inherent lack of spatial order complicates feature aggregation, with no standardized framework yet established. Bridging the gap between highfidelity reconstruction and actionable semantic/physical understanding will define the next frontier for 3D GS, moving beyond passive mapping towards embodied intelligence.  \n\n![](images/8ab55cb7fba7b6888d6ccb8af23a1cf37bdb885ba640d99e5a218cef75dd41df.jpg)  \nFig. 6. Typical applications benefited from GS (Sec. 5). Some images are borrowed from [132], [135], [146], [154], [160], [166] and redrawn  \n\n# 5.2 Dynamic Scene Reconstruction  \n\nDynamic scene reconstruction refers to the process of capturing and representing the three-dimensional structure and appearance of a scene that changes over time [184]–[187]. This involves creating a digital model that accurately reflects the geometry, motion, and visual aspects of the objects in the scene as they evolve. Dynamic scene reconstruction is crucial in various applications, e.g., VR/AR, 3D animation, and autonomous driving [188]–[190].  \n\nThe key to adapt 3D GS to dynamic scenes is the modeling of temporal dimension which allows for the representation of scenes that change over time. 3D GS based methods [93]–[95], [106], [125]–[130], [191]–[199] for dynamic scene reconstruction can generally be divided into two main categories as discussed in Sec. 4.5 and Sec. 4.6. The first category utilizes additional fields like spatial MLPs or grids to model deformation (Sec. 4.6). For example, Yang et al. [94] first proposed deformable 3D Gaussians tailored for dynamic scenes. These 3D Gaussians are learned in a canonical space and can be used to model spatial-temporal deformation with an implicit deformation field (implemented as an MLP). GaGS [132] devised the voxelization of a set of Gaussian distributions, followed by the use of sparse convolutions to extract geometry-aware features, which are then utilized for deformation learning. On the other hand, the second category is based on the idea that scene changes can be encoded into the 3D Gaussian representation with a specially designed rendering process (Sec. 4.5). For instance, Luiten et al. [125] introduced dynamic 3D Gaussians to model dynamic scenes by keeping the properties of 3D Gaussians unchanged over time while allowing their positions and orientations to change. Yang et al. [93] designed a 4D Gaussian representation, where additional properties are used to represent 4D rotations and spherindrical harmonics, to approximate the spatial-temporal volume of scenes.  \n\nWhile 3D GS advances dynamic scene reconstruction by modeling per-Gaussian deformations, its reliance on finegrained primitives limits scalability and robustness. Current methods struggle to balance computational efficiency and precision: small-scale reconstructions unify dynamic and static elements but become intractable in large environments, often requiring manual priors to segment regions — a barrier in unstructured settings. Furthermore, the absence of object-level motion reasoning leads to artifacts and poor generalization over long sequences. Future work might prioritize object-centric frameworks that hierarchically group Gaussians into persistent entities, enabling efficient largescale reconstruction through inherent motion disentanglement (dynamic vs. static).  \n\n# 5.3 Generation and Editing  \n\nContent generation and editing represent two fundamental and inherently interconnected capabilities in modern AI systems. While generation enables the synthesis of novel digital content from scratch or conditional inputs [200]– [202], editing provides the crucial ability to refine, adapt, and manipulate existing content with precise control [203]. Together, these capabilities revolutionize creative workflows by combining initial content creation with iterative refinement, enabling applications from professional content production to interactive consumer tools.  \n\nRecent advances in generation [133]–[138], [204]–[227] have led to the emergence of three main approaches. Optimization based methods [133], [134], [204] distill diffusion priors (gradients) to guide 3D model updates with the score functions. While these methods demonstrate impressive fidelity, they face significant computational overhead due to the necessity of comparing multiple viewpoints during the optimization process. Reconstruction based methods [135], [225], [227] reframe the generation problem as a multiview reconstruction task utilizing pre-trained multi-view diffusion models. Although this approach offers an intuitive and straightforward solution, it grapples with fundamental limitations in maintaining view consistency. The lack of strict geometric constraints across different viewpoints often results in inconsistent surface geometry and degraded texture quality, particularly in regions with complex visual features. Direct 3D generation methods train diffusion models on 3D representations [138], [220], [226]. While the learned 3D diffusion models facilitate multi-view consistency, the demanding computational costs impede the expansion of training scales necessary for improved generative diversity. Current editing works [90]–[92], [126]–[128], [140]–[143], [228]–[239] fall into two primary classes. The first class leverages 2D image-editing models (e.g., diffusion-based editors) to iteratively refine 3D Gaussians. Early efforts [141], [142], [233] adopt optimization- or reconstruction-based strategies akin to methods in generation, but introduce task-specific control signals. However, naively applying 2D edits independently across views often introduces multi-view inconsistencies. Subsequent works [140], [238]–[240] mitigate this through iterative refinement or cross-view attention, albeit at increased computational costs for alignment. A notable challenge is unintended object deformations, attributed to the weak 3D geometric priors in 2D editing models and the difficulty of reconciling 2D edits with underlying 3D structures. The second class exploits the explicit nature of 3D GS to enable direct manipulation based on embedded properties such as semantics [91], [92], [143], [232] and key points [128]. However, this class remains underexplored due to essentail challenges: the lack of inherent ordering of Gaussians complicates the design of efficient indexing schemes, while editing attributes (e.g., texture and geometry) requires careful regularization and alignment to preserve plausibility. 5.4 Avatar  \n\nAvatars, the digital representations of users in virtual spaces, bridge physical and digital realms, enabling immersive interaction, identity expression, and remote collaboration. Spanning entertainment (gaming, virtual influencers), enterprise (AI agents, virtual meetings), healthcare, and education, they underpin metaverse economies. Advances in AR and VR amplify their role in redefining social, industrial, and creative landscapes.  \n\n3D GS has emerged as a powerful tool for human avatar reconstruction, primarily advancing along two directions:  \n\nfull-body modeling and head-centric modeling. For fullbody avatars [139], [144]–[147], [241]–[252], the current methods typically anchor 3D Gaussians in a canonical space and deform them via parametric body models (e.g., SMPL) or cage-based rigging to model dynamic motions. These approaches adopt a hybrid deformation strategy: linear blend skinning handles rigid skeletal transformations such as joint rotations, while pose-conditioned deformation fields account for secondary non-rigid effects like muscle jiggles. For head avatars [23], [148]–[151], [253]–[256], the emphasis shifts to modeling intricate facial expressions, fine-grained geometry (e.g., wrinkles, hair [257]), and dynamic speechdriven animations. Techniques mainly combine parametric morphable face models (e.g., FLAME) with deformable 3D Gaussians, employing diffusion strategies and expressionaware deformation fields to disentangle rigid head poses from non-rigid facial movements. Both directions exploit the speed advantage and editability of 3D GS to enable efficient training, real-time rendering, and precise control over deformations, while addressing challenges in crossframe correspondence, topology flexibility, and multi-view consistency.  \n\nReconstruction in challenging scenes (e.g., occlusions, sparse single-view inputs, or loose clothing) and enhancing avatar interactivity represent critical challenges and opportunities. Parametric model-free methods, which bypass predefined priors by learning skinning weights directly from data, show promise for such scenarios. Complementary to this, generative models can mitigate ambiguities inherent in underconstrained settings. Further integrating physicsbased constraints might bridge the gap between static reconstructions and responsive, lifelike interactions, unlocking applications in AR, embodied AI, etc.  \n\n# 5.5 Endoscopic Scene Reconstruction  \n\nSurgical 3D reconstruction represents a fundamental task in robot-assisted minimally invasive surgery, aimed at enhancing intraoperative navigation, preoperative planning, and educational simulations through precise modeling of dynamic surgical scenes. Pioneering the integration of dynamic radiance fields into this domain, recent advancements have focused on surmounting the inherent challenges of single-viewpoint video reconstructions such as occlusions by surgical instruments and sparse viewpoint diversity within the confined spaces of endoscopic exploration [258]– [260]. Despite the progress, the call for high fidelity in tissue deformability and topological variation remains, coupled with the pressing demand for faster rendering to bridge the utility in applications sensitive to latency [152]–[154]. This synthesis of immediacy and precision in reconstructing deformable tissues from endoscopic videos is essential in propelling robotic surgery towards reduced patient trauma and AR/VR applications, ultimately fostering a more intuitive surgical environment and nurturing the future of surgical automation and robotic proficiency.  \n\nEndoscopic scene reconstruction introduces distinct challenges compared to general dynamic scenes, including sparse training data from limited camera mobility in narrow cavities, frequent tool occlusions obscuring critical regions, and single-view geometry ambiguities. Existing approaches mainly used additional depth guidance to infer the geometry of tissues [152]–[154]. For instance, EndoGS [154] integrated depth-guided supervision with spatial-temporal weight masks and surface-aligned regularization terms to enhance the quality and speed of 3D tissue rendering while addressing tool occlusion. EndoGaussian [153] introduced two new strategies: holistic Gaussian initialization for dense initialization and spatiotemporal Gaussian tracking for modeling surface dynamics. Zhao et al. [155] argued that these methods suffer from under-reconstruction and proposed to alleviate this problem from frequency perspectives. In addition, EndoGSLAM [156] and Gaussian Pancake [157] devised SLAM systems for endoscopic scenes and showed significant speed advantages.  \n\nAdvancing endoscopic 3D reconstruction requires targeted efforts in both data and dynamics modeling. Data limitations arise from single-viewpoint videos, which produce ill-posed reconstruction problems due to instrument occlusions and constrained camera mobility, leaving critical tissue regions unobserved. While depth estimators provide temporary workarounds, integrating multi-view camera systems addresses the root cause. In addition, existing datasets often feature truncated sequences (e.g., $4 \\sim 8 s$ in EndoNeRF [258]), which fail to capture prolonged tissue deformation dynamics or complex surgical workflows. Extending temporal coverage to include longer, clinically representative sequences would benefit downstream applications as aforementioned. Modeling limitations persist in current methods, which often represent tissue dynamics at the Gaussian level rather than object- or 3D region-level. This reduces their capacity to encode semantically meaningful anatomical interactions and deserves further explorations.  \n\n# 5.6 Large-scale Scene Reconstruction  \n\nLarge-scale scene reconstruction is a critical component in fields such as autonomous driving, aerial surveying, and AR/VR, demanding both photorealistic visual quality and real-time rendering capabilities. Before the emergence of 3D GS, the task has been approached using NeRF based methods, which, while effective for smaller scenes, often fall short in detail and rendering speed when scaled to larger areas (e.g., over $1 . 5 ~ k m ^ { 2 }$ ). Though 3D GS has demonstrated considerable advantages over NeRFs, the direct application of 3D GS to large-scale environments introduces significant challenges. 3D GS requires an immense number of Gaussians to maintain visual quality over extensive areas, leading to prohibitive GPU memory demands and considerable computational burdens during rendering. For instance, a scene spanning $2 . 7 \\ k m ^ { 2 }$ may require over 20 million Gaussians, pushing the limits of even the most advanced hardware (e.g., NVIDIA A100 with 40GB memory) [163].  \n\nTo address the highlighted challenges, researchers have made significant strides in two key areas: i) For training, a divide-and-conquer strategy [162]–[165] has been adopted, which segments a large scene into multiple, independent cells. This facilitates parallel optimization for expansive environments. With the same spirit, Zhao et al. [161] proposed a distributed implementation of 3D GS training. An additional challenge lies in maintaining visual quality, as large-scale scenes often feature texture-less surfaces that can hamper the effectiveness of optimization such as Gaussian initialization and density control (Sec. 3.2). Enhancing the optimization algorithm presents a viable solution to mitigate this issue [44], [164]. ii) Regarding rendering, the adoption of the Level of Details (LoD) technique from computer graphics has proven instrumental. LoD adjusts the complexity of 3D scenes to balance visual quality with computational efficiency. Current implementations involve feeding only the essential Gaussians to the rasterizer [164], or designing explicit LoD structures like the Octree [165] and hierarchy [162]. Furthermore, integrating extra input modalities like LiDAR can further enhanced the reconstruction process [158]–[160].  \n\nOne prominent challenge in large-scale scene reconstruction lies in handling sparse or incomplete capture data, which can be mitigated through few-shot adaptation schemes (see Sec. 4.1) or generalizable priors (see “learning physical priors from large-scale data” in Sec. 7). Meanwhile, memory and computational bottlenecks can be addressed via distributed learning strategies [161], such as parameter partitioning across GPU clusters and parallel batched multiview optimization.  \n\n# 5.7 Physics  \n\nThe simulation of complex real-world dynamics, such as seed dispersal or fluid motion, is pivotal for applications spanning virtual reality, animation, and scientific modeling, where realism hinges on accurate physical behavior. Advances in video diffusion models have driven progress in 4D content generation, yet these methods might produce visually plausible results that violate fundamental physical laws. 3D GS emerges as a promising solution by embedding physical constraints and properties into scene representations, enabling both visually convincing and physically coherent simulations.  \n\nExisting methods differ in how they formulate and integrate physics-based priors into their frameworks. The most common approach is employing physics simulation engines (e.g., MLS-MPM [268]) to guide the dynamics generation. The material point method [268] and position based dynamics [269] — numerical methods used in computer graphics for simulating deformations in materials like fluids, granular media, and fracturing solids — have been extensively explored by the community through various customizations [21], [143], [166]–[171]. Analytical material models, such as mass-spring systems, have also demonstrated success in approximating deformations by explicitly encoding material properties into 3D Gaussians [172]. Across these methods, 3D Gaussians are treated as discrete particles (with one exception [173] using a continuous representation) and serve as computational units within the chosen simulator. Unknown material properties or physical parameters are typically learned through video-based supervision from conditional generative models.  \n\nDespite advancements in physics based 3D GS frameworks, critical limitations persist. Current systems struggle to unify diverse physical behaviors (e.g., rigid, elastic, or soft-body dynamics) into cohesive simulations, handle complex multi-object interactions without manual intervention, and model scene-level interactions such as environmental feedback and dynamic lighting changes. Integrating adaptive physics engines capable of multi-object and multimaterial interactions, developing new simulation architectures that are compatible with priors learned from largescale data, and expanding datasets to encompass diverse materials and dynamic scenarios are equally vital.  \n\nTABLE 1 Comparison of localization methods $( \\ S \\quad )$ on Replica [261] (static scenes), in terms of absolute trajectory error (ATE, cm). (The three bes scores are marked in red, blue, and green, respectively. These notes also apply to the other tables.)   \n\n\n<html><body><table><tr><td colspan=\"3\">Method</td><td>GS</td><td>Room0</td><td>Room1</td><td>Room2</td><td>Office0</td><td>Office1</td><td>Office2</td><td>Office3</td><td>Office4</td><td>Avarage</td></tr><tr><td>iMAP [262]</td><td></td><td>[ICCV21]</td><td></td><td>3.12</td><td>2.54</td><td>2.31</td><td>1.69</td><td>1.03</td><td>3.99</td><td>4.05</td><td>1.93</td><td>2.58</td></tr><tr><td>Vox-Fusion [263]</td><td></td><td>[ISMAR22]</td><td></td><td>1.37</td><td>4.70</td><td>1.47</td><td>8.48</td><td>2.04</td><td>2.58</td><td>1.11</td><td>2.94</td><td>3.09</td></tr><tr><td>NICE-SLAM [264]</td><td></td><td>[CVPR22]</td><td></td><td>0.97</td><td>1.31</td><td>1.07</td><td>0.88</td><td>1.00</td><td>1.06</td><td>1.10</td><td>1.13</td><td>1.06</td></tr><tr><td></td><td>ESLAM[265]</td><td>[CVPR23]</td><td></td><td>0.71</td><td>0.70</td><td>0.52</td><td>0.57</td><td>0.55</td><td>0.58</td><td>0.72</td><td>0.63</td><td>0.63</td></tr><tr><td>Point-SLAM [266]</td><td></td><td>[ICCV23]</td><td></td><td>0.61</td><td>0.41</td><td>0.37</td><td>0.38</td><td>0.48</td><td>0.54</td><td>0.69</td><td>0.72</td><td>0.52</td></tr><tr><td>Co-SLAM [267]</td><td></td><td>[CVPR23]</td><td></td><td>0.70</td><td>0.95</td><td>1.35</td><td>0.59</td><td>0.55</td><td>2.03</td><td>1.56</td><td>0.72</td><td>1.00</td></tr><tr><td>Gaussian-SLAM[114]</td><td></td><td>[arXiv]</td><td>√</td><td>3.35</td><td>8.74</td><td>3.13</td><td>1.11</td><td>0.81</td><td>0.78</td><td>1.08</td><td>7.21</td><td>3.27</td></tr><tr><td>GSSLAM[113]</td><td></td><td>[CVPR24]</td><td>√</td><td>0.47</td><td>0.43</td><td>0.31</td><td>0.70</td><td>0.57</td><td>0.31</td><td>0.31</td><td>3.20</td><td>0.79</td></tr><tr><td>GS-SLAM[111]</td><td></td><td>[CVPR24]</td><td>√</td><td>0.48</td><td>0.53</td><td>0.33</td><td>0.52</td><td>0.41</td><td>0.59</td><td>0.46</td><td>0.70</td><td>0.50</td></tr><tr><td>SplaTAM [112]</td><td></td><td>[CVPR24]</td><td>√</td><td>0.31</td><td>0.40</td><td>0.29</td><td>0.47</td><td>0.27</td><td>0.29</td><td>0.32</td><td>0.55</td><td>0.36</td></tr></table></body></html>  \n\nTABLE 2 Collection of representative datasets for 3D GS. Here PC represents point clouds.   \n\n\n<html><body><table><tr><td>Name</td><td></td><td>Type</td><td># Sample</td><td>Task</td></tr><tr><td>Tanks&Temples[270][ToG17]</td><td></td><td>RGB</td><td>14</td><td rowspan=\"6\">Novel View Synthesis</td></tr><tr><td>RealEstate10K [271][TOG18]</td><td></td><td>RGB</td><td>1,000</td></tr><tr><td>DeepBlending [272][ToG18]</td><td></td><td>RGB</td><td>19</td></tr><tr><td></td><td>LLFF [273][TOG19]</td><td>RGB</td><td>8</td></tr><tr><td>NeRF [12][ECCV20]</td><td></td><td>RGB</td><td>8</td></tr><tr><td>ACID [274][IcCV21]</td><td></td><td>RGB</td><td>700+</td></tr><tr><td>Mip-NeRF360[8]</td><td>[CVPR22]</td><td>RGB</td><td>9</td><td rowspan=\"8\">Robotics</td></tr><tr><td>TUM RGB-D [275][1ROS12]</td><td></td><td>RGB-D</td><td>39</td></tr><tr><td></td><td>KITTI [276][cVPR12]</td><td>RGB-D&PC</td><td>11</td></tr><tr><td></td><td>ScanNet [277][cVPR17]</td><td>RGB-D</td><td>1,513</td></tr><tr><td></td><td>Replica [261][arXiv19]</td><td>RGB-D</td><td>18</td></tr><tr><td></td><td>Waymo [278][cvPR20]</td><td>RGB-D&PC</td><td>1,150</td></tr><tr><td></td><td>nuScenes [279][cvPR20]</td><td>RGB-D&PC</td><td>1,000</td></tr><tr><td></td><td>RLBench [280][RA-L20]</td><td>RGB</td><td>100</td></tr><tr><td>Robomimic [281][CoRL22]</td><td></td><td>RGB</td><td>800</td><td rowspan=\"2\">Dynamic Scene Reconstruction</td></tr><tr><td></td><td></td><td></td><td>86</td><td></td></tr><tr><td>HypDr-NeRF [185]roc2]</td><td></td><td></td><td>RGB</td><td></td><td rowspan=\"3\"></td></tr><tr><td>NeRF-DS</td><td>[282][cVPR23]</td><td></td><td>RGB</td><td>8</td></tr><tr><td>CoNeRF</td><td>[283][cvPR22]</td><td></td><td>RGB</td><td>7</td></tr><tr><td>SPIn-NeRF [284][cvPR23]</td><td></td><td></td><td>RGB</td><td>10</td><td rowspan=\"4\">Generation and Editing</td></tr><tr><td>Tensor4D</td><td>[285][cvPR23]</td><td></td><td>RGB</td><td>4</td></tr><tr><td>OmniObject3D [286][cvPR23]</td><td></td><td></td><td>3D Object</td><td>6,000</td></tr><tr><td>Objaverse [287][cvPR23]</td><td></td><td></td><td>3D Object</td><td>800K+</td></tr><tr><td>People-Snapshot</td><td></td><td>[288][cvPR18]</td><td>RGB</td><td>24</td><td rowspan=\"5\">Avatar</td></tr><tr><td></td><td>VOCASET [289][cVPR19]</td><td></td><td>RGB</td><td>12</td></tr><tr><td>THUman </td><td>[290][IccV19]</td><td></td><td>RGB</td><td>200</td></tr><tr><td>THUman2.0</td><td>[291][cvPR21]</td><td></td><td>RGB-D</td><td>500</td></tr><tr><td>ZJU-Mocap [292][cvPR21]</td><td></td><td></td><td>RGB</td><td>9</td></tr><tr><td></td><td>H3DS [293][IccV21]</td><td></td><td>RGB</td><td>23</td><td rowspan=\"4\">Medical</td></tr><tr><td>THUman3.0</td><td>[294][TPAMI22]</td><td></td><td>3D Scan</td><td>20</td></tr><tr><td>SCARED</td><td></td><td>[295][MICCAI19</td><td>RGB-D</td><td>9</td></tr><tr><td>EndoNeRF [258][MICCA122]</td><td></td><td></td><td>RGB</td><td>2</td></tr><tr><td>X3D</td><td></td><td>[296][cVPR24]</td><td>X-ray</td><td>15</td><td rowspan=\"4\">Large-scale Reconstrction</td></tr><tr><td>CityNeRF</td><td></td><td>297][ECCV22]</td><td>RGB</td><td>12</td></tr><tr><td>Waymo Block-NeRF UrbanBIS</td><td></td><td>[298][cvPR22]</td><td>RGB&PC RGB&PC</td><td>1</td></tr><tr><td>GauU-Scene [160][arXiv24]</td><td>[299][SIGGR23]</td><td></td><td>RGB&PC</td><td>6 1</td></tr></table></body></html>  \n\n# 6 PERFORMANCE COMPARISON  \n\nIn this section, we provide more empirical evidence by presenting the performance of several 3D GS algorithms that we previously discussed. The diverse applications of 3D GS across numerous tasks, coupled with the customtailored algorithmic designs for each task, render a uniform comparison of all 3D GS algorithms across a single task or dataset impracticable. For comprehensiveness, we provide a collection of representative datasets in Table 2 according to our analysis in Sec. 5. Due to the limited space, we have chosen several representative tasks for an in-depth performance evaluation. The performance scores are primarily sourced from the original papers, except where indicated otherwise. We also maintain a Github repository for this section.  \n\n# 6.1 Performance Benchmarking: Localization  \n\nThe localization task in SLAM involves determining the precise position and orientation of a robot or device within an environment, typically using sensor data.  \n\n• Dataset: Replica [261] dataset is a collection of 18 highly detailed 3D indoor scenes. These scenes are not only visually realistic but also offer comprehensive data including dense meshes, high-quality HDR textures, and detailed semantic information for each element. Following [262], three sequences about rooms and five sequences about offices are used for the evaluation.  \n\n• Benchmarking Algorithms: For performance comparison, we involve four recent 3D GS based algorithms [111]–[114] and six typical SLAM methods [262]–[267].  \n\nEvaluation Metric: The root mean square error (RMSE) of the absolute trajectory error (ATE) is a commonly used metric in evaluating SLAM systems [275], which measures the root mean square of the Euclidean distances between the estimated and true positions over the entire trajectory.  \n\nResult: As shown in Table 1, the recent 3D Gaussians based localization algorithms have a clear advantage over existing NeRF based dense visual SLAM. For example, SplaTAM [112] achieves a trajectory error improvement of $\\sim 5 0 \\%$ , decreasing it from $0 . 5 2 \\mathrm { c m }$ to $\\mathbf { 0 . 3 6 c m }$ compared to the previous state-of-the-art (SOTA) [266]. We attribute this to the dense and accurate 3D Gaussians reconstructed for scenes, which can handle the noise of real sensors. This reveals that effective scene representations can improve the accuracy of localization tasks.  \n\n# 6.2 Performance Benchmarking: Static Scenes  \n\nRendering focuses on transforming computer-readable information (e.g., 3D objects in the scene) to pixel-based images. This section focuses on evaluating the quality of rendering results in static scenes.  \n\nDataset: The same dataset as in Sec. 6.1, i.e., Replica [261], is used for comparison. The testing views are the same as those collected by [262].  \n\nTABLE 3 Comparison of mapping methods (§6.2) on Replica [261] (static scenes), in terms of PSNR, SSIM, and LPIPS. The results for FPS are taken from [113] using one 4090 GPU.   \n\n\n<html><body><table><tr><td>Method</td><td>GS</td><td>Metric</td><td>Room0</td><td>Room1</td><td>Room2</td><td>Office0</td><td>Office1</td><td>Office2</td><td>Office3</td><td>Office4</td><td>Avarage</td><td>FPS</td></tr><tr><td rowspan=\"3\">NICE-SLAM [264] [cVPR22]</td><td rowspan=\"3\"></td><td>PSNR↑</td><td>22.12</td><td>22.47</td><td>24.52</td><td>29.07</td><td>30.34</td><td>19.66</td><td>22.23</td><td>24.94</td><td>24.42</td><td></td></tr><tr><td>SSIM↑</td><td>0.69</td><td>0.76</td><td>0.81</td><td>0.87</td><td>0.89</td><td>0.80</td><td>0.80</td><td>0.86</td><td>0.81</td><td>0.54</td></tr><tr><td>LPIPS↓</td><td>0.33</td><td>0.27</td><td>0.21</td><td>0.23</td><td>0.18</td><td>0.23</td><td>0.21</td><td>0.20</td><td>0.23</td><td></td></tr><tr><td rowspan=\"3\">Vox-Fusion [263][ISMAR22]</td><td rowspan=\"3\"></td><td>PSNR↑</td><td>22.39</td><td>22.36</td><td>23.92</td><td>27.79</td><td>29.83</td><td>20.33</td><td>23.47</td><td>25.21</td><td>24.41</td><td></td></tr><tr><td>SSIM↑</td><td>0.68</td><td>0.75</td><td>0.80</td><td>0.86</td><td>0.88</td><td>0.79</td><td>0.80</td><td>0.85</td><td>0.80</td><td>2.17</td></tr><tr><td>LPIPS↓</td><td>0.30</td><td>0.27</td><td>0.23</td><td>0.24</td><td>0.18</td><td>0.24</td><td>0.21</td><td>0.20</td><td>0.24</td><td></td></tr><tr><td rowspan=\"3\">Point-SLAM [266][IccV23]</td><td rowspan=\"3\"></td><td>PSNR↑</td><td>32.40</td><td>34.08</td><td>35.50</td><td>38.26</td><td>39.16</td><td>33.99</td><td>33.48</td><td>33.49</td><td>35.17</td><td></td></tr><tr><td>SSIM↑</td><td>0.97</td><td>0.98</td><td>0.98</td><td>0.98</td><td>0.99</td><td>0.96</td><td>0.96</td><td>0.98</td><td>0.97</td><td>1.33</td></tr><tr><td>LPIPS↓</td><td>0.11</td><td>0.12</td><td>0.11</td><td>0.10</td><td>0.12</td><td>0.16</td><td>0.13</td><td>0.14</td><td>0.12</td><td></td></tr><tr><td rowspan=\"3\">SplaTAM [112][cVPR24]</td><td rowspan=\"3\"></td><td>PSNR↑</td><td>32.86</td><td>33.89</td><td>35.25</td><td>38.26</td><td>39.17</td><td>31.97</td><td>29.70</td><td>31.81</td><td>34.11</td><td></td></tr><tr><td>SSIM↑</td><td>0.98</td><td>0.97</td><td>0.98</td><td>0.98</td><td>0.98</td><td>0.97</td><td>0.95</td><td>0.95</td><td>0.97</td><td></td></tr><tr><td>LPIPS↓</td><td>0.07</td><td>0.10</td><td>0.08</td><td>0.09</td><td>0.09</td><td>0.10</td><td>0.12</td><td>0.15</td><td>0.10</td><td></td></tr><tr><td rowspan=\"3\">GS-SLAM [111][cVPR24]</td><td rowspan=\"3\">√</td><td>PSNR↑</td><td>31.56</td><td>32.86</td><td>32.59</td><td>38.70</td><td>41.17</td><td>32.36</td><td>32.03</td><td>32.92</td><td>34.27 0.97</td><td></td></tr><tr><td>SSIM↑</td><td>0.97</td><td>0.97</td><td>0.97</td><td>0.99</td><td>0.99</td><td>0.98</td><td>0.97</td><td>0.97</td><td></td><td></td></tr><tr><td>LPIPS↓</td><td>0.09</td><td>0.07</td><td>0.09</td><td>0.05</td><td>0.03</td><td>0.09</td><td>0.11</td><td>0.11</td><td>0.08</td><td></td></tr><tr><td rowspan=\"3\">GSSLAM [113][cvPR24]</td><td rowspan=\"3\">√</td><td>PSNR↑</td><td>34.83</td><td>36.43</td><td>37.49</td><td>39.95</td><td>42.09</td><td>36.24</td><td>36.70</td><td>36.07</td><td>37.50 0.96</td><td></td></tr><tr><td>SSIM↑</td><td>0.95</td><td>0.96</td><td>0.96</td><td>0.97</td><td>0.98</td><td>0.96</td><td>0.96</td><td>0.96</td><td></td><td>769</td></tr><tr><td>LPIPS↓</td><td>0.07</td><td>0.08</td><td>0.07</td><td>0.07</td><td>0.06</td><td>0.08</td><td>0.07</td><td>0.10</td><td>0.07</td><td></td></tr><tr><td rowspan=\"3\">Gaussian-SLAM[114][arXiv]</td><td></td><td>PSNR↑</td><td>34.31</td><td>37.28</td><td>38.18</td><td>43.97</td><td>43.56</td><td>37.39</td><td>36.48</td><td>40.19</td><td>38.90</td><td></td></tr><tr><td></td><td>SSIM↑ LPIPS↓</td><td>0.99 0.08</td><td>0.99 0.07</td><td>0.99 0.07</td><td>1.00 0.04</td><td>0.99 0.07</td><td>0.99 0.08</td><td>0.99 0.08</td><td>1.00 0.07</td><td>0.99 0.07</td><td></td></tr></table></body></html>  \n\nBenchmarking Algorithms: For performance comparison, we involve four recent papers which introduce 3D Gaussians into their systems [111]–[114], as well as three dense SLAM methods [263], [264], [266].  \n\nEvaluation Metric: Peak signal-to-noise ratio (PSNR), structural similarity (SSIM) [300], and learned perceptual image patch similarity (LPIPS) [301] are used for measuring RGB rendering performance.  \n\nResult: Table 3 shows that 3D Gaussians based systems generally outperform the three dense SLAM competitors. For example, Gaussian-SLAM [114] establishes new SOTA and outperforms previous methods by a large margin. Compared to Point-SLAM [266], GSSLAM [113] is about 578 times faster in achieving very competitive accuracy. In contrast to previous method [266] that relies on depth information, such as depth-guided ray sampling, for synthesizing novel views, 3D GS based system eliminates this need, allowing for high-fidelity rendering for any views.  \n\n# 6.3 Performance Benchmarking: Dynamic Scenes  \n\nThis section focuses on evaluating the rendering quality in dynamic scenes.  \n\nDataset: D-NeRF [184] dataset includes videos with 50 to 200 frames each, captured from unique viewpoints. It features synthetic, animated objects in complex scenes, with non-Lambertian materials. The dataset provides 50 to 200 training images and 20 test images per scene, designed for evaluating models in the monocular setting. The testing views are the same as the original paper [184].  \n\nBenchmarking Algorithms: For performance comparison, we involve five recent papers that model dynamic scenes with 3D GS [93]–[95], [126], [132], as well as six NeRF based approaches [37], [184], [187], [302]–[304].  \n\nEvaluation Metric: The same metrics as in Sec. 6.2, i.e., PSNR, SSIM [300], and LPIPS [301], are used for evaluation. Result: From Table 4 we can observe that 3D GS based methods outperform existing SOTAs by a clear margin. The static version of 3D GS [10] fails to reconstruct dynamic scenes, resulting in a sharp drop in performance. By modeling the dynamics, D-3DGS [94] outperforms the SOTA method, FFDNeRF [187], by 6.83dB in terms of PSNR. These results indicate the effectiveness of introducing additional properties or structured information to model the deformation of Gaussians so as to model the scene dynamics.  \n\nTABLE 4 Comparison of reconstruction methods (§6.3) on D-NeRF [184] (dynamic scenes), in terms of PSNR, SSIM, and LPIPS. ∗ denotes results reported in [95].   \n\n\n<html><body><table><tr><td colspan=\"2\">Method</td><td>GS</td><td>PSNR↑</td><td>SSIM↑</td><td>LPIPS↓</td></tr><tr><td colspan=\"2\">D-NeRF[184]</td><td colspan=\"3\" rowspan=\"6\"></td><td>0.95</td><td>0.07</td></tr><tr><td>TiNeuVox-B[302]</td><td>[CVPR21]</td><td>30.50 32.67</td><td></td></tr><tr><td></td><td>[SGA22]</td><td>0.97</td><td>0.04</td></tr><tr><td>KPlanes[37]</td><td>[CVPR23]</td><td>31.61 0.97 32.68</td><td>1</td></tr><tr><td>HexPlane-Slim [303]</td><td>[CVPR23]</td><td>0.97</td><td>0.02</td></tr><tr><td colspan=\"2\">FFDNeRF[187]</td><td>[ICCV23]</td><td>32.68 0.97</td><td>0.02</td></tr><tr><td colspan=\"2\">MSTH[304]</td><td>[NeurIPS23]</td><td>31.34</td><td>0.98 0.02</td></tr><tr><td colspan=\"2\">3D GS* [10]</td><td>[TOG23]</td><td>23.19</td><td>0.93 0.08</td></tr><tr><td colspan=\"2\">4DGS [93]</td><td>[ICLR24] √</td><td>34.09</td><td>0.98</td></tr><tr><td colspan=\"2\">4D-GS[95]</td><td>[CVPR24] √</td><td>34.05</td><td>0.98 0.02</td></tr><tr><td colspan=\"2\">GaGS[132]</td><td>[CVPR24] √</td><td>37.36</td><td>0.99 0.01</td></tr><tr><td colspan=\"2\">CoGS [126]</td><td>[CVPR24] √</td><td>37.90</td><td>0.98 0.02</td></tr><tr><td colspan=\"2\">D-3DGS [94]</td><td>[CVPR24] √</td><td>39.51</td><td>0.99 0.01</td></tr></table></body></html>  \n\n# 6.4 Performance Benchmarking: Human Avatar  \n\nHuman avatar modeling aims to create the model of human avatars from a given multi-view video.  \n\n• Dataset: ZJU-MoCap [292] is a prevalent benchmark in human modeling from videos, captured with 23 synchronized cameras at a $1 0 2 4 \\times 1 0 2 4$ resolution. Six subjects (i.e., 377, 386, 387, 392, 393, and 394) are used for evaluation [305]. The same testing views following [306] are adopted.  \n\nBenchmarking Algorithms: For performance comparison, we involve three recent papers which model human avatar with 3D GS [145], [146], [249], as well as six human rendering approaches [292], [305]–[309].  \n\nEvaluation Metric: PSNR, SSIM [300], and LPIPS\\* [301] are used for measuring RGB rendering performance. Here LPIPS\\* equals to $\\mathrm { L P I P S } \\times 1 0 0 0$ .  \n\nTABLE 5 Comparison of reconstruction methods (§6.4) on ZJU-MoCap [292] (avatar), in terms of PSNR, SSIM, and LPIPS\\*. The results for non-GS methods are taken from [146].   \n\n\n<html><body><table><tr><td>Method</td><td>GS</td><td>PSNR↑</td><td>SSIM↑</td><td>LPIPS*↓</td></tr><tr><td>NeuralBody [292] [CVPR21]</td><td></td><td>29.03</td><td>0.96</td><td>42.47</td></tr><tr><td>AnimNeRF [307][IcCV21]</td><td></td><td>29.77</td><td>0.96</td><td>46.89</td></tr><tr><td>PixelNeRF</td><td>[308] [IccV21]</td><td>24.71</td><td>0.89</td><td>121.86</td></tr><tr><td>NHP</td><td>[309] [NeurIPS21]</td><td>28.25</td><td>0.95</td><td>64.77</td></tr><tr><td>HumanNeRF [305] [cvPR22]</td><td></td><td>30.66</td><td>0.97</td><td>33.38</td></tr><tr><td>Instant-NVR [306] [cvPR23]</td><td></td><td>31.01</td><td>0.97</td><td>38.45</td></tr><tr><td>GauHuman</td><td>[145] [cVPR24] √</td><td>31.34</td><td>0.97</td><td>30.51</td></tr><tr><td>3DGS-Avatar [249] [cvPR24]</td><td>√</td><td>30.61</td><td>0.97</td><td>29.58</td></tr><tr><td>GART [146] [cVPR24]</td><td>√</td><td>32.22</td><td>0.98</td><td>29.21</td></tr></table></body></html>  \n\nTABLE 6 Comparison of reconstruction methods (§6.5) on EndoNeRF [258] (surgical scenes), in terms of PSNR, SSIM, and LPIPS. The results for non-GS methods are taken from [153]. FPS and GPU usage for training (Mem.) are measured using one 4090 GPU [153].   \n\n\n<html><body><table><tr><td colspan=\"2\">Method</td><td>GS</td><td>PSNR↑</td><td>SSIM↑</td><td>LPIPS↓</td><td>FPS↑</td><td>Mem.↓</td></tr><tr><td>EndoNeRF [258][MICCA22</td><td></td><td rowspan=\"6\"></td><td>36.06</td><td>0.93</td><td>0.09</td><td>0.04</td><td>19GB</td></tr><tr><td>EndoSurf [260][MICCAl23]</td><td></td><td>36.53</td><td>0.95</td><td>0.07</td><td>0.04</td><td>17GB</td></tr><tr><td>LerPlane-9k [259]MICCA123]</td><td></td><td>35.00</td><td>0.93</td><td>0.08</td><td>0.91</td><td>20GB</td></tr><tr><td>LerPlane-32k [259]MCCA23</td><td></td><td>37.38</td><td>0.95</td><td>0.05</td><td>0.87</td><td>20GB</td></tr><tr><td>Endo-4DGS [152][MICCAl24]</td><td></td><td>37.00</td><td>0.96</td><td>0.05</td><td></td><td>4GB</td></tr><tr><td>EndoGaussian [153]iv]</td><td></td><td>37.85</td><td>0.96</td><td>0.05</td><td>195.09</td><td>2GB</td></tr><tr><td></td><td>HFGS[155][BMVC24]</td><td>√ √</td><td>38.14</td><td>0.97</td><td>0.03</td><td></td><td></td></tr></table></body></html>  \n\n• Result: Table 5 presents the numerical results of topleading solutions in human avatar modeling. We observe that introducing 3D GS into the framework leads to consistent performance improvements in both rendering quality and speed. For instance, GART [146] outperforms current SOTA, Instant-NVR [306], by 1.21dB in terms of PSNR. Considering the enhanced fidelity, inference speed and editability, 3D GS based avatar modeling may revolutionize the field of 3D animation, interactive gaming, etc.  \n\n# 6.5 Performance Benchmarking: Surgical Scenes  \n\n3D reconstruction from endoscopic video is critical to robotic-assisted minimally invasive surgery, enabling preoperative planning, training through AR/VR simulations, and intraoperative guidance.  \n\nDataset: EndoNeRF [258] dataset presents a specialized collection of stereo camera captures, comprising two samples of in-vivo prostatectomy. It is tailored to represent realworld surgical complexities, including challenging scenes with tool occlusion and pronounced non-rigid deformation. The same testing views as in [260] are used.  \n\nBenchmarking Algorithms: For performance comparison, we involve three recent papers which reconstruct dynamic 3D endoscopic scenes with GS [152], [153], [155], as well as three NeRF-based surgical reconstruction approaches [258]– [260].  \n\nEvaluation Metric: PSNR, SSIM [300], and LPIPS [301] are adopted for evaluation. In addition, the requirement for GPU memory is also reported.  \n\n• Result: Table 6 shows that introducing the explicit representation of 3D Gaussians leads to several significant improvements. For instance, EndoGaussian [153] outperforms a strong baseline, LerPlane- $\\boldsymbol { \\cdot } 3 2 \\mathbf { k }$ [259], among all metrics. In particular, EndoGaussian demonstrates an approximate 224- fold increase in speed while consumes just $1 0 \\%$ of the GPU resources. These impressive results attest to the efficiency of GS-based methods, which not only expedite processing but also minimize GPU load, thus easing the demands on hardware. Such attributes are vitally significant for realworld surgical application deployment, where optimized resource usage can be a key determinant of practical utility.  \n\n# 7 FUTURE RESEARCH DIRECTIONS  \n\nAs impressive as those follow-up works on 3D GS are, and as much as those fields have been or might be revolutionized by 3D GS, there is a general agreement that 3D GS still has considerable room for improvement.  \n\nPhysics- and Semantics-aware Scene Representation. As a new, explicit scene representation technique, 3D Gaussian offers transformative potential beyond merely enhancing novel-view synthesis. It has the potential to pave the way for simultaneous advancements in scene reconstruction and understanding by devising physics- and semantics-aware 3D GS systems. While significant progress has been made in physics (Sec. 5.7) and semantics [310]–[315] individually, there remains considerable untapped potential in their synergistic integration. This is poised to revolutionize a range of fields and downstream applications. For instance, incorporating prior knowledge such as the general shape of objects can reduce the need for extensive training viewpoints [47], [48] while improving geometry/surface reconstruction [77], [316]. A critical metric for assessing scene representation is the quality of its generated scenes, which encompasses challenges in geometry, texture, and lighting fidelity [66], [128], [141]. By merging physical principles and semantic information within the 3D GS framework, one can expect that the quality will be enhanced, thereby facilitating dynamics modeling [21], [166], editing [90], [92], generation [133], [134], and beyond. In a nutshell, pursuing this advanced and versatile scene representation opens up new possibilities for innovation in computational creativity and practical applications across diverse domains.  \n\nLearning Physical Priors from Large-scale Data. As we explore the potential of physics- and semantics-aware scene representations, leveraging large-scale datasets to learn generalizable, physical priors emerges as a promising direction. The goal is to model the inherent physical properties and dynamics embedded within real-world data, transforming them into actionable insights that can be applied across various domains such as robotics and visual effects. Establishing a learning framework for extracting these generalizable priors enables the application of these insights to specific tasks in a few-shot manner. For instance, it allows for rapid adaptation to new objects and environments with minimal data input. Furthermore, integrating physical priors can enhance not only the accuracy and quality of generated scenes but also their interactive and dynamic qualities. This is particularly valuable in AR/VR environments, where users interact with virtual objects that behave in ways consistent with their real-world counterparts. However, the existing body of work on capturing and distilling physics-based knowledge from extensive 2D and 3D datasets remains sparse. Notable efforts in related area include the continuum mechanics based GS systems (Sec. 5.7), and the generalizable Gaussian representation based on multi-view stereo [317].  \n\nFurther exploration on real2sim and sim2real might offer viable routes for advancements in this field.  \n\nModeling Internal Structures of Objects with 3D GS. Despite the ability of 3D GS to produce highly photorealistic renderings, modeling internal structures of objects (e.g., for a scanned object in computed tomography) within the current GS framework presents a notable challenge. Due to the splatting and density control process, the current representation of 3D Gaussian is unorganized and cannot align well with the object’s actual internal structures. Moreover, there is a strong preference in various applications to depict objects as volumes (e.g., computed tomography). However, the disordered nature of 3D GS makes volume modeling particularly difficult. Li et al. [318] used 3D Gaussians with density control as the basis for the volumetric representation and did not involve the splatting process. X-Gaussian [319] involves the splatting process for fast training and inference but cannot generate volumetric representation. Using 3D GS to model the internal structures of objects remains unanswered and deserves further exploration.  \n\n3D GS for Simulation in Autonomous Driving and beyond. Collecting real-world datasets for autonomous driving is both expensive and logistically challenging, yet crucial for training effective image-based perception systems. To mitigate these issues, simulation emerges as a cost-effective alternative, enabling the generation of synthetic datasets across diverse environments. However, the development of simulators capable of producing photorealistic and diverse synthetic data is fraught with challenges. These include achieving a high level of quality, accommodating various control methods, and accurately simulating a range of lighting conditions. While early efforts [188]–[190] in reconstructing urban/street scenes with 3D GS have been encouraging, they are just the tip of the iceberg in terms of the full capabilities. There remain numerous critical aspects to be explored, such as the integration of user-defined object models, the modeling of physics-aware scene changes (e.g., the rotation of vehicle wheels), and the enhancement of controllability and quality (e.g., in varying lighting conditions). Mastery of these capabilities would not only advance autonomous systems but also redefine computational understanding of physical spaces — a leap with implications for world models, spatial intelligence, embodied AI, and beyond.  \n\nEmpowering 3D GS with More Possibilities. Despite the significant potential of 3D GS, the full scope of applications for 3D GS remains largely untapped. A promising avenue for exploration involves augmenting 3D Gaussians with additional attributes (e.g., linguistic and spatiotemporal properties as mentioned in Sec. 4.5) and introducing structured information (e.g., spatial MLPs and grids as mentioned in Sec. 4.6), tailored for specific applications. Moreover, recent studies have begun to unveil the capability of 3D GS in several domains, e.g., point cloud registration [320], image representation and compression [60], and fluid synthesis [171]. These findings highlight a significant opportunity for interdisciplinary scholars to explore 3D GS further.  \n\n# 8 CONCLUSIONS  \n\nTo the best of our knowledge, this survey presents the first comprehensive overview of 3D GS, a groundbreaking technique revolutionizing explicit radiance fields, computer graphics, and computer vision. It delineates the paradigm shift from traditional NeRF based methods, spotlighting the advantages of 3D GS in real-time rendering and enhanced editability. Our in-depth analysis and extensive quantitative studies demonstrate the superiority of 3D GS in practical applications, particularly those highly sensitive to latency. We offer insights into principles, prospective research directions, and the unresolved challenges within this domain. Overall, 3D GS stands as a transformative technology, poised to significantly influence future advancements in 3D reconstruction and representation. This survey is intended to serve as a foundational resource, propelling further exploration and progress in this rapidly evolving field.\n"
]