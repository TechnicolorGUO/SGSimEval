sentence,references
"The journey of 3D scene reconstruction began long before the surge of deep learning, with early endeavors focusing on light fields and basic scene reconstruction methods [1]–[3]","[1] S. J. Gortler, R. Grzeszczuk, R. Szeliski, and M. F. Cohen, “The lumigraph,” in Seminal Graphics Papers: Pushing the Boundaries, Volume 2, 2023, pp. 453–464.;[2] M. Levoy and P. Hanrahan, “Light field rendering,” in Seminal Graphics Papers: Pushing the Boundaries, Volume 2, 2023, pp. 441– 452.;[3] C. Buehler, M. Bosse, L. McMillan, S. Gortler, and M. Cohen, “Unstructured lumigraph rendering,” in Seminal Graphics Papers: Pushing the Boundaries, Volume 2, 2023, pp. 497–504."
The emergence of structure-frommotion [4] and subsequent advancements in multi-view stereo [5] algorithms provided a more robust framework for 3D scene reconstruction,"[4] N. Snavely, S. M. Seitz, and R. Szeliski, “Photo tourism: exploring photo collections in $3 \\mathrm { d } , \\prime \\prime$ in ACM Trans. Graph., 2006, pp. 835–846.;[5] M. Goesele, N. Snavely, B. Curless, H. Hoppe, and S. M. Seitz, “Multi-view stereo for community photo collections,” in Proc. IEEE Int. Conf. Comput. Vis., 2007, pp. 1–8."
"NeRF based methods are computationally intensive [6]–[9], often requiring extensive training times and substantial resources for rendering, especially for highresolution outputs. ii) Editability","[6] S. J. Garbin, M. Kowalski, M. Johnson, J. Shotton, and J. Valentin, “Fastnerf: High-fidelity neural rendering at 200fps,” in Proc. IEEE Int. Conf. Comput. Vis., 2021, pp. 14 346–14 355.;[7] C. Reiser, S. Peng, Y. Liao, and A. Geiger, “Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps,” in Proc. IEEE Int. Conf. Comput. Vis., 2021, pp. 14 335–14 345.;[8] J. T. Barron, B. Mildenhall, D. Verbin, P. P. Srinivasan, and P. Hedman, “Mip-nerf 360: Unbounded anti-aliased neural radiance fields,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2022, pp. 5470–5479.;[9] T. M ¨uller, A. Evans, C. Schied, and A. Keller, “Instant neural graphics primitives with a multiresolution hash encoding,” ACM Trans. Graph., vol. 41, no. 4, pp. 1–15, 2022."
"It is in this context that 3D Gaussian splatting (GS) [10] emerges, not merely as an incremental improvement but as a paradigm-shifting approach that redefines the boundaries of scene representation and rendering","[10] B. Kerbl, G. Kopanas, T. Leimk ¨uhler, and G. Drettakis, “3d gaussian splatting for real-time radiance field rendering,” ACM Trans. Graph., vol. 42, no. 4, 2023."
"Unlike the implicit, coordinate-based models [11], [12], 3D GS employs an explicit representation and highly parallelized workflows, facilitating more efficient computation and rendering","[11] V. Sitzmann, J. Thies, F. Heide, M. Nießner, G. Wetzstein, and M. Zollhofer, “Deepvoxels: Learning persistent 3d feature embeddings,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019, pp. 2437–2446.;[12] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng, “Nerf: Representing scenes as neural radiance fields for view synthesis,” in Proc. Eur. Conf. Comput. Vis., 2020, pp. 405–421."
The innovation of 3D GS lies in its unique blend of the benefits of differentiable pipelines and point-based rendering techniques [13]–[17],"[13] H. Pfister, M. Zwicker, J. Van Baar, and M. Gross, “Surfels: Surface elements as rendering primitives,” in Proceedings of the 27th annual conference on Computer graphics and interactive techniques, 2000, pp. 335–342.;[14] M. Zwicker, H. Pfister, J. Van Baar, and M. Gross, “Surface splatting,” in Proceedings of the 28th annual conference on Computer graphics and interactive techniques, 2001, pp. 371–378.;[15] L. Ren, H. Pfister, and M. Zwicker, “Object space ewa surface splatting: A hardware accelerated approach to high quality point rendering,” in Comput. Graph. Forum, no. 3, 2002, pp. 461–470.;[16] W. Yifan, F. Serena, S. Wu, C. O¨ ztireli, and O. Sorkine-Hornung, “Differentiable surface splatting for point-based geometry processing,” ACM Trans. Graph., vol. 38, no. 6, pp. 1–14, 2019.;[17] O. Wiles, G. Gkioxari, R. Szeliski, and J. Johnson, “Synsin: Endto-end view synthesis from a single image,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2020, pp. 7467–7477."
"By enabling real-time rendering capabilities without compromising on visual quality, 3D GS opens up a plethora of possibilities for applications ranging from virtual reality and augmented reality to real-time cinematic rendering and beyond [18]–[21]","[18] D. Kalkofen, E. Mendez, and D. Schmalstieg, “Comprehensible visualization for augmented reality,” IEEE Trans. Vis. Comput. Graph., vol. 15, no. 2, pp. 193–204, 2008.;[19] A. Patney, M. Salvi, J. Kim, A. Kaplanyan, C. Wyman, N. Benty, D. Luebke, and A. Lefohn, “Towards foveated rendering for gazetracked virtual reality,” ACM Trans. Graph., vol. 35, no. 6, pp. 1–12, 2016.;[20] R. Albert, A. Patney, D. Luebke, and J. Kim, “Latency requirements for foveated rendering in virtual reality,” ACM Transactions on Applied Perception, vol. 14, no. 4, pp. 1–13, 2017.;[21] Y. Jiang, C. Yu, T. Xie, X. Li, Y. Feng, H. Wang, M. Li, H. Lau, F. Gao, Y. Yang et al., “Vr-gs: A physical dynamics-aware interactive gaussian splatting system in virtual reality,” arXiv preprint arXiv:2401.16663, 2024."
"Furthermore, 3D GS’s explicit scene representation offers unprecedented flexibility to control the objects and scene dynamics, a crucial factor in complex scenarios involving intricate geometries and varying lighting conditions [22]–[24]","[22] T. Lu, M. Yu, L. Xu, Y. Xiangli, L. Wang, D. Lin, and B. Dai, “Scaffold-gs: Structured 3d gaussians for view-adaptive rendering,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[23] S. Saito, G. Schwartz, T. Simon, J. Li, and G. Nam, “Relightable gaussian codec avatars,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[24] T. Zhang, K. Huang, W. Zhi, and M. Johnson-Roberson, “Darkgs: Learning neural illumination and 3d gaussians relighting for robotic exploration in the dark,” in Proc. IEEE/RSJ Int. Conf. Intell. Robot. Syst., 2024."
"The distinctions of this survey from existing literature [25]– [28] are evident in the following aspects:  
We provide the first systematic and comprehensive review that examines 3D GS from a macro-level perspective by establishing clear taxonomies and frameworks","[25] B. Fei, J. Xu, R. Zhang, Q. Zhou, W. Yang, and Y. He, “3d gaussian splatting as new era: A survey,” IEEE Trans. Vis. Comput. Graph., 2024.;[26] A. Dalal, D. Hagen, K. G. Robbersmyr, and K. M. Knausg˚ard, “Gaussian splatting: 3d reconstruction and novel view synthesis, a review,” IEEE Access, 2024.;[27] Y. Bao, T. Ding, J. Huo, Y. Liu, Y. Li, W. Li, Y. Gao, and J. Luo, “3d gaussian splatting: Survey, technologies, challenges, and opportunities,” arXiv preprint arXiv:2407.17418, 2024.;[28] T. Wu, Y.-J. Yuan, L.-X. Zhang, J. Yang, Y.-P. Cao, L.-Q. Yan, and L. Gao, “Recent advances in 3d gaussian splatting,” Comput. Vis. Media, pp. 1–30, 2024."
"For a comprehensive overview of radiance fields, scene reconstruction and representation, and rendering methods, please see the excellent surveys [29]–[33] for more insights","[29] L. Kobbelt and M. Botsch, “A survey of point-based techniques in computer graphics,” Comput. Graph., vol. 28, no. 6, pp. 801–814, 2004.;[30] Y. Xie, T. Takikawa, S. Saito, O. Litany, S. Yan, N. Khan, F. Tombari, J. Tompkin, V. Sitzmann, and S. Sridhar, “Neural fields in visual computing and beyond,” in Comput. Graph. Forum, no. 2, 2022, pp. 641–676.;[31] W. Wang, Y. Yang, and Y. Pan, “Visual knowledge in the big model era: Retrospect and prospect,” arXiv preprint arXiv:2404.04308, 2024.;[32] A. Tewari, J. Thies, B. Mildenhall, P. Srinivasan, E. Tretschk, W. Yifan, C. Lassner, V. Sitzmann, R. Martin-Brualla, S. Lombardi et al., “Advances in neural rendering,” in Comput. Graph. Forum, no. 2, 2022, pp. 703–735.;[33] X.-F. Han, H. Laga, and M. Bennamoun, “Image-based 3d object reconstruction: State-of-the-art and trends in the deep learning era,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 43, no. 5, pp. 1578–1604, 2019."
"In the deep learning era, neural networks are often used to learn a continuous volumetric scene representation [34], [35]","[34] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and A. Geiger, “Occupancy networks: Learning 3d reconstruction in function space,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019, pp. 4460–4470.;[35] J. J. Park, P. Florence, J. Straub, R. Newcombe, and S. Lovegrove, “Deepsdf: Learning continuous signed distance functions for shape representation,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019, pp. 165–174."
The most prominent example is NeRF [12],"[12] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng, “Nerf: Representing scenes as neural radiance fields for view synthesis,” in Proc. Eur. Conf. Comput. Vis., 2020, pp. 405–421."
"Note that typically, the color $c$ is direction-dependent, whereas the volume density $\\sigma$ is not [12].  
• Explicit Radiance Field","[12] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng, “Nerf: Representing scenes as neural radiance fields for view synthesis,” in Proc. Eur. Conf. Comput. Vis., 2020, pp. 405–421."
"An explicit radiance field directly represents the distribution of light in a discrete spatial structure, such as a voxel grid or a set of points [36], [37]","[36] C. Sun, M. Sun, and H.-T. Chen, “Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2022, pp. 5459– 5469.;[37] S. Fridovich-Keil, G. Meanti, F. R. Warburg, B. Recht, and A. Kanazawa, “K-planes: Explicit radiance fields in space, time, and appearance,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2023, pp. 12 479–12 488."
"Another one is directly storing coefficients of directional basis functions, such as spherical harmonics or spherical Gaussians, where the final color is computed as a function of these coefficients and the viewing direction.  
• 3D Gaussian Splatting: Best-of-Both Worlds. 3D GS [10] is an explicit radiance field with the advantages of implicit radiance fields","[10] B. Kerbl, G. Kopanas, T. Leimk ¨uhler, and G. Drettakis, “3d gaussian splatting for real-time radiance field rendering,” ACM Trans. Graph., vol. 42, no. 4, 2023."
"The pixel color $C ( \\pmb { r } )$ is computed through a line integral along the ray $\\mathbf { } r ( t ) .$ , mathematically expressed as [12]:  
$$
C ( \\pmb { r } ) = \\int _ { t _ { \\mathrm { n e a r } } } ^ { t _ { \\mathrm { f a r } } } T ( t ) \\sigma ( \\pmb { r } ( t ) ) c ( \\pmb { r } ( t ) , \\pmb { d } ) d t ,
$$  
where $\\sigma ( \\boldsymbol { r } ( t ) )$ is the volume density at point $\\boldsymbol { r } ( t ) , \\boldsymbol { c } ( \\boldsymbol { r } ( t ) , d )$ is the color at that point, and $T ( t )$ is the transmittance","[12] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng, “Nerf: Representing scenes as neural radiance fields for view synthesis,” in Proc. Eur. Conf. Comput. Vis., 2020, pp. 405–421."
NeRF [12] shares the same spirit of ray-marching and introduces importance sampling and positional encoding to improve the quality of synthesized images,"[12] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng, “Nerf: Representing scenes as neural radiance fields for view synthesis,” in Proc. Eur. Conf. Comput. Vis., 2020, pp. 405–421."
"Its simplest form [38] rasterizes point clouds with a fixed size, which introduces drawbacks such as holes and rendering artifacts","[38] J. P. Grossman and W. J. Dally, “Point sample rendering,” in Render. Tech., 1998, pp. 181–192."
"Seminal works addressed these limitations through various methods, including: i) splatting point primitives with a spatial extent [14], [15], [39], [40], and ii) more recently, embedding neural features directly into points for subsequent network-based rendering [41], [42]. 3D GS uses 3D Gaussian as the point primitive that contains explicit attributes (e.g., color and opacity) instead of implicit neural features","[14] M. Zwicker, H. Pfister, J. Van Baar, and M. Gross, “Surface splatting,” in Proceedings of the 28th annual conference on Computer graphics and interactive techniques, 2001, pp. 371–378.;[15] L. Ren, H. Pfister, and M. Zwicker, “Object space ewa surface splatting: A hardware accelerated approach to high quality point rendering,” in Comput. Graph. Forum, no. 3, 2002, pp. 461–470.;[39] M. Zwicker, H. Pfister, J. Van Baar, and M. Gross, “Ewa volume splatting,” in Proceedings Visualization, 2001. VIS’01., 2001, pp. 29– 538.;[40] “Ewa splatting,” IEEE Trans. Vis. Comput. Graph., vol. 8, no. 3, pp. 223–238, 2002.;[41] K.-A. Aliev, A. Sevastopolsky, M. Kolos, D. Ulyanov, and V. Lempitsky, “Neural point-based graphics,” in Proc. Eur. Conf. Comput. Vis., 2020, pp. 696–712.;[42] D. R ¨uckert, L. Franke, and M. Stamminger, “Adop: Approximate differentiable one-pixel point rendering,” ACM Trans. Graph., vol. 41, no. 4, pp. 1–14, 2022."
"The rendering approach, i.e., pointbased $\\alpha$ -blending (exemplified in Eq. 5), shares the same image formation model as NeRF-style volumetric rendering (Eq. 3) [10], but demonstrates substantial speed advantages","[10] B. Kerbl, G. Kopanas, T. Leimk ¨uhler, and G. Drettakis, “3d gaussian splatting for real-time radiance field rendering,” ACM Trans. Graph., vol. 42, no. 4, 2023."
"Point-based methods render point clouds using rasterization, which inherently benefits from parallel computational strategies [43].  
3D GS offers a breakthrough in real-time, high-resolution image rendering, without relying on deep neural networks","[43] C. Lassner and M. Zollhofer, “Pulsar: Efficient sphere-based neural rendering,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2021, pp. 1440–1449."
"Such a paradigm struggles with high-resolution image synthesis, failing to achieve real-time rendering, especially for platforms with limited computing resources [10]","[10] B. Kerbl, G. Kopanas, T. Leimk ¨uhler, and G. Drettakis, “3d gaussian splatting for real-time radiance field rendering,” ACM Trans. Graph., vol. 42, no. 4, 2023."
"By contrast, 3D GS begins by projecting these 3D Gaussians onto a pixelbased image plane, a process termed “splatting” [39], [40] (see Fig. 3b)","[39] M. Zwicker, H. Pfister, J. Van Baar, and M. Gross, “Ewa volume splatting,” in Proceedings Visualization, 2001. VIS’01., 2001, pp. 29– 538.;[40] “Ewa splatting,” IEEE Trans. Vis. Comput. Graph., vol. 8, no. 3, pp. 223–238, 2002."
"Mathematically, given the 3D covariance matrix $\\pmb { \\Sigma }$ describing a 3D Gaussian’s spatial distribution, and the viewing transformation matrix $W$ , the 2D covariance matrix $\\Sigma ^ { \\prime }$ characterizing the projected 2D Gaussian is computed through:  
$$
\\pmb { \\Sigma } ^ { \\prime } = \\pmb { J } \\pmb { W } \\pmb { \\Sigma } \\pmb { W } ^ { \\top } \\pmb { J } ^ { \\top } ,
$$  
where $J$ is the Jacobian of the affine approximation of the projective transformation [10], [39]","[10] B. Kerbl, G. Kopanas, T. Leimk ¨uhler, and G. Drettakis, “3d gaussian splatting for real-time radiance field rendering,” ACM Trans. Graph., vol. 42, no. 4, 2023.;[39] M. Zwicker, H. Pfister, J. Van Baar, and M. Gross, “Ewa volume splatting,” in Proceedings Visualization, 2001. VIS’01., 2001, pp. 29– 538."
This is because its mappings are not affine and therefore cannot directly project $\\pmb { \\Sigma }$ . 3D GS adopts an affine one proposed in [39] which approximates the projective transformation using the first two terms (including $J$ ) of the Taylor expansion (see Sec. 4.4 in [39]),"[39] M. Zwicker, H. Pfister, J. Van Baar, and M. Gross, “Ewa volume splatting,” in Proceedings Visualization, 2001. VIS’01., 2001, pp. 29– 538."
"To avoid the cost computation of deriving Gaussians for each pixel, 3D GS shifts the precision from pixel-level to patch-level detail, which is inspired by tilebased rasterization [43]","[43] C. Lassner and M. Zollhofer, “Pulsar: Efficient sphere-based neural rendering,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2021, pp. 1440–1449."
Each tile comprises $1 6 \\times 1 6$ pixels as suggested in [10]. 3D GS further determines which tiles intersect with these projected Gaussians,"[10] B. Kerbl, G. Kopanas, T. Leimk ¨uhler, and G. Drettakis, “3d gaussian splatting for real-time radiance field rendering,” ACM Trans. Graph., vol. 42, no. 4, 2023."
"In the official implementation of the original paper [10], the framework regards the processing of tiles and pixels as analogous to the blocks and threads, respectively, in CUDA programming architecture","[10] B. Kerbl, G. Kopanas, T. Leimk ¨uhler, and G. Drettakis, “3d gaussian splatting for real-time radiance field rendering,” ACM Trans. Graph., vol. 42, no. 4, 2023."
Note that a good initialization is essential to convergence and reconstruction quality [44],"[44] K. Cheng, X. Long, K. Yang, Y. Yao, W. Yin, Y. Ma, W. Wang, and X. Chen, “Gaussianpro: 3d gaussian splatting with progressive propagation,” in Proc. ACM Int. Conf. Mach. Learn., 2024."
"These are: i) 3D GS for Sparse Input [45]–[55] (Sec. 4.1), ii) Memoryefficient 3D GS [56]–[64] (Sec. 4.2), iii) Photorealistic 3D GS [65]–[80] (Sec. 4.3), iv) Improved Optimization Algorithms [22], [77], [81]–[86] (Sec. 4.4), v) 3D Gaussian with More Properties [87]–[93] (Sec. 4.5), vi) Hybrid Representation [94]–[96] (Sec. 4.6), and vii) New Rendering Algorithm (Sec. 4.7)","[22] T. Lu, M. Yu, L. Xu, Y. Xiangli, L. Wang, D. Lin, and B. Dai, “Scaffold-gs: Structured 3d gaussians for view-adaptive rendering,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[45] H. Xiong, S. Muttukuru, R. Upadhyay, P. Chari, and A. Kadambi, “Sparsegs: Real-time 360 {\\deg} sparse view synthesis using gaussian splatting,” arXiv preprint arXiv:2312.00206, 2023.;[46] Z. Zhu, Z. Fan, Y. Jiang, and Z. Wang, “Fsgs: Real-time fewshot view synthesis using gaussian splatting,” in Proc. Eur. Conf. Comput. Vis., 2024.;[47] D. Charatan, S. Li, A. Tagliasacchi, and V. Sitzmann, “pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[48] S. Szymanowicz, C. Rupprecht, and A. Vedaldi, “Splatter image: Ultra-fast single-view 3d reconstruction,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[49] J. Li, J. Zhang, X. Bai, J. Zheng, X. Ning, J. Zhou, and L. Gu, “Dngaussian: Optimizing sparse-view 3d gaussian radiance fields with global-local depth normalization,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[50] A. Swann, M. Strong, W. K. Do, G. S. Camps, M. Schwager, and M. Kennedy III, “Touch-gs: Visual-tactile supervised 3d gaussian splatting,” arXiv preprint arXiv:2403.09875, 2024.;[51] Y. Chen, H. Xu, C. Zheng, B. Zhuang, M. Pollefeys, A. Geiger, T.-J. Cham, and J. Cai, “Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images,” in Proc. Eur. Conf. Comput. Vis., 2024.;[52] C. Wewer, K. Raj, E. Ilg, B. Schiele, and J. E. Lenssen, “latentsplat: Autoencoding variational gaussians for fast generalizable 3d reconstruction,” arXiv preprint arXiv:2403.16292, 2024.;[53] Y. Xu, Z. Shi, W. Yifan, H. Chen, C. Yang, S. Peng, Y. Shen, and G. Wetzstein, “Grm: Large gaussian reconstruction model for efficient 3d reconstruction and generation,” arXiv preprint arXiv:2403.14621, 2024.;[54] Q. Shen, X. Yi, Z. Wu, P. Zhou, H. Zhang, S. Yan, and X. Wang, “Gamba: Marry gaussian splatting with mamba for single view 3d reconstruction,” arXiv preprint arXiv:2403.18795, 2024.;[55] J. Zhang, J. Li, X. Yu, L. Huang, L. Gu, J. Zheng, and X. Bai, “Corgs: Sparse-view 3d gaussian splatting via co-regularization,” in Proc. Eur. Conf. Comput. Vis., 2024.;[56] Z. Fan, K. Wang, K. Wen, Z. Zhu, D. ${ \\tt X u } ,$ and Z. Wang, “Lightgaussian: Unbounded 3d gaussian compression with 15x reduction and $2 0 0 +$ fps,” arXiv preprint arXiv:2311.17245, 2023.;[57] K. Navaneet, K. P. Meibodi, S. A. Koohpayegani, and H. Pirsiavash, “Compact3d: Compressing gaussian splat radiance field models with vector quantization,” arXiv preprint arXiv:2311.18159, 2023.;[58] J. C. Lee, D. Rho, X. Sun, J. H. Ko, and E. Park, “Compact 3d gaussian representation for radiance field,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[59] W. Morgenstern, F. Barthel, A. Hilsmann, and P. Eisert, “Compact 3d scene representation via self-organizing gaussian grids,” arXiv preprint arXiv:2312.13299, 2023.;[60] X. Zhang, X. Ge, T. Xu, D. He, Y. Wang, H. Qin, G. Lu, J. Geng, and J. Zhang, “Gaussianimage: 1000 fps image representation and compression by 2d gaussian splatting,” in Proc. Eur. Conf. Comput. Vis., 2024.;[61] S. Niedermayr, J. Stumpfegger, and R. Westermann, “Compressed 3d gaussian splatting for accelerated novel view synthesis,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[62] Y. Chen, Q. Wu, J. Cai, M. Harandi, and W. Lin, “Hac: Hash-grid assisted context for 3d gaussian splatting compression,” in Proc. Eur. Conf. Comput. Vis., 2024.;[63] P. Papantonakis, G. Kopanas, B. Kerbl, A. Lanvin, and G. Drettakis, “Reducing the memory footprint of 3d gaussian splatting,” in I3D, 2024, pp. 1–17.;[64] G. Fang and B. Wang, “Mini-splatting: Representing scenes with a constrained number of gaussians,” arXiv preprint arXiv:2403.14166, 2024;[65] Z. Yu, A. Chen, B. Huang, T. Sattler, and A. Geiger, “Mipsplatting: Alias-free 3d gaussian splatting,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024, pp. 19 447–19 456.;[66] J. Gao, C. Gu, Y. Lin, H. Zhu, X. Cao, L. Zhang, and Y. Yao, “Relightable 3d gaussian: Real-time point cloud relighting with brdf decomposition and ray tracing,” arXiv preprint arXiv:2311.16043, 2023.;[67] Z. Yan, W. F. Low, Y. Chen, and G. H. Lee, “Multi-scale 3d gaussian splatting for anti-aliased rendering,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[68] Y. Jiang, J. Tu, Y. Liu, X. Gao, X. Long, W. Wang, and Y. Ma, “Gaussianshader: 3d gaussian splatting with shading functions for reflective surfaces,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[69] B. Lee, H. Lee, X. Sun, U. Ali, and E. Park, “Deblurring 3d gaussian splatting,” arXiv preprint arXiv:2401.00834, 2024.;[70] D. Malarz, W. Smolak, J. Tabor, S. Tadeja, and P. Spurek, “Gaussian splitting algorithm with color and opacity depended on viewing direction,” arXiv preprint arXiv:2312.13729, 2023.;[71] L. Bolanos, S.-Y. Su, and H. Rhodin, “Gaussian shadow casting for neural characters,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[72] L. Radl, M. Steiner, M. Parger, A. Weinrauch, B. Kerbl, and M. Steinberger, “Stopthepop: Sorted gaussian splatting for viewconsistent real-time rendering,” ACM Trans. Graph., 2024.;[73] Z. Yang, X. Gao, Y. Sun, Y. Huang, X. Lyu, W. Zhou, S. Jiao, X. $\\mathrm { \\ Q i , }$ and X. Jin, “Spec-gaussian: Anisotropic view-dependent appearance for 3d gaussian splatting,” arXiv preprint arXiv:2402.15870, 2024.;[74] C. Peng, Y. Tang, Y. Zhou, N. Wang, X. Liu, D. Li, and R. Chellappa, “Bags: Blur agnostic gaussian splatting through multiscale kernel modeling,” arXiv preprint arXiv:2403.04926, 2024.;[75] L. Zhao, P. Wang, and P. Liu, “Bad-gaussians: Bundle adjusted deblur gaussian splatting,” arXiv preprint arXiv:2403.11831, 2024.;[76] H. Dahmani, M. Bennehar, N. Piasco, L. Roldao, and D. Tsishkou, “Swag: Splatting in the wild images with appearanceconditioned gaussians,” arXiv preprint arXiv:2403.10427, 2024.;[77] Y. Li, C. Lyu, Y. Di, G. Zhai, G. H. Lee, and F. Tombari, “Geogaussian: Geometry-aware gaussian splatting for scene rendering,” arXiv preprint arXiv:2403.11324, 2024.;[78] Z. Liang, Q. Zhang, W. Hu, Y. Feng, L. Zhu, and K. Jia, “Analyticsplatting: Anti-aliased 3d gaussian splatting via analytic integration,” arXiv preprint arXiv:2403.11056, 2024.;[79] O. Seiskari, J. Ylilammi, V. Kaatrasalo, P. Rantalankila, M. Turkulainen, J. Kannala, E. Rahtu, and A. Solin, “Gaussian splatting on the move: Blur and rolling shutter compensation for natural camera motion,” arXiv preprint arXiv:2403.13327, 2024.;[80] X. Song, J. Zheng, S. Yuan, H.-a. Gao, J. Zhao, X. He, W. Gu, and H. Zhao, “Sa-gs: Scale-adaptive gaussian splatting for trainingfree anti-aliasing,” arXiv preprint arXiv:2403.19615, 2024.;[81] Y. Fu, S. Liu, A. Kulkarni, J. Kautz, A. A. Efros, and X. Wang, “Colmap-free 3d gaussian splatting,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[82] J. Jung, J. Han, H. An, J. Kang, S. Park, and S. Kim, “Relaxing accurate initialization constraint for 3d gaussian splatting,” arXiv preprint arXiv:2403.09413, 2024.;[83] M. Yu, T. Lu, L. Xu, L. Jiang, Y. Xiangli, and B. Dai, “Gsdf: 3dgs meets sdf for improved rendering and reconstruction,” arXiv preprint arXiv:2403.16964, 2024.;[84] J. Zhang, F. Zhan, M. Xu, S. Lu, and E. Xing, “Fregs: 3d gaussian splatting with progressive frequency regularization,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[85] L. Huang, J. Bai, J. Guo, and Y. Guo, $^ { \\prime \\prime } G s { + + }$ : Error analyzing and optimal gaussian splatting,” arXiv preprint arXiv:2402.00752, 2024.;[86] J. Li, L. Cheng, Z. Wang, T. Mu, and J. He, “Loopgaussian: Creating 3d cinemagraph with multi-view images via eulerian motion field,” arXiv preprint arXiv:2404.08966, 2024.;[87] J.-C. Shi, M. Wang, H.-B. Duan, and S.-H. Guan, “Language embedded 3d gaussians for open-vocabulary scene understanding,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[88] M. Qin, W. Li, J. Zhou, H. Wang, and H. Pfister, “Langsplat: 3d language gaussian splatting,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[89] X. Zuo, P. Samangouei, Y. Zhou, Y. Di, and M. Li, “Fmgs: Foundation model embedded 3d gaussian splatting for holistic 3d scene understanding,” arXiv preprint arXiv:2401.01970, 2024.;[90] S. Zhou, H. Chang, S. Jiang, Z. Fan, Z. Zhu, D. Xu, P. Chari, S. You, Z. Wang, and A. Kadambi, “Feature 3dgs: Supercharging 3d gaussian splatting to enable distilled feature fields,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[91] M. Ye, M. Danelljan, F. Yu, and L. Ke, “Gaussian grouping: Segment and edit anything in 3d scenes,” in Proc. Eur. Conf. Comput. Vis., 2024.;[92] J. Cen, J. Fang, C. Yang, L. Xie, X. Zhang, W. Shen, and Q. Tian, “Segment any 3d gaussians,” arXiv preprint arXiv:2312.00860, 2023.;[93] Z. Yang, H. Yang, Z. Pan, X. Zhu, and L. Zhang, “Real-time photorealistic dynamic scene representation and rendering with 4d gaussian splatting,” in Proc. Int. Conf. Learn. Represent., 2024.;[94] Z. Yang, X. Gao, W. Zhou, S. Jiao, Y. Zhang, and X. Jin, “Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[95] G. Wu, T. Yi, J. Fang, L. Xie, X. Zhang, W. Wei, W. Liu, Q. Tian, and X. Wang, $^ { \\prime \\prime } { 4 \\mathrm { d } }$ gaussian splatting for real-time dynamic scene rendering,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[96] Y. Xu, B. Chen, Z. Li, H. Zhang, L. Wang, Z. Zheng, and Y. Liu, “Gaussian head avatar: Ultra high-fidelity head avatar via dynamic gaussians,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024."
"Existing methods can be categorized into two primary groups. i) Regularization based methods introduce additional constraints such as depth information to enhance the detail and global consistency [46], [49], [51], [55]","[46] Z. Zhu, Z. Fan, Y. Jiang, and Z. Wang, “Fsgs: Real-time fewshot view synthesis using gaussian splatting,” in Proc. Eur. Conf. Comput. Vis., 2024.;[49] J. Li, J. Zhang, X. Bai, J. Zheng, X. Ning, J. Zhou, and L. Gu, “Dngaussian: Optimizing sparse-view 3d gaussian radiance fields with global-local depth normalization,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[51] Y. Chen, H. Xu, C. Zheng, B. Zhuang, M. Pollefeys, A. Geiger, T.-J. Cham, and J. Cai, “Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images,” in Proc. Eur. Conf. Comput. Vis., 2024.;[55] J. Zhang, J. Li, X. Yu, L. Huang, L. Gu, J. Zheng, and X. Bai, “Corgs: Sparse-view 3d gaussian splatting via co-regularization,” in Proc. Eur. Conf. Comput. Vis., 2024."
"For example, DNGaussian [49] introduced a depth-regularized approach to address the challenge of geometry degradation in sparse input","[49] J. Li, J. Zhang, X. Bai, J. Zheng, X. Ning, J. Zhou, and L. Gu, “Dngaussian: Optimizing sparse-view 3d gaussian radiance fields with global-local depth normalization,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024."
FSGS [46] devised a Gaussian Unpooling process for initialization and also introduced depth regularization,"[46] Z. Zhu, Z. Fan, Y. Jiang, and Z. Wang, “Fsgs: Real-time fewshot view synthesis using gaussian splatting,” in Proc. Eur. Conf. Comput. Vis., 2024."
MVSplat [51] proposed a cost volume representation so as to provide geometry cues,"[51] Y. Chen, H. Xu, C. Zheng, B. Zhuang, M. Pollefeys, A. Geiger, T.-J. Cham, and J. Cai, “Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images,” in Proc. Eur. Conf. Comput. Vis., 2024."
"Unfortunately, when dealing with a limited number of views, or even just one, the efficacy of regularization techniques tends to diminish, which leads to ii) generalizability based methods that use learned priors [47], [48], [53], [97]","[47] D. Charatan, S. Li, A. Tagliasacchi, and V. Sitzmann, “pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[48] S. Szymanowicz, C. Rupprecht, and A. Vedaldi, “Splatter image: Ultra-fast single-view 3d reconstruction,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[53] Y. Xu, Z. Shi, W. Yifan, H. Chen, C. Yang, S. Peng, Y. Shen, and G. Wetzstein, “Grm: Large gaussian reconstruction model for efficient 3d reconstruction and generation,” arXiv preprint arXiv:2403.14621, 2024.;[97] S. Szymanowicz, E. Insafutdinov, C. Zheng, D. Campbell, J. F. Henriques, C. Rupprecht, and A. Vedaldi, “Flash3d: Feedforward generalisable 3d scene reconstruction from a single image,” arXiv preprint arXiv:2406.04343, 2024."
"One approach involves synthesizing additional views through generative models, which can be seamlessly integrated into existing reconstruction pipelines [98]","[98] K. Sargent, Z. Li, T. Shah, C. Herrmann, H.-X. Yu, Y. Zhang, E. R. Chan, D. Lagun, L. Fei-Fei, D. Sun et al., “Zeronvs: Zero-shot 360- degree view synthesis from a single image,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024, pp. 9420–9429."
"For instance, PixelSplat [47] proposed to sample Gaussians from dense probability distributions","[47] D. Charatan, S. Li, A. Tagliasacchi, and V. Sitzmann, “pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024."
Splatter Image [48] introduced a 2D image-to-image network that maps an input image to a 3D Gaussian per pixel,"[48] S. Szymanowicz, C. Rupprecht, and A. Vedaldi, “Splatter image: Ultra-fast single-view 3d reconstruction,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024."
"First, several approaches focus on reducing the number of 3D Gaussians [58], [62], [63]","[58] J. C. Lee, D. Rho, X. Sun, J. H. Ko, and E. Park, “Compact 3d gaussian representation for radiance field,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[62] Y. Chen, Q. Wu, J. Cai, M. Harandi, and W. Lin, “Hac: Hash-grid assisted context for 3d gaussian splatting compression,” in Proc. Eur. Conf. Comput. Vis., 2024.;[63] P. Papantonakis, G. Kopanas, B. Kerbl, A. Lanvin, and G. Drettakis, “Reducing the memory footprint of 3d gaussian splatting,” in I3D, 2024, pp. 1–17."
"These methods either employ strategic pruning of lowimpact Gaussians, such as the volume-based masking [58], or represent neighboring Gaussians using the same properties stored within a “local anchor” obtained by clustering [22], hash-grid [62], etc","[22] T. Lu, M. Yu, L. Xu, Y. Xiangli, L. Wang, D. Lin, and B. Dai, “Scaffold-gs: Structured 3d gaussians for view-adaptive rendering,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[58] J. C. Lee, D. Rho, X. Sun, J. H. Ko, and E. Park, “Compact 3d gaussian representation for radiance field,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[62] Y. Chen, Q. Wu, J. Cai, M. Harandi, and W. Lin, “Hac: Hash-grid assisted context for 3d gaussian splatting compression,” in Proc. Eur. Conf. Comput. Vis., 2024."
"Second, researchers have developed methods for compressing Gaussian’s properties [58], [61], [62]","[58] J. C. Lee, D. Rho, X. Sun, J. H. Ko, and E. Park, “Compact 3d gaussian representation for radiance field,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[61] S. Niedermayr, J. Stumpfegger, and R. Westermann, “Compressed 3d gaussian splatting for accelerated novel view synthesis,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[62] Y. Chen, Q. Wu, J. Cai, M. Harandi, and W. Lin, “Hac: Hash-grid assisted context for 3d gaussian splatting compression,” in Proc. Eur. Conf. Comput. Vis., 2024."
"For instance, Niedermayr et al. [61] compressed color and Gaussian parameters into compact codebooks, using sensitivity measures for effective quantization and fine-tuning","[61] S. Niedermayr, J. Stumpfegger, and R. Westermann, “Compressed 3d gaussian splatting for accelerated novel view synthesis,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024."
HAC [62] predicted the probability of each quantized attribute using Gaussian distributions and then devise an adaptive quantization module,"[62] Y. Chen, Q. Wu, J. Cai, M. Harandi, and W. Lin, “Hac: Hash-grid assisted context for 3d gaussian splatting compression,” in Proc. Eur. Conf. Comput. Vis., 2024."
"For instance, the simple visibility algorithm may lead to a drastic switch in the depth/blending order of Gaussians [10]","[10] B. Kerbl, G. Kopanas, T. Leimk ¨uhler, and G. Drettakis, “3d gaussian splatting for real-time radiance field rendering,” ACM Trans. Graph., vol. 42, no. 4, 2023."
"Researchers developed training-time improvements from the sampling rate perspective and introduced schemes such as multi-scale Gaussians [67], 2D Mip filter [65], and conditioned logistic function [78]","[65] Z. Yu, A. Chen, B. Huang, T. Sattler, and A. Geiger, “Mipsplatting: Alias-free 3d gaussian splatting,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024, pp. 19 447–19 456.;[67] Z. Yan, W. F. Low, Y. Chen, and G. H. Lee, “Multi-scale 3d gaussian splatting for anti-aliased rendering,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[78] Z. Liang, Q. Zhang, W. Hu, Y. Feng, L. Zhu, and K. Jia, “Analyticsplatting: Anti-aliased 3d gaussian splatting via analytic integration,” arXiv preprint arXiv:2403.11056, 2024."
"Inference-time solutions, such as 2D scaleadaptive filtering [80], offer enhanced fidelity that can be integrated into any existing 3D GS frameworks. ii) Reflection","[80] X. Song, J. Zheng, S. Yuan, H.-a. Gao, J. Zhao, X. He, W. Gu, and H. Zhao, “Sa-gs: Scale-adaptive gaussian splatting for trainingfree anti-aliasing,” arXiv preprint arXiv:2403.19615, 2024."
"Recent works have introduced various approaches to model reflective materials [68], [73], [99] and enable relightable Gaussian representation [23], though achieving physically accurate specular effects remains challenging. iii) Blur","[23] S. Saito, G. Schwartz, T. Simon, J. Li, and G. Nam, “Relightable gaussian codec avatars,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[68] Y. Jiang, J. Tu, Y. Liu, X. Gao, X. Long, W. Wang, and Y. Ma, “Gaussianshader: 3d gaussian splatting with shading functions for reflective surfaces,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[73] Z. Yang, X. Gao, Y. Sun, Y. Huang, X. Lyu, W. Zhou, S. Jiao, X. $\\mathrm { \\ Q i , }$ and X. Jin, “Spec-gaussian: Anisotropic view-dependent appearance for 3d gaussian splatting,” arXiv preprint arXiv:2402.15870, 2024.;[99] J. Meng, H. Li, Y. Wu, Q. Gao, S. Yang, J. Zhang, and S. Ma, “Mirror-3dgs: Incorporating mirror reflections into 3d gaussian splatting,” arXiv preprint arXiv:2404.01168, 2024."
"Recent approaches explicitly incorporated blur modeling during training, employing techniques such as coarse-to-fine kernel optimization [74] and photometric bundle adjustment [75] to address this challenge","[74] C. Peng, Y. Tang, Y. Zhou, N. Wang, X. Liu, D. Li, and R. Chellappa, “Bags: Blur agnostic gaussian splatting through multiscale kernel modeling,” arXiv preprint arXiv:2403.04926, 2024.;[75] L. Zhao, P. Wang, and P. Liu, “Bad-gaussians: Bundle adjusted deblur gaussian splatting,” arXiv preprint arXiv:2403.11831, 2024."
"Three main directions stand out for improving the optimization of 3D GS. i) Additional Regularization (e.g., frequency [84] and geometry [22], [77])","[22] T. Lu, M. Yu, L. Xu, Y. Xiangli, L. Wang, D. Lin, and B. Dai, “Scaffold-gs: Structured 3d gaussians for view-adaptive rendering,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[77] Y. Li, C. Lyu, Y. Di, G. Zhai, G. H. Lee, and F. Tombari, “Geogaussian: Geometry-aware gaussian splatting for scene rendering,” arXiv preprint arXiv:2403.11324, 2024.;[84] J. Zhang, F. Zhan, M. Xu, S. Lu, and E. Xing, “Fregs: 3d gaussian splatting with progressive frequency regularization,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024."
"Geometry-aware approaches have been particularly successful, preserving scene structure through the incorporation of local anchor points [22], depth and surface constraints [100]–[102], Gaussian volumes [103], etc. ii) Optimization Procedure Enhancement [44], [101], [104]","[22] T. Lu, M. Yu, L. Xu, Y. Xiangli, L. Wang, D. Lin, and B. Dai, “Scaffold-gs: Structured 3d gaussians for view-adaptive rendering,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[44] K. Cheng, X. Long, K. Yang, Y. Yao, W. Yin, Y. Ma, W. Wang, and X. Chen, “Gaussianpro: 3d gaussian splatting with progressive propagation,” in Proc. ACM Int. Conf. Mach. Learn., 2024.;[100] H. Chen, C. Li, and G. H. Lee, “Neusg: Neural implicit surface reconstruction with 3d gaussian splatting guidance,” arXiv preprint arXiv:2312.00846, 2023.;[101] Z. Yu, T. Sattler, and A. Geiger, “Gaussian opacity fields: Efficient and compact surface reconstruction in unbounded scenes,” arXiv preprint arXiv:2404.10772, 2024.;[102] B. Zhang, C. Fang, R. Shrestha, Y. Liang, X. Long, and P. Tan, “Rade-gs: Rasterizing depth in gaussian splatting,” arXiv preprint arXiv:2406.01467, 2024.;[103] A. Chen, H. Xu, S. Esposito, S. Tang, and A. Geiger, “Lara: Efficient large-baseline radiance fields,” arXiv preprint arXiv:2407.04699, 2024.;[104] E. Ververas, R. A. Potamias, J. Song, J. Deng, and S. Zafeiriou, “Sags: Structure-aware 3d gaussian splatting,” in Proc. Eur. Conf. Comput. Vis., 2024, pp. 221–238."
"For example, GaussianPro [44] addresses the challenge of dense initialization in texture-less surfaces and large-scale scenes through an advanced Gaussian densification strategy. iii) Constraint  
Relaxation","[44] K. Cheng, X. Long, K. Yang, Y. Yao, W. Yin, Y. Ma, W. Wang, and X. Chen, “Gaussianpro: 3d gaussian splatting with progressive propagation,” in Proc. ACM Int. Conf. Mach. Learn., 2024."
"Recent works have begun exploring COLMAP-free approaches utilizing stream continuity [81], [105], potentially enabling learning from internet-scale unposed video datasets","[81] Y. Fu, S. Liu, A. Kulkarni, J. Kautz, A. A. Efros, and X. Wang, “Colmap-free 3d gaussian splatting,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[105] C. Smith, D. Charatan, A. Tewari, and V. Sitzmann, “Flowmap: High-quality camera poses, intrinsics, and depth via gradient descent,” arXiv preprint arXiv:2404.15259, 2024."
"By augmenting 3D Gaussian with additional properties, such as linguistic [87]–[89], semantic/instance [90]–[92], and spatialtemporal [93] properties, 3D GS demonstrates its considerable potential to revolutionize various domains","[87] J.-C. Shi, M. Wang, H.-B. Duan, and S.-H. Guan, “Language embedded 3d gaussians for open-vocabulary scene understanding,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[88] M. Qin, W. Li, J. Zhou, H. Wang, and H. Pfister, “Langsplat: 3d language gaussian splatting,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[89] X. Zuo, P. Samangouei, Y. Zhou, Y. Di, and M. Li, “Fmgs: Foundation model embedded 3d gaussian splatting for holistic 3d scene understanding,” arXiv preprint arXiv:2401.01970, 2024.;[90] S. Zhou, H. Chang, S. Jiang, Z. Fan, Z. Zhu, D. Xu, P. Chari, S. You, Z. Wang, and A. Kadambi, “Feature 3dgs: Supercharging 3d gaussian splatting to enable distilled feature fields,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[91] M. Ye, M. Danelljan, F. Yu, and L. Ke, “Gaussian grouping: Segment and edit anything in 3d scenes,” in Proc. Eur. Conf. Comput. Vis., 2024.;[92] J. Cen, J. Fang, C. Yang, L. Xie, X. Zhang, W. Shen, and Q. Tian, “Segment any 3d gaussians,” arXiv preprint arXiv:2312.00860, 2023.;[93] Z. Yang, H. Yang, Z. Pan, X. Zhu, and L. Zhang, “Real-time photorealistic dynamic scene representation and rendering with 4d gaussian splatting,” in Proc. Int. Conf. Learn. Represent., 2024."
Here we list several interesting applications using 3D Gaussians with specially designed properties. i) Language Embedded Scene Representation [87]–[89],"[87] J.-C. Shi, M. Wang, H.-B. Duan, and S.-H. Guan, “Language embedded 3d gaussians for open-vocabulary scene understanding,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[88] M. Qin, W. Li, J. Zhou, H. Wang, and H. Pfister, “Langsplat: 3d language gaussian splatting,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[89] X. Zuo, P. Samangouei, Y. Zhou, Y. Di, and M. Li, “Fmgs: Foundation model embedded 3d gaussian splatting for holistic 3d scene understanding,” arXiv preprint arXiv:2401.01970, 2024."
"Due to the high computational and memory demands of current languageembedded scene representations, Shi et al. [87] proposed a quantization scheme that augments 3D Gaussian with streamlined language embeddings instead of the original high-dimensional embeddings","[87] J.-C. Shi, M. Wang, H.-B. Duan, and S.-H. Guan, “Language embedded 3d gaussians for open-vocabulary scene understanding,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024."
"This method also mitigated semantic ambiguity and enhanced the precision of open-vocabulary querying by smoothing out semantic features across different views, guided by uncertainty values. ii) Scene Understanding and Editing [90]–[92]","[90] S. Zhou, H. Chang, S. Jiang, Z. Fan, Z. Zhu, D. Xu, P. Chari, S. You, Z. Wang, and A. Kadambi, “Feature 3dgs: Supercharging 3d gaussian splatting to enable distilled feature fields,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[91] M. Ye, M. Danelljan, F. Yu, and L. Ke, “Gaussian grouping: Segment and edit anything in 3d scenes,” in Proc. Eur. Conf. Comput. Vis., 2024.;[92] J. Cen, J. Fang, C. Yang, L. Xie, X. Zhang, W. Shen, and Q. Tian, “Segment any 3d gaussians,” arXiv preprint arXiv:2312.00860, 2023."
Feature 3DGS [90] integrated 3D GS with feature field distillation from 2D foundation models,"[90] S. Zhou, H. Chang, S. Jiang, Z. Fan, Z. Zhu, D. Xu, P. Chari, S. You, Z. Wang, and A. Kadambi, “Feature 3dgs: Supercharging 3d gaussian splatting to enable distilled feature fields,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024."
"By learning a lowerdimensional feature field and applying a lightweight convolutional decoder for upsampling, Feature 3DGS achieved faster training and rendering speeds while enabling highquality feature field distillation, supporting applications like semantic segmentation and language-guided editing. iii) Spatiotemporal Modeling [93], [106]","[93] Z. Yang, H. Yang, Z. Pan, X. Zhu, and L. Zhang, “Real-time photorealistic dynamic scene representation and rendering with 4d gaussian splatting,” in Proc. Int. Conf. Learn. Represent., 2024.;[106] Y. Lin, Z. Dai, S. Zhu, and Y. Yao, “Gaussian-flow: 4d reconstruction with dynamic 3d gaussian particle,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024."
"To capture the complex spatial and temporal dynamics of 3D scenes, Yang et al. [93] conceptualized spacetime as a unified entity and approximates the spatiotemporal volume of dynamic scenes using a collection of 4D Gaussians","[93] Z. Yang, H. Yang, Z. Pan, X. Zhu, and L. Zhang, “Real-time photorealistic dynamic scene representation and rendering with 4d gaussian splatting,” in Proc. Int. Conf. Learn. Represent., 2024."
"Considering the challenge of creating high-fidelity 3D head avatars under sparse view conditions, Gaussian Head Avatar [96] introduced controllable 3D Gaussians and an MLP-based deformation field","[96] Y. Xu, B. Chen, Z. Li, H. Zhang, L. Wang, Z. Zheng, and Y. Liu, “Gaussian head avatar: Ultra high-fidelity head avatar via dynamic gaussians,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024."
Yang et al. [94] proposed to reconstruct dynamic scenes with deformable 3D Gaussians,"[94] Z. Yang, X. Gao, W. Zhou, S. Jiao, Y. Zhang, and X. Jin, “Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024."
"Saroha et al. [107] proposed GS in style, an advanced approach for real-time neural scene stylization","[107] A. Saroha, M. Gladkova, C. Curreli, T. Yenamandra, and D. Cremers, “Gaussian splatting in style,” arXiv preprint arXiv:2403.08498, 2024."
Recent works [108]–[110] explored ray tracing based rendering algorithms as an alternative,"[108] N. Moenne-Loccoz, A. Mirzaei, O. Perel, R. de Lutio, J. Martinez Esturo, G. State, S. Fidler, N. Sharp, and Z. Gojcic, “3d gaussian ray tracing: Fast tracing of particle scenes,” ACM Trans. Graph., vol. 43, no. 6, pp. 1–19, 2024.;[109] A. Mai, P. Hedman, G. Kopanas, D. Verbin, D. Futschik, Q. Xu, F. Kuester, J. T. Barron, and Y. Zhang, “Ever: Exact volumetric ellipsoid rendering for real-time view synthesis,” arXiv preprint arXiv:2410.01804, 2024.;[110] J. Condor, S. Speierer, L. Bode, A. Bozic, S. Green, P. Didyk, and A. Jarabo, “Don’t splat your gaussians: Volumetric ray-traced primitives for modeling and rendering scattering and emissive media,” ACM Trans. Graph., 2025."
"For instance, GaussianTracer [108] introduced a new ray tracing implementation for Gaussian primitives, and devised several accelerating strategies according to the uneven density and interleaved nature of Gaussians","[108] N. Moenne-Loccoz, A. Mirzaei, O. Perel, R. de Lutio, J. Martinez Esturo, G. State, S. Fidler, N. Sharp, and Z. Gojcic, “3d gaussian ray tracing: Fast tracing of particle scenes,” ACM Trans. Graph., vol. 43, no. 6, pp. 1–19, 2024."
"EVER [109] deivsed a physically accurate, constant density ellipsoid representation that allows for the exact computation of the volume rendering integral, rather than relying on somewhat satisfactory approximations","[109] A. Mai, P. Hedman, G. Kopanas, D. Verbin, D. Futschik, Q. Xu, F. Kuester, J. T. Barron, and Y. Zhang, “Ever: Exact volumetric ellipsoid rendering for real-time view synthesis,” arXiv preprint arXiv:2410.01804, 2024."
"Building on the rapid advancements in 3D GS, a wide range of innovative applications has emerged across multiple domains (Fig. 6) such as robotics (Sec. 5.1), dynamic scene reconstruction and representation (Sec. 5.2), generation and editing (Sec. 5.3), avatar (Sec. 5.4), medical systems (Sec. 5.5), large-scale scene reconstruction (Sec. 5.6), physics (Sec. 5.7), and even other scientific disciplines [24], [174]–[176]","[24] T. Zhang, K. Huang, W. Zhi, and M. Johnson-Roberson, “Darkgs: Learning neural illumination and 3d gaussians relighting for robotic exploration in the dark,” in Proc. IEEE/RSJ Int. Conf. Intell. Robot. Syst., 2024.;[174] S. Zhang, H. Zhao, Z. Zhou, G. Wu, C. Zheng, X. Wang, and W. Liu, “Togs: Gaussian splatting with temporal opacity offset for real-time 4d dsa rendering,” arXiv preprint arXiv:2403.19586, 2024.;[175] R. Wu, Z. Zhang, Y. Yang, and W. Zuo, “Dual-camera smooth zoom on mobile phones,” arXiv preprint arXiv:2404.04908, 2024.;[176] H. Li, Y. Gao, D. Zhang, C. Wu, Y. Dai, C. Zhao, H. Feng, E. Ding, J. Wang, and J. Han, “Ggrt: Towards generalizable 3d gaussians without pose priors in real-time,” arXiv preprint arXiv:2403.10147, 2024."
"In SLAM, GS-based methods [111]–[117], [123], [124], [177]– [182] excel in real-time dense mapping but face inherent trade-offs","[111] C. Yan, D. Qu, D. Wang, D. Xu, Z. Wang, B. Zhao, and X. Li, “Gs-slam: Dense visual slam with 3d gaussian splatting,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[112] N. Keetha, J. Karhade, K. M. Jatavallabhula, G. Yang, S. Scherer, D. Ramanan, and J. Luiten, “Splatam: Splat, track & map 3d Pattern Recognit., 2024.;[113] H. Matsuki, R. Murai, P. H. Kelly, and A. J. Davison, “Gaussian splatting slam,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[114] V. Yugay, Y. Li, T. Gevers, and M. R. Oswald, “Gaussian-slam: Photo-realistic dense slam with gaussian splatting,” arXiv preprint arXiv:2312.10070, 2023.;[115] H. Huang, L. Li, H. Cheng, and S.-K. Yeung, “Photo-slam: Realtime simultaneous localization and photorealistic mapping for monocular, stereo, and rgb-d cameras,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[116] M. Li, S. Liu, and H. Zhou, “Sgs-slam: Semantic gaussian splatting for neural dense slam,” in Proc. Eur. Conf. Comput. Vis., 2024.;[117] Y. Ji, Y. Liu, G. Xie, B. Ma, and Z. Xie, “Neds-slam: A novel neural explicit dense semantic slam framework using 3d gaussian splatting,” arXiv preprint arXiv:2403.11679, 2024.;[123] S. Zhu, R. Qin, G. Wang, J. Liu, and H. Wang, “Semgaussslam: Dense semantic gaussian splatting slam,” arXiv preprint arXiv:2403.07494, 2024.;[124] Z. Peng, T. Shao, Y. Liu, J. Zhou, Y. Yang, J. Wang, and K. Zhou, “Rtg-slam: Real-time 3d reconstruction at scale using gaussian splatting,” ACM Trans. Graph., 2024.;[177] S. Hong, J. He, X. Zheng, H. Wang, H. Fang, K. Liu, C. Zheng, and S. Shen, “Liv-gaussmap: Lidar-inertial-visual fusion for real-time 3d radiance field map rendering,” arXiv preprint arXiv:2401.14857, 2024.;[178] S. Sun, M. Mielle, A. J. Lilienthal, and M. Magnusson, “Highfidelity slam using gaussian splatting with rendering-guided densification and regularized optimization,” in Proc. IEEE/RSJ Int. Conf. Intell. Robot. Syst., 2024.;[179] F. Tosi, Y. Zhang, Z. Gong, E. Sandstro¨m, S. Mattoccia, M. R. Oswald, and M. Poggi, “How nerfs and 3d gaussian splatting are reshaping slam: a survey,” arXiv preprint arXiv:2402.13255, 2024. W. Chen, “Compact 3d gaussian splatting for dense visual slam,” arXiv preprint arXiv:2403.11247, 2024.;[181] J. Hu, X. Chen, B. Feng, G. Li, L. Yang, H. Bao, G. Zhang, and Z. Cui, “Cg-slam: Efficient dense rgb-d slam in a consistent uncertainty-aware 3d gaussian field,” arXiv preprint arXiv:2403.16095, 2024.;[182] X. Lang, L. Li, H. Zhang, F. Xiong, M. Xu, Y. Liu, X. Zuo, and J. Lv, “Gaussian-lic: Photo-realistic lidar-inertial-camera slam with 3d gaussian splatting,” arXiv preprint arXiv:2404.06926, 2024."
"Visual SLAM frameworks, particularly RGBD variants [112], [114], [178], leverage depth supervision for geometric fidelity but falter in low-texture or motiondegraded environments","[112] N. Keetha, J. Karhade, K. M. Jatavallabhula, G. Yang, S. Scherer, D. Ramanan, and J. Luiten, “Splatam: Splat, track & map 3d Pattern Recognit., 2024.;[114] V. Yugay, Y. Li, T. Gevers, and M. R. Oswald, “Gaussian-slam: Photo-realistic dense slam with gaussian splatting,” arXiv preprint arXiv:2312.10070, 2023.;[178] S. Sun, M. Mielle, A. J. Lilienthal, and M. Magnusson, “Highfidelity slam using gaussian splatting with rendering-guided densification and regularized optimization,” in Proc. IEEE/RSJ Int. Conf. Intell. Robot. Syst., 2024."
"RGB-only approaches [113], [115], [183] circumvent depth sensors but grapple with scale ambiguity and drift","[113] H. Matsuki, R. Murai, P. H. Kelly, and A. J. Davison, “Gaussian splatting slam,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[115] H. Huang, L. Li, H. Cheng, and S.-K. Yeung, “Photo-slam: Realtime simultaneous localization and photorealistic mapping for monocular, stereo, and rgb-d cameras,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[183] E. Sandstr¨om, K. Tateno, M. Oechsle, M. Niemeyer, L. Van Gool, M. R. Oswald, and F. Tombari, “Splat-slam: Globally optimized rgb-only slam with 3d gaussians,” arXiv preprint arXiv:2405.16544, 2024."
"Multi-sensor fusion strategies, such as LiDAR integration [159], [177], [182], enhance robustness in unstructured settings at the cost of calibration complexity","[159] C. Wu, Y. Duan, X. Zhang, Y. Sheng, J. Ji, and Y. Zhang, “Mmgaussian: 3d gaussian-based multi-modal fusion for localization and reconstruction in unbounded scenes,” in Proc. IEEE/RSJ Int. Conf. Intell. Robot. Syst., 2024.;[177] S. Hong, J. He, X. Zheng, H. Wang, H. Fang, K. Liu, C. Zheng, and S. Shen, “Liv-gaussmap: Lidar-inertial-visual fusion for real-time 3d radiance field map rendering,” arXiv preprint arXiv:2401.14857, 2024.;[182] X. Lang, L. Li, H. Zhang, F. Xiong, M. Xu, Y. Liu, X. Zuo, and J. Lv, “Gaussian-lic: Photo-realistic lidar-inertial-camera slam with 3d gaussian splatting,” arXiv preprint arXiv:2404.06926, 2024."
"Semantic SLAM [116], [117], [123] extends scene understanding through object-level semantics but struggles with scalability due to lighting sensitivity in color-based methods or computational overhead in feature-based methods. 3D GS based manipulation [118]–[122] bypasses the need for auxiliary pose estimation in NeRF-based methods, enabling rapid single-stage tasks like grasping in static environments via geometric and semantic attributes encoded in Gaussian properties","[116] M. Li, S. Liu, and H. Zhou, “Sgs-slam: Semantic gaussian splatting for neural dense slam,” in Proc. Eur. Conf. Comput. Vis., 2024.;[117] Y. Ji, Y. Liu, G. Xie, B. Ma, and Z. Xie, “Neds-slam: A novel neural explicit dense semantic slam framework using 3d gaussian splatting,” arXiv preprint arXiv:2403.11679, 2024.;[118] G. Lu, S. Zhang, Z. Wang, C. Liu, J. Lu, and Y. Tang, “Manigaussian: Dynamic gaussian splatting for multi-task robotic manipulation,” arXiv preprint arXiv:2403.08321, 2024.;[119] J. Abou-Chakra, K. Rana, F. Dayoub, and N. S ¨underhauf, “Physically embodied gaussian splatting: A realtime correctable world model for robotics,” in Proc. Annu. Conf. Robot Learn., 2024.;[120] O. Shorinwa, J. Tucker, A. Smith, A. Swann, T. Chen, R. Firoozi, M. D. Kennedy, and M. Schwager, “Splat-mover: Multi-stage, open-vocabulary robotic manipulation via editable gaussian splatting,” in Proc. Annu. Conf. Robot Learn., 2024.;[121] M. Ji, R.-Z. Qiu, X. Zou, and X. Wang, “Graspsplats: Efficient manipulation with 3d feature splatting,” arXiv preprint arXiv:2409.02084, 2024.;[122] Y. Zheng, X. Chen, Y. Zheng, S. Gu, R. Yang, B. Jin, P. Li, C. Zhong, Z. Wang, L. Liu et al., “Gaussiangrasper: 3d language gaussian splatting for open-vocabulary robotic grasping,” arXiv preprint arXiv:2403.09637, 2024.;[123] S. Zhu, R. Qin, G. Wang, J. Liu, and H. Wang, “Semgaussslam: Dense semantic gaussian splatting slam,” arXiv preprint arXiv:2403.07494, 2024."
"Multi-stage manipulation [118], [120], where environmental dynamics demand real-time map updates, requires explicit modeling of dynamic adjustments (e.g., object motions and interactions), material compliance, etc","[118] G. Lu, S. Zhang, Z. Wang, C. Liu, J. Lu, and Y. Tang, “Manigaussian: Dynamic gaussian splatting for multi-task robotic manipulation,” arXiv preprint arXiv:2403.08321, 2024.;[120] O. Shorinwa, J. Tucker, A. Smith, A. Swann, T. Chen, R. Firoozi, M. D. Kennedy, and M. Schwager, “Splat-mover: Multi-stage, open-vocabulary robotic manipulation via editable gaussian splatting,” in Proc. Annu. Conf. Robot Learn., 2024."
"Some images are borrowed from [132], [135], [146], [154], [160], [166] and redrawn  
Dynamic scene reconstruction refers to the process of capturing and representing the three-dimensional structure and appearance of a scene that changes over time [184]–[187]","[132] Z. Lu, X. Guo, L. Hui, T. Chen, M. Yang, X. Tang, F. Zhu, and Y. Dai, $"" 3 \\mathrm { d }$ geometry-aware deformable gaussian splatting for dynamic view synthesis,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[135] J. Tang, Z. Chen, X. Chen, T. Wang, G. Zeng, and Z. Liu, “Lgm: Large multi-view gaussian model for high-resolution 3d content creation,” in Proc. Eur. Conf. Comput. Vis., 2024.;[146] J. Lei, Y. Wang, G. Pavlakos, L. Liu, and K. Daniilidis, “Gart: Gaussian articulated template models,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[154] L. Zhu, Z. Wang, Z. Jin, G. Lin, and L. Yu, “Deformable endoscopic tissues reconstruction with gaussian splatting,” arXiv preprint arXiv:2401.11535, 2024.;[160] B. Xiong, Z. Li, and Z. Li, “Gauu-scene: A scene reconstruction benchmark on large scale 3d reconstruction dataset using gaussian splatting,” arXiv preprint arXiv:2401.14032, 2024.;[166] T. Xie, Z. Zong, Y. Qiu, X. Li, Y. Feng, Y. Yang, and C. Jiang, “Physgaussian: Physics-integrated 3d gaussians for generative dynamics,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[184] A. Pumarola, E. Corona, G. Pons-Moll, and F. Moreno-Noguer, “D-nerf: Neural radiance fields for dynamic scenes,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2021, pp. 10 318–10 327.;[185] K. Park, U. Sinha, P. Hedman, J. T. Barron, S. Bouaziz, D. B. Goldman, R. Martin-Brualla, and S. M. Seitz, “Hypernerf: a higher-dimensional representation for topologically varying neural radiance fields,” ACM Trans. Graph., vol. 40, no. 6, pp. 1–12, 2021.;[186] K. Park, U. Sinha, J. T. Barron, S. Bouaziz, D. B. Goldman, S. M. Seitz, and R. Martin-Brualla, “Nerfies: Deformable neural radiance fields,” in Proc. IEEE Int. Conf. Comput. Vis., 2021, pp. 5865–5874.;[187] X. Guo, J. Sun, Y. Dai, G. Chen, X. Ye, X. Tan, E. Ding, Y. Zhang, and J. Wang, “Forward flow for novel view synthesis of dynamic scenes,” in Proc. IEEE Int. Conf. Comput. Vis., 2023, pp. 16 022– 16 033."
"Dynamic scene reconstruction is crucial in various applications, e.g., VR/AR, 3D animation, and autonomous driving [188]–[190]","[188] X. Zhou, Z. Lin, X. Shan, Y. Wang, D. Sun, and M.-H. Yang, “Drivinggaussian: Composite gaussian splatting for surrounding dynamic autonomous driving scenes,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[189] Y. Yan, H. Lin, C. Zhou, W. Wang, H. Sun, K. Zhan, X. Lang, X. Zhou, and S. Peng, “Street gaussians for modeling dynamic urban scenes,” in Proc. Eur. Conf. Comput. Vis., 2024.;[190] H. Zhou, J. Shao, L. Xu, D. Bai, W. Qiu, B. Liu, Y. Wang, A. Geiger, and Y. Liao, “Hugs: Holistic urban 3d scene understanding via gaussian splatting,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024, pp. 21 336–21 345."
"The key to adapt 3D GS to dynamic scenes is the modeling of temporal dimension which allows for the representation of scenes that change over time. 3D GS based methods [93]–[95], [106], [125]–[130], [191]–[199] for dynamic scene reconstruction can generally be divided into two main categories as discussed in Sec. 4.5 and Sec. 4.6","[93] Z. Yang, H. Yang, Z. Pan, X. Zhu, and L. Zhang, “Real-time photorealistic dynamic scene representation and rendering with 4d gaussian splatting,” in Proc. Int. Conf. Learn. Represent., 2024.;[94] Z. Yang, X. Gao, W. Zhou, S. Jiao, Y. Zhang, and X. Jin, “Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[95] G. Wu, T. Yi, J. Fang, L. Xie, X. Zhang, W. Wei, W. Liu, Q. Tian, and X. Wang, $^ { \\prime \\prime } { 4 \\mathrm { d } }$ gaussian splatting for real-time dynamic scene rendering,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[106] Y. Lin, Z. Dai, S. Zhu, and Y. Yao, “Gaussian-flow: 4d reconstruction with dynamic 3d gaussian particle,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[125] J. Luiten, G. Kopanas, B. Leibe, and D. Ramanan, “Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis,” in Proc. Int. Conf. 3D Vis., 2024.;[126] H. Yu, J. Julin, Z. ´A. Milacski, K. Niinuma, and L. A. Jeni, “Cogs: Controllable gaussian splatting,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[127] R. Shao, J. Sun, C. Peng, Z. Zheng, B. Zhou, H. Zhang, and Y. Liu, “Control4d: Efficient 4d portrait editing with text,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[128] Y.-H. Huang, Y.-T. Sun, Z. Yang, X. Lyu, Y.-P. Cao, and X. Qi, “Sc-gs: Sparse-controlled gaussian splatting for editable dynamic scenes,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[129] D. Das, C. Wewer, R. Yunus, E. Ilg, and J. E. Lenssen, “Neural parametric gaussians for monocular non-rigid object reconstruction,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[130] Z. Li, Z. Chen, Z. Li, and Y. Xu, “Spacetime gaussian feature splatting for real-time dynamic view synthesis,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[191] A. Kratimenos, J. Lei, and K. Daniilidis, “Dynmf: Neural motion factorization for real-time dynamic view synthesis with 3d gaussian splatting,” arXiv preprint arXiv:2312.00112, 2023.;[192] R. Shaw, J. Song, A. Moreau, M. Nazarczuk, S. Catley-Chandar, H. Dhamo, and E. Perez-Pellitero, “Swags: Sampling windows adaptively for dynamic 3d gaussian splatting,” arXiv preprint arXiv:2312.13308, 2023.;[193] Y. Liang, N. Khan, Z. Li, T. Nguyen-Phuoc, D. Lanman, J. Tompkin, and L. Xiao, “Gaufre: Gaussian deformation fields for real-time dynamic novel view synthesis,” arXiv preprint arXiv:2312.11458, 2023.;[194] K. Katsumata, D. M. Vo, and H. Nakayama, “An efficient 3d gaussian representation for monocular/multi-view dynamic scenes,” arXiv preprint arXiv:2311.12897, 2023.;[195] Z. Guo, W. Zhou, L. Li, M. Wang, and H. Li, “Motion-aware 3d gaussian splatting for efficient dynamic scene reconstruction,” arXiv preprint arXiv:2403.11447, 2024.;[196] J. Bae, S. Kim, Y. Yun, H. Lee, G. Bang, and Y. Uh, “Per-gaussian embedding-based deformation for deformable 3d gaussian splatting,” arXiv preprint arXiv:2404.03613, 2024.;[197] J. Lei, Y. Weng, A. Harley, L. Guibas, and K. Daniilidis, “Mosca: Dynamic gaussian fusion from casual videos via 4d motion scaffolds,” arXiv preprint arXiv:2405.17421, 2024.;[198] Q. Wang, V. Ye, H. Gao, J. Austin, Z. Li, and A. Kanazawa, “Shape of motion: 4d reconstruction from a single video,” arXiv preprint arXiv:2407.13764, 2024.;[199] Y. Duan, F. Wei, Q. Dai, Y. He, W. Chen, and B. Chen, “4drotor gaussian splatting: towards efficient novel view synthesis for dynamic scenes,” in Proc. ACM Spec. Interest Group Comput. Graph. Interact. Tech., 2024, pp. 1–11."
"For example, Yang et al. [94] first proposed deformable 3D Gaussians tailored for dynamic scenes","[94] Z. Yang, X. Gao, W. Zhou, S. Jiao, Y. Zhang, and X. Jin, “Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024."
"GaGS [132] devised the voxelization of a set of Gaussian distributions, followed by the use of sparse convolutions to extract geometry-aware features, which are then utilized for deformation learning","[132] Z. Lu, X. Guo, L. Hui, T. Chen, M. Yang, X. Tang, F. Zhu, and Y. Dai, $"" 3 \\mathrm { d }$ geometry-aware deformable gaussian splatting for dynamic view synthesis,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024."
"For instance, Luiten et al. [125] introduced dynamic 3D Gaussians to model dynamic scenes by keeping the properties of 3D Gaussians unchanged over time while allowing their positions and orientations to change","[125] J. Luiten, G. Kopanas, B. Leibe, and D. Ramanan, “Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis,” in Proc. Int. Conf. 3D Vis., 2024."
"Yang et al. [93] designed a 4D Gaussian representation, where additional properties are used to represent 4D rotations and spherindrical harmonics, to approximate the spatial-temporal volume of scenes","[93] Z. Yang, H. Yang, Z. Pan, X. Zhu, and L. Zhang, “Real-time photorealistic dynamic scene representation and rendering with 4d gaussian splatting,” in Proc. Int. Conf. Learn. Represent., 2024."
"While generation enables the synthesis of novel digital content from scratch or conditional inputs [200]– [202], editing provides the crucial ability to refine, adapt, and manipulate existing content with precise control [203]","[200] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. WardeFarley, S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial networks,” Communications of the ACM, vol. 63, no. 11, pp. 139–144, 2020. models,” in Proc. Adv. Neural Inf. Process. Syst., 2020, pp. 6840– 6851.;[202] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “High-resolution image synthesis with latent diffusion models,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2022, pp. 10 684– 10 695.;[203] L. Zhang, A. Rao, and M. Agrawala, “Adding conditional control to text-to-image diffusion models,” in Proc. IEEE Int. Conf. Comput. Vis., 2023, pp. 3836–3847."
"Recent advances in generation [133]–[138], [204]–[227] have led to the emergence of three main approaches","[133] J. Tang, J. Ren, H. Zhou, Z. Liu, and G. Zeng, “Dreamgaussian: Generative gaussian splatting for efficient 3d content creation,” in Proc. Int. Conf. Learn. Represent., 2024.;[134] T. Yi, J. Fang, J. Wang, G. Wu, L. Xie, X. Zhang, W. Liu, Q. Tian, and X. Wang, “Gaussiandreamer: Fast generation from text to 3d gaussians by bridging 2d and 3d diffusion models,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[135] J. Tang, Z. Chen, X. Chen, T. Wang, G. Zeng, and Z. Liu, “Lgm: Large multi-view gaussian model for high-resolution 3d content creation,” in Proc. Eur. Conf. Comput. Vis., 2024.;[136] S. Zhou, Z. Fan, D. Xu, H. Chang, P. Chari, T. Bharadwaj, S. You, Z. Wang, and A. Kadambi, “Dreamscene360: Unconstrained textto-3d scene generation with panoramic gaussian splatting,” in Proc. Eur. Conf. Comput. Vis., 2024.;[137] Z. Li, Y. Chen, L. Zhao, and P. Liu, “Controllable text-to-3d generation via surface-aligned gaussian splatting,” arXiv preprint arXiv:2403.09981, 2024.;[138] Y. Mu, X. Zuo, C. Guo, Y. Wang, J. Lu, X. Wu, S. Xu, P. Dai, Y. Yan, and L. Cheng, “Gsd: View-guided gaussian splatting diffusion for 3d reconstruction,” in Proc. Eur. Conf. Comput. Vis., 2024.;[204] Z. Chen, F. Wang, and H. Liu, “Text-to-3d using gaussian splatting,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[205] Y. Liang, X. Yang, J. Lin, H. Li, X. Xu, and Y. Chen, “Luciddreamer: Towards high-fidelity text-to-3d generation via interval score matching,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[206] X. Liu, X. Zhan, J. Tang, Y. Shan, G. Zeng, D. Lin, X. Liu, and Z. Liu, “Humangaussian: Text-driven 3d human generation with gaussian splatting,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[207] X. Yang, Y. Chen, C. Chen, C. Zhang, Y. Xu, X. Yang, F. Liu, and G. Lin, “Learn to optimize denoising scores for 3d generation: A unified and improved diffusion prior on nerf and 3d gaussian splatting,” arXiv preprint arXiv:2312.04820, 2023.;[208] Z.-X. Zou, Z. Yu, Y.-C. Guo, Y. Li, D. Liang, Y.-P. Cao, and S.-H. Zhang, “Triplane meets gaussian splatting: Fast and generalizable single-view 3d reconstruction with transformers,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[209] H. Ling, S. W. Kim, A. Torralba, S. Fidler, and K. Kreis, “Align your gaussians: Text-to-4d with dynamic 3d gaussians and composed diffusion models,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[210] J. Ren, L. Pan, J. Tang, C. Zhang, A. Cao, G. Zeng, and Z. Liu, “Dreamgaussian4d: Generative 4d gaussian splatting,” arXiv preprint arXiv:2312.17142, 2023.;[211] Y. Yin, D. Xu, Z. Wang, Y. Zhao, and Y. Wei, “4dgen: Grounded 4d content generation with spatial-temporal consistency,” arXiv preprint arXiv:2312.17225, 2023.;[212] J. Zhang, Z. Tang, Y. Pang, X. Cheng, P. Jin, Y. Wei, W. Yu, M. Ning, and L. Yuan, “Repaint123: Fast and high-quality one image to 3d generation with progressive controllable 2d repainting,” arXiv preprint arXiv:2312.13271, 2023.;[213] Z. Pan, Z. Yang, X. Zhu, and L. Zhang, “Fast dynamic 3d object generation from a single-view video,” arXiv preprint arXiv:2401.08742, 2024.;[214] D. Xu, Y. Yuan, M. Mardani, S. Liu, J. Song, Z. Wang, and A. Vahdat, “Agg: Amortized generative 3d gaussians for single image to 3d,” arXiv preprint arXiv:2401.04099, 2024.;[215] C. Yang, S. Li, J. Fang, R. Liang, L. Xie, X. Zhang, W. Shen, and Q. Tian, “Gaussianobject: Just taking four images to get a high-quality 3d object with gaussian splatting,” arXiv preprint arXiv:2402.10259, 2024.;[216] F. Barthel, A. Beckmann, W. Morgenstern, A. Hilsmann, and P. Eisert, “Gaussian splatting decoder for 3d-aware generative adversarial networks,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Worksh., 2024.;[217] L. Jiang and L. Wang, “Brightdreamer: Generic 3d gaussian generative framework for fast text-to-3d synthesis,” arXiv preprint arXiv:2403.11273, 2024.;[218] W. Zhuo, F. Ma, H. Fan, and Y. Yang, “Vividdreamer: Invariant score distillation for hyper-realistic text-to-3d generation,” in Proc. Eur. Conf. Comput. Vis., 2024.;[219] Z. Wu, C. Yu, Y. Jiang, C. Cao, F. Wang, and X. Bai, “Sc4d: Sparse-controlled video-to-4d generation and motion transfer,” arXiv preprint arXiv:2404.03736, 2024.;[220] X. He, J. Chen, S. Peng, D. Huang, Y. Li, X. Huang, C. Yuan, W. Ouyang, and T. He, “Gvgen: Text-to-3d generation with volumetric representation,” in Proc. Eur. Conf. Comput. Vis., 2024.;[221] X. Yang and X. Wang, “Hash3d: Training-free acceleration for 3d generation,” arXiv preprint arXiv:2404.06091, 2024.;[222] J. Kim, J. Koo, K. Yeo, and M. Sung, “Synctweedies: A general generative framework based on synchronized diffusions,” arXiv preprint arXiv:2403.14370, 2024.;[223] Q. Feng, Z. Xing, Z. Wu, and Y.-G. Jiang, “Fdgaussian: Fast gaussian splatting from single image via geometric-aware diffusion model,” arXiv preprint arXiv:2403.10242, 2024. Lee, and P. Zhou, “Dreamscene: 3d gaussian-based text-to-3d scene generation via formation pattern sampling,” arXiv preprint arXiv:2404.03575, 2024.;[225] L. Melas-Kyriazi, I. Laina, C. Rupprecht, N. Neverova, A. Vedaldi, O. Gafni, and F. Kokkinos, “Im-3d: Iterative multiview diffusion and reconstruction for high-quality 3d generation,” in Proc. ACM Int. Conf. Mach. Learn., 2024.;[226] B. Zhang, Y. Cheng, J. Yang, C. Wang, F. Zhao, Y. Tang, D. Chen, and B. Guo, “Gaussiancube: Structuring gaussian splatting using optimal transport for 3d generative modeling,” arXiv preprint arXiv:2403.19655, 2024.;[227] Y.-C. Lee, Y.-T. Chen, A. Wang, T.-H. Liao, B. Y. Feng, and J.-B. Huang, “Vividdream: Generating 3d scene with ambient dynamics,” arXiv preprint arXiv:2405.20334, 2024."
"Optimization based methods [133], [134], [204] distill diffusion priors (gradients) to guide 3D model updates with the score functions","[133] J. Tang, J. Ren, H. Zhou, Z. Liu, and G. Zeng, “Dreamgaussian: Generative gaussian splatting for efficient 3d content creation,” in Proc. Int. Conf. Learn. Represent., 2024.;[134] T. Yi, J. Fang, J. Wang, G. Wu, L. Xie, X. Zhang, W. Liu, Q. Tian, and X. Wang, “Gaussiandreamer: Fast generation from text to 3d gaussians by bridging 2d and 3d diffusion models,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[204] Z. Chen, F. Wang, and H. Liu, “Text-to-3d using gaussian splatting,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024."
"Reconstruction based methods [135], [225], [227] reframe the generation problem as a multiview reconstruction task utilizing pre-trained multi-view diffusion models","[135] J. Tang, Z. Chen, X. Chen, T. Wang, G. Zeng, and Z. Liu, “Lgm: Large multi-view gaussian model for high-resolution 3d content creation,” in Proc. Eur. Conf. Comput. Vis., 2024.;[225] L. Melas-Kyriazi, I. Laina, C. Rupprecht, N. Neverova, A. Vedaldi, O. Gafni, and F. Kokkinos, “Im-3d: Iterative multiview diffusion and reconstruction for high-quality 3d generation,” in Proc. ACM Int. Conf. Mach. Learn., 2024.;[227] Y.-C. Lee, Y.-T. Chen, A. Wang, T.-H. Liao, B. Y. Feng, and J.-B. Huang, “Vividdream: Generating 3d scene with ambient dynamics,” arXiv preprint arXiv:2405.20334, 2024."
"Direct 3D generation methods train diffusion models on 3D representations [138], [220], [226]","[138] Y. Mu, X. Zuo, C. Guo, Y. Wang, J. Lu, X. Wu, S. Xu, P. Dai, Y. Yan, and L. Cheng, “Gsd: View-guided gaussian splatting diffusion for 3d reconstruction,” in Proc. Eur. Conf. Comput. Vis., 2024.;[220] X. He, J. Chen, S. Peng, D. Huang, Y. Li, X. Huang, C. Yuan, W. Ouyang, and T. He, “Gvgen: Text-to-3d generation with volumetric representation,” in Proc. Eur. Conf. Comput. Vis., 2024.;[226] B. Zhang, Y. Cheng, J. Yang, C. Wang, F. Zhao, Y. Tang, D. Chen, and B. Guo, “Gaussiancube: Structuring gaussian splatting using optimal transport for 3d generative modeling,” arXiv preprint arXiv:2403.19655, 2024."
"Current editing works [90]–[92], [126]–[128], [140]–[143], [228]–[239] fall into two primary classes","[90] S. Zhou, H. Chang, S. Jiang, Z. Fan, Z. Zhu, D. Xu, P. Chari, S. You, Z. Wang, and A. Kadambi, “Feature 3dgs: Supercharging 3d gaussian splatting to enable distilled feature fields,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[91] M. Ye, M. Danelljan, F. Yu, and L. Ke, “Gaussian grouping: Segment and edit anything in 3d scenes,” in Proc. Eur. Conf. Comput. Vis., 2024.;[92] J. Cen, J. Fang, C. Yang, L. Xie, X. Zhang, W. Shen, and Q. Tian, “Segment any 3d gaussians,” arXiv preprint arXiv:2312.00860, 2023.;[126] H. Yu, J. Julin, Z. ´A. Milacski, K. Niinuma, and L. A. Jeni, “Cogs: Controllable gaussian splatting,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[127] R. Shao, J. Sun, C. Peng, Z. Zheng, B. Zhou, H. Zhang, and Y. Liu, “Control4d: Efficient 4d portrait editing with text,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[128] Y.-H. Huang, Y.-T. Sun, Z. Yang, X. Lyu, Y.-P. Cao, and X. Qi, “Sc-gs: Sparse-controlled gaussian splatting for editable dynamic scenes,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[140] Y. Wang, Q. Wu, G. Zhang, and D. Xu, “Gscream: Learning 3d geometry and feature consistent gaussian splatting for object removal,” in Proc. Eur. Conf. Comput. Vis., 2024.;[141] Y. Chen, Z. Chen, C. Zhang, F. Wang, X. Yang, Y. Wang, Z. Cai, L. Yang, H. Liu, and G. Lin, “Gaussianeditor: Swift and controllable 3d editing with gaussian splatting,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[142] J. Fang, J. Wang, X. Zhang, L. Xie, and Q. Tian, “Gaussianeditor: Editing 3d gaussians delicately with text instructions,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[143] R.-Z. Qiu, G. Yang, W. Zeng, and X. Wang, “Feature splatting: Language-driven physics-based scene synthesis and editing,” arXiv preprint arXiv:2404.01223, 2024.;[228] J. Huang and H. Yu, “Point’n move: Interactive scene object manipulation on gaussian splatting radiance fields,” arXiv preprint arXiv:2311.16737, 2023.;[229] K. Lan, H. Li, H. Shi, W. Wu , Y. Liao, L. Wang, and P. Zhou, “2d-guided 3d gaussian segmentation,” arXiv preprint arXiv:2312.16047, 2023.;[230] J. Zhuang, D. Kang, Y.-P. Cao, G. Li, L. Lin, and Y. Shan, “Tipeditor: An accurate 3d editor following both text-prompts and image-prompts,” arXiv preprint arXiv:2401.14828, 2024.;[231] B. Dou, T. Zhang, Y. Ma, Z. Wang, and Z. Yuan, “Cosseggaussians: Compact and swift scene segmenting 3d gaussians,” arXiv preprint arXiv:2401.05925, 2024.;[232] X. Hu, Y. Wang, L. Fan, J. Fan, J. Peng, Z. Lei, Q. Li, and Z. Zhang, “Semantic anything in 3d gaussians,” arXiv preprint arXiv:2401.17857, 2024.;[233] F. Palandra, A. Sanchietti, D. Baieri, and E. Rodola\\`, “Gsedit: Efficient text-guided editing of 3d objects via gaussian splatting,” arXiv preprint arXiv:2403.05154, 2024.;[234] Q. Gu, Z. Lv, D. Frost, S. Green, J. Straub, and C. Sweeney, “Egolifter: Open-world 3d segmentation for egocentric perception,” arXiv preprint arXiv:2403.18118, 2024.;[235] W. Lyu, X. Li, A. Kundu, Y.-H. Tsai, and M.-H. Yang, “Gaga: Group any gaussians via 3d-aware memory bank,” arXiv preprint arXiv:2404.07977, 2024.;[236] Z. Liu, H. Ouyang, Q. Wang, K. L. Cheng, J. Xiao, K. Zhu, N. Xue, Y. Liu, Y. Shen, and Y. Cao, “Infusion: Inpainting 3d gaussians via learning depth completion from diffusion prior,” arXiv preprint arXiv:2404.11613, 2024.;[237] D. Zhang, Z. Chen, Y.-J. Yuan, F.-L. Zhang, Z. He, S. Shan, and L. Gao, “Stylizedgs: Controllable stylization for 3d gaussian splatting,” arXiv preprint arXiv:2404.05220, 2024.;[238] Q. Zhang, Y. Xu, C. Wang, H.-Y. Lee, G. Wetzstein, B. Zhou, and C. Yang, “3ditscene: Editing any scene via language-guided disentangled gaussian splatting,” arXiv preprint arXiv:2405.18424, 2024.;[239] J. Wu, J.-W. Bian, X. Li, G. Wang, I. Reid, P. Torr, and V. A. Prisacariu, “Gaussctrl: Multi-view consistent text-driven 3d gaussian splatting editing,” in Proc. Eur. Conf. Comput. Vis., 2024, pp. 55–71."
"Early efforts [141], [142], [233] adopt optimization- or reconstruction-based strategies akin to methods in generation, but introduce task-specific control signals","[141] Y. Chen, Z. Chen, C. Zhang, F. Wang, X. Yang, Y. Wang, Z. Cai, L. Yang, H. Liu, and G. Lin, “Gaussianeditor: Swift and controllable 3d editing with gaussian splatting,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[142] J. Fang, J. Wang, X. Zhang, L. Xie, and Q. Tian, “Gaussianeditor: Editing 3d gaussians delicately with text instructions,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[233] F. Palandra, A. Sanchietti, D. Baieri, and E. Rodola\\`, “Gsedit: Efficient text-guided editing of 3d objects via gaussian splatting,” arXiv preprint arXiv:2403.05154, 2024."
"Subsequent works [140], [238]–[240] mitigate this through iterative refinement or cross-view attention, albeit at increased computational costs for alignment","[140] Y. Wang, Q. Wu, G. Zhang, and D. Xu, “Gscream: Learning 3d geometry and feature consistent gaussian splatting for object removal,” in Proc. Eur. Conf. Comput. Vis., 2024.;[238] Q. Zhang, Y. Xu, C. Wang, H.-Y. Lee, G. Wetzstein, B. Zhou, and C. Yang, “3ditscene: Editing any scene via language-guided disentangled gaussian splatting,” arXiv preprint arXiv:2405.18424, 2024.;[239] J. Wu, J.-W. Bian, X. Li, G. Wang, I. Reid, P. Torr, and V. A. Prisacariu, “Gaussctrl: Multi-view consistent text-driven 3d gaussian splatting editing,” in Proc. Eur. Conf. Comput. Vis., 2024, pp. 55–71.;[240] Y. Wang, X. Yi, Z. Wu, N. Zhao, L. Chen, and H. Zhang, “Viewconsistent 3d editing with gaussian splatting,” in Proc. Eur. Conf. Comput. Vis., 2024, pp. 404–420."
"The second class exploits the explicit nature of 3D GS to enable direct manipulation based on embedded properties such as semantics [91], [92], [143], [232] and key points [128]","[91] M. Ye, M. Danelljan, F. Yu, and L. Ke, “Gaussian grouping: Segment and edit anything in 3d scenes,” in Proc. Eur. Conf. Comput. Vis., 2024.;[92] J. Cen, J. Fang, C. Yang, L. Xie, X. Zhang, W. Shen, and Q. Tian, “Segment any 3d gaussians,” arXiv preprint arXiv:2312.00860, 2023.;[128] Y.-H. Huang, Y.-T. Sun, Z. Yang, X. Lyu, Y.-P. Cao, and X. Qi, “Sc-gs: Sparse-controlled gaussian splatting for editable dynamic scenes,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[143] R.-Z. Qiu, G. Yang, W. Zeng, and X. Wang, “Feature splatting: Language-driven physics-based scene synthesis and editing,” arXiv preprint arXiv:2404.01223, 2024.;[232] X. Hu, Y. Wang, L. Fan, J. Fan, J. Peng, Z. Lei, Q. Li, and Z. Zhang, “Semantic anything in 3d gaussians,” arXiv preprint arXiv:2401.17857, 2024."
"For fullbody avatars [139], [144]–[147], [241]–[252], the current methods typically anchor 3D Gaussians in a canonical space and deform them via parametric body models (e.g., SMPL) or cage-based rigging to model dynamic motions","[139] Y. Jiang, Z. Shen, P. Wang, Z. Su, Y. Hong, Y. Zhang, J. Yu, and L. Xu, “Hifi4g: High-fidelity human performance rendering via compact gaussian splatting,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[144] Z. Li, Z. Zheng, L. Wang, and Y. Liu, “Animatable gaussians: Learning pose-dependent gaussian maps for high-fidelity human avatar modeling,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[145] S. Hu and Z. Liu, “Gauhuman: Articulated gaussian splatting from monocular human videos,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[146] J. Lei, Y. Wang, G. Pavlakos, L. Liu, and K. Daniilidis, “Gart: Gaussian articulated template models,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[147] Y. Yuan, X. Li, Y. Huang, S. De Mello, K. Nagano, J. Kautz, and U. Iqbal, “Gavatar: Animatable 3d gaussian avatars with implicit mesh learning,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[241] R. Jena, G. S. Iyer, S. Choudhary, B. Smith, P. Chaudhari, and J. Gee, “Splatarmor: Articulated gaussian splatting for animatable humans from monocular rgb videos,” arXiv preprint arXiv:2311.10812, 2023.;[242] K. Ye, T. Shao, and K. Zhou, “Animatable 3d gaussians for high-fidelity synthesis of human motions,” arXiv preprint arXiv:2311.13404, 2023.;[243] A. Moreau, J. Song, H. Dhamo, R. Shaw, Y. Zhou, and E. P´erezPellitero, “Human gaussian splatting: Real-time rendering of animatable avatars,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[244] M. Kocabas, J.-H. R. Chang, J. Gabriel, O. Tuzel, and A. Ranjan, “Hugs: Human gaussian splats,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[245] R. Abdal, W. Yifan, Z. Shi, Y. Xu, R. Po, Z. Kuang, Q. Chen, D.- Y. Yeung, and G. Wetzstein, “Gaussian shell maps for efficient 3d human generation,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[246] S. Zheng, B. Zhou, R. Shao, B. Liu, S. Zhang, L. Nie, and Y. Liu, “Gps-gaussian: Generalizable pixel-wise 3d gaussian splatting;[247] L. Hu, H. Zhang, Y. Zhang, B. Zhou, B. Liu, S. Zhang, and L. Nie, “Gaussianavatar: Towards realistic human avatar modeling from a single video via animatable 3d gaussians,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[248] H. Pang, H. Zhu, A. Kortylewski, C. Theobalt, and M. Habermann, “Ash: Animatable gaussian splats for efficient and photoreal human rendering,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[249] Z. Qian, S. Wang, M. Mihajlovic, A. Geiger, and S. Tang, “3dgsavatar: Animatable avatars via deformable 3d gaussian splatting,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[250] H. Jung, N. Brasch, J. Song, E. Perez-Pellitero, Y. Zhou, Z. Li, N. Navab, and B. Busam, “Deformable 3d gaussian splatting for animatable human avatars,” arXiv preprint arXiv:2312.15059, 2023.;[251] M. Li, J. Tao, Z. Yang, and Y. Yang, “Human101: Training $^ { 1 0 0 + }$ fps human gaussians in 100s from 1 view,” arXiv preprint arXiv:2312.15258, 2023.;[252] M. Li, S. Yao, Z. Xie, K. Chen, and Y.-G. Jiang, “Gaussianbody: Clothed human reconstruction via 3d gaussian splatting,” arXiv preprint arXiv:2401.09720, 2024."
"For head avatars [23], [148]–[151], [253]–[256], the emphasis shifts to modeling intricate facial expressions, fine-grained geometry (e.g., wrinkles, hair [257]), and dynamic speechdriven animations","[23] S. Saito, G. Schwartz, T. Simon, J. Li, and G. Nam, “Relightable gaussian codec avatars,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[148] Z. Zhou, F. Ma, H. Fan, and Y. Yang, “Headstudio: Text to animatable head avatars with 3d gaussian splatting,” in Proc. Eur. Conf. Comput. Vis., 2024.;[149] S. Qian, T. Kirschstein, L. Schoneveld, D. Davoli, S. Giebenhain, and M. Nießner, “Gaussianavatars: Photorealistic head avatars with rigged 3d gaussians,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[150] H. Dhamo, Y. Nie, A. Moreau, J. Song, R. Shaw, Y. Zhou, and E. Pe´rez-Pellitero, “Headgas: Real-time animatable head avatars via 3d gaussian splatting,” arXiv preprint arXiv:2312.02902, 2023.;[151] J. Li, J. Zhang, X. Bai, J. Zheng, X. Ning, J. Zhou, and L. Gu, “Talkinggaussian: Structure-persistent 3d talking head synthesis via gaussian splatting,” in Proc. Eur. Conf. Comput. Vis., 2024.;[253] J. Xiang, X. Gao, Y. Guo, and J. Zhang, “Flashavatar: Highfidelity digital avatar rendering at 300fps,” arXiv preprint arXiv:2312.02214, 2023.;[254] Y. Chen, L. Wang, Q. Li, H. Xiao, S. Zhang, H. Yao, and Y. Liu, “Monogaussianavatar: Monocular gaussian point-based head avatar,” arXiv preprint arXiv:2312.04558, 2023.;[255] Z. Zhao, Z. Bao, Q. Li, G. Qiu, and K. Liu, “Psavatar: A pointbased morphable shape model for real-time head avatar creation with 3d gaussian splatting,” arXiv preprint arXiv:2401.12900, 2024.;[256] A. Rivero, S. Athar, Z. Shu, and D. Samaras, “Rig3dgs: Creating controllable portraits from casual monocular videos,” arXiv preprint arXiv:2402.03723, 2024.;[257] H. Luo, M. Ouyang, Z. Zhao, S. Jiang, L. Zhang, Q. Zhang, W. Yang, L. Xu, and J. Yu, “Gaussianhair: Hair modeling and rendering with light-aware gaussians,” arXiv preprint arXiv:2402.10483, 2024."
"Pioneering the integration of dynamic radiance fields into this domain, recent advancements have focused on surmounting the inherent challenges of single-viewpoint video reconstructions such as occlusions by surgical instruments and sparse viewpoint diversity within the confined spaces of endoscopic exploration [258]– [260]","[258] Y. Wang, Y. Long, S. H. Fan, and Q. Dou, “Neural rendering for stereo 3d reconstruction of deformable tissues in robotic surgery,” in Proc. Int. Conf. Med. Image Comput. Comput. Assist. Interv., 2022, pp. 431–441.;[259] C. Yang, K. Wang, Y. Wang, X. Yang, and W. Shen, “Neural lerplane representations for fast 4d reconstruction of deformable tissues,” in Proc. Int. Conf. Med. Image Comput. Comput. Assist. Interv., 2023, pp. 46–56.;[260] R. Zha, X. Cheng, H. Li, M. Harandi, and Z. Ge, “Endosurf: Neural surface reconstruction of deformable tissues with stereo endoscope videos,” in Proc. Int. Conf. Med. Image Comput. Comput. Assist. Interv., 2023, pp. 13–23."
"Despite the progress, the call for high fidelity in tissue deformability and topological variation remains, coupled with the pressing demand for faster rendering to bridge the utility in applications sensitive to latency [152]–[154]","[152] Y. Huang, B. Cui, L. Bai, Z. Guo, M. ${ \\tt X u } ,$ and H. Ren, “Endo4dgs: Distilling depth ranking for endoscopic monocular scene reconstruction with 4d gaussian splatting,” in Proc. Int. Conf. Med. Image Comput. Comput. Assist. Interv., 2024.;[153] Y. Liu, C. Li, C. Yang, and Y. Yuan, “Endogaussian: Gaussian splatting for deformable surgical scene reconstruction,” arXiv preprint arXiv:2401.12561, 2024.;[154] L. Zhu, Z. Wang, Z. Jin, G. Lin, and L. Yu, “Deformable endoscopic tissues reconstruction with gaussian splatting,” arXiv preprint arXiv:2401.11535, 2024."
Existing approaches mainly used additional depth guidance to infer the geometry of tissues [152]–[154],"[152] Y. Huang, B. Cui, L. Bai, Z. Guo, M. ${ \\tt X u } ,$ and H. Ren, “Endo4dgs: Distilling depth ranking for endoscopic monocular scene reconstruction with 4d gaussian splatting,” in Proc. Int. Conf. Med. Image Comput. Comput. Assist. Interv., 2024.;[153] Y. Liu, C. Li, C. Yang, and Y. Yuan, “Endogaussian: Gaussian splatting for deformable surgical scene reconstruction,” arXiv preprint arXiv:2401.12561, 2024.;[154] L. Zhu, Z. Wang, Z. Jin, G. Lin, and L. Yu, “Deformable endoscopic tissues reconstruction with gaussian splatting,” arXiv preprint arXiv:2401.11535, 2024."
"For instance, EndoGS [154] integrated depth-guided supervision with spatial-temporal weight masks and surface-aligned regularization terms to enhance the quality and speed of 3D tissue rendering while addressing tool occlusion","[154] L. Zhu, Z. Wang, Z. Jin, G. Lin, and L. Yu, “Deformable endoscopic tissues reconstruction with gaussian splatting,” arXiv preprint arXiv:2401.11535, 2024."
EndoGaussian [153] introduced two new strategies: holistic Gaussian initialization for dense initialization and spatiotemporal Gaussian tracking for modeling surface dynamics,"[153] Y. Liu, C. Li, C. Yang, and Y. Yuan, “Endogaussian: Gaussian splatting for deformable surgical scene reconstruction,” arXiv preprint arXiv:2401.12561, 2024."
Zhao et al. [155] argued that these methods suffer from under-reconstruction and proposed to alleviate this problem from frequency perspectives,"[155] H. Zhao, X. Zhao, L. Zhu, W. Zheng, and Y. Xu, “Hfgs: 4d gaussian splatting with emphasis on spatial and temporal highfrequency components for endoscopic scene reconstruction,” arXiv preprint arXiv:2405.17872, 2024."
"In addition, EndoGSLAM [156] and Gaussian Pancake [157] devised SLAM systems for endoscopic scenes and showed significant speed advantages","[156] K. Wang, C. Yang, Y. Wang, S. Li, Y. Wang, Q. Dou, X. Yang, and W. Shen, “Endogslam: Real-time dense reconstruction and tracking in endoscopic surgeries using gaussian splatting,” arXiv preprint arXiv:2403.15124, 2024.;[157] S. Bonilla, S. Zhang, D. Psychogyios, D. Stoyanov, F. Vasconcelos, and S. Bano, “Gaussian pancakes: Geometrically-regularized 3d gaussian splatting for realistic endoscopic reconstruction,” arXiv preprint arXiv:2404.06128, 2024. Z. Gan, and W. Ding, “Hgs-mapping: Online dense mapping using hybrid gaussian representation in urban scenes,” arXiv preprint arXiv:2403.20159, 2024."
"In addition, existing datasets often feature truncated sequences (e.g., $4 \\sim 8 s$ in EndoNeRF [258]), which fail to capture prolonged tissue deformation dynamics or complex surgical workflows","[258] Y. Wang, Y. Long, S. H. Fan, and Q. Dou, “Neural rendering for stereo 3d reconstruction of deformable tissues in robotic surgery,” in Proc. Int. Conf. Med. Image Comput. Comput. Assist. Interv., 2022, pp. 431–441."
"For instance, a scene spanning $2 . 7 \\ k m ^ { 2 }$ may require over 20 million Gaussians, pushing the limits of even the most advanced hardware (e.g., NVIDIA A100 with 40GB memory) [163]","[163] Y. Liu, H. Guan, C. Luo, L. Fan, J. Peng, and Z. Zhang, “Citygaussian: Real-time high-quality large-scale scene rendering with gaussians,” in Proc. Eur. Conf. Comput. Vis., 2024."
"To address the highlighted challenges, researchers have made significant strides in two key areas: i) For training, a divide-and-conquer strategy [162]–[165] has been adopted, which segments a large scene into multiple, independent cells","[162] B. Kerbl, A. Meuleman, G. Kopanas, M. Wimmer, A. Lanvin, and G. Drettakis, “A hierarchical 3d gaussian representation for realtime rendering of very large datasets,” ACM Trans. Graph., vol. 44, no. 3, 2024.;[163] Y. Liu, H. Guan, C. Luo, L. Fan, J. Peng, and Z. Zhang, “Citygaussian: Real-time high-quality large-scale scene rendering with gaussians,” in Proc. Eur. Conf. Comput. Vis., 2024.;[164] J. Lin, Z. Li, X. Tang, J. Liu, S. Liu, J. Liu, Y. Lu, X. Wu, S. Xu, Y. Yan et al., “Vastgaussian: Vast 3d gaussians for large scene reconstruction,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[165] K. Ren, L. Jiang, T. Lu, M. Yu, L. Xu, Z. Ni, and B. Dai, “Octreegs: Towards consistent real-time rendering with lod-structured 3d gaussians,” arXiv preprint arXiv:2403.17898, 2024."
"With the same spirit, Zhao et al. [161] proposed a distributed implementation of 3D GS training","[161] H. Zhao, H. Weng, D. Lu, A. Li, J. Li, A. Panda, and S. Xie, “On scaling up 3d gaussian splatting training,” arXiv preprint arXiv:2406.18533, 2024."
"Enhancing the optimization algorithm presents a viable solution to mitigate this issue [44], [164]. ii) Regarding rendering, the adoption of the Level of Details (LoD) technique from computer graphics has proven instrumental","[44] K. Cheng, X. Long, K. Yang, Y. Yao, W. Yin, Y. Ma, W. Wang, and X. Chen, “Gaussianpro: 3d gaussian splatting with progressive propagation,” in Proc. ACM Int. Conf. Mach. Learn., 2024.;[164] J. Lin, Z. Li, X. Tang, J. Liu, S. Liu, J. Liu, Y. Lu, X. Wu, S. Xu, Y. Yan et al., “Vastgaussian: Vast 3d gaussians for large scene reconstruction,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024."
"Current implementations involve feeding only the essential Gaussians to the rasterizer [164], or designing explicit LoD structures like the Octree [165] and hierarchy [162]","[162] B. Kerbl, A. Meuleman, G. Kopanas, M. Wimmer, A. Lanvin, and G. Drettakis, “A hierarchical 3d gaussian representation for realtime rendering of very large datasets,” ACM Trans. Graph., vol. 44, no. 3, 2024.;[164] J. Lin, Z. Li, X. Tang, J. Liu, S. Liu, J. Liu, Y. Lu, X. Wu, S. Xu, Y. Yan et al., “Vastgaussian: Vast 3d gaussians for large scene reconstruction,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[165] K. Ren, L. Jiang, T. Lu, M. Yu, L. Xu, Z. Ni, and B. Dai, “Octreegs: Towards consistent real-time rendering with lod-structured 3d gaussians,” arXiv preprint arXiv:2403.17898, 2024."
"Furthermore, integrating extra input modalities like LiDAR can further enhanced the reconstruction process [158]–[160]","[159] C. Wu, Y. Duan, X. Zhang, Y. Sheng, J. Ji, and Y. Zhang, “Mmgaussian: 3d gaussian-based multi-modal fusion for localization and reconstruction in unbounded scenes,” in Proc. IEEE/RSJ Int. Conf. Intell. Robot. Syst., 2024.;[160] B. Xiong, Z. Li, and Z. Li, “Gauu-scene: A scene reconstruction benchmark on large scale 3d reconstruction dataset using gaussian splatting,” arXiv preprint arXiv:2401.14032, 2024."
"Meanwhile, memory and computational bottlenecks can be addressed via distributed learning strategies [161], such as parameter partitioning across GPU clusters and parallel batched multiview optimization","[161] H. Zhao, H. Weng, D. Lu, A. Li, J. Li, A. Panda, and S. Xie, “On scaling up 3d gaussian splatting training,” arXiv preprint arXiv:2406.18533, 2024."
"The material point method [268] and position based dynamics [269] — numerical methods used in computer graphics for simulating deformations in materials like fluids, granular media, and fracturing solids — have been extensively explored by the community through various customizations [21], [143], [166]–[171]","[21] Y. Jiang, C. Yu, T. Xie, X. Li, Y. Feng, H. Wang, M. Li, H. Lau, F. Gao, Y. Yang et al., “Vr-gs: A physical dynamics-aware interactive gaussian splatting system in virtual reality,” arXiv preprint arXiv:2401.16663, 2024.;[143] R.-Z. Qiu, G. Yang, W. Zeng, and X. Wang, “Feature splatting: Language-driven physics-based scene synthesis and editing,” arXiv preprint arXiv:2404.01223, 2024.;[166] T. Xie, Z. Zong, Y. Qiu, X. Li, Y. Feng, Y. Yang, and C. Jiang, “Physgaussian: Physics-integrated 3d gaussians for generative dynamics,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[167] F. Liu, H. Wang, S. Yao, S. Zhang, J. Zhou, and Y. Duan, “Physics3d: Learning physical properties of 3d gaussians via video diffusion,” arXiv preprint arXiv:2406.04338, 2024.;[168] P. Borycki, W. Smolak, J. Waczy´nska, M. Mazur, S. Tadeja, and P. Spurek, “Gasp: Gaussian splatting for physic-based simulations,” arXiv preprint arXiv:2409.05819, 2024.;[169] T. Huang, Y. Zeng, H. Li, W. Zuo, and R. W. Lau, “Dreamphysics: Learning physical properties of dynamic 3d gaussians with video diffusion priors,” in Proc. AAAI Conf. Artif. Intell., 2025.;[170] T. Zhang, H.-X. Yu, R. Wu, B. Y. Feng, C. Zheng, N. Snavely, J. Wu, and W. T. Freeman, “Physdreamer: Physics-based interaction with 3d objects via video generation,” in Proc. Eur. Conf. Comput. Vis., 2024, pp. 388–406.;[171] Y. Feng, X. Feng, Y. Shang, Y. Jiang, C. Yu, Z. Zong, T. Shao, H. Wu, K. Zhou, C. Jiang et al., “Gaussian splashing: Dynamic fluid synthesis with gaussian splatting,” arXiv preprint arXiv:2401.15318, 2024.;[269] M. M ¨uller, B. Heidelberger, M. Hennix, and J. Ratcliff, “Position based dynamics,” Journal of Visual Communication and Image Representation, vol. 18, no. 2, pp. 109–118, 2007."
"Analytical material models, such as mass-spring systems, have also demonstrated success in approximating deformations by explicitly encoding material properties into 3D Gaussians [172]","[172] L. Zhong, H.-X. Yu, J. Wu, and Y. Li, “Reconstruction and simulation of elastic objects with spring-mass 3d gaussians,” in Proc. Eur. Conf. Comput. Vis., 2024."
"Across these methods, 3D Gaussians are treated as discrete particles (with one exception [173] using a continuous representation) and serve as computational units within the chosen simulator","[173] Y. Shao, M. Huang, C. C. Loy, and B. Dai, “Gausim: Registering elastic objects into digital world by gaussian simulator,” arXiv preprint arXiv:2412.17804, 2024."
"TABLE 1 Comparison of localization methods $( \\ S \\quad )$ on Replica [261] (static scenes), in terms of absolute trajectory error (ATE, cm). (The three bes scores are marked in red, blue, and green, respectively","[261] J. Straub, T. Whelan, L. Ma, Y. Chen, E. Wijmans, S. Green, J. J. Engel, R. Mur-Artal, C. Ren, S. Verma et al., “The replica dataset: A digital replica of indoor spaces,” arXiv preprint arXiv:1906.05797, 2019."
"The localization task in SLAM involves determining the precise position and orientation of a robot or device within an environment, typically using sensor data.  
• Dataset: Replica [261] dataset is a collection of 18 highly detailed 3D indoor scenes","[261] J. Straub, T. Whelan, L. Ma, Y. Chen, E. Wijmans, S. Green, J. J. Engel, R. Mur-Artal, C. Ren, S. Verma et al., “The replica dataset: A digital replica of indoor spaces,” arXiv preprint arXiv:1906.05797, 2019."
"Following [262], three sequences about rooms and five sequences about offices are used for the evaluation.  
• Benchmarking Algorithms: For performance comparison, we involve four recent 3D GS based algorithms [111]–[114] and six typical SLAM methods [262]–[267]","[111] C. Yan, D. Qu, D. Wang, D. Xu, Z. Wang, B. Zhao, and X. Li, “Gs-slam: Dense visual slam with 3d gaussian splatting,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[112] N. Keetha, J. Karhade, K. M. Jatavallabhula, G. Yang, S. Scherer, D. Ramanan, and J. Luiten, “Splatam: Splat, track & map 3d Pattern Recognit., 2024.;[113] H. Matsuki, R. Murai, P. H. Kelly, and A. J. Davison, “Gaussian splatting slam,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[114] V. Yugay, Y. Li, T. Gevers, and M. R. Oswald, “Gaussian-slam: Photo-realistic dense slam with gaussian splatting,” arXiv preprint arXiv:2312.10070, 2023.;[262] E. Sucar, S. Liu, J. Ortiz, and A. J. Davison, “imap: Implicit mapping and positioning in real-time,” in Proc. IEEE Int. Conf. Comput. Vis., 2021, pp. 6229–6238.;[263] X. Yang, H. Li, H. Zhai, Y. Ming, Y. Liu, and G. Zhang, “Voxfusion: Dense tracking and mapping with voxel-based neural implicit representation,” in IEEE International Symposium on Mixed and Augmented Reality, 2022, pp. 499–507.;[264] Z. Zhu, S. Peng, V. Larsson, W. Xu, H. Bao, Z. Cui, M. R. Oswald, and M. Pollefeys, “Nice-slam: Neural implicit scalable encoding for slam,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2022, pp. 12 786–12 796.;[265] M. M. Johari, C. Carta, and F. Fleuret, “Eslam: Efficient dense slam system based on hybrid representation of signed distance fields,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2023, pp. 17 408–17 419.;[266] E. Sandstr ¨om, Y. Li, L. Van Gool, and M. R. Oswald, “Point-slam: Dense neural point cloud-based slam,” in Proc. IEEE Int. Conf. Comput. Vis., 2023, pp. 18 433–18 444.;[267] H. Wang, J. Wang, and L. Agapito, “Co-slam: Joint coordinate and sparse parametric encodings for neural real-time slam,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2023, pp. 13 293–13 302. “A moving least squares material point method with displacement discontinuity and two-way rigid body coupling,” ACM Trans. Graph., vol. 37, no. 4, pp. 1–14, 2018."
"Evaluation Metric: The root mean square error (RMSE) of the absolute trajectory error (ATE) is a commonly used metric in evaluating SLAM systems [275], which measures the root mean square of the Euclidean distances between the estimated and true positions over the entire trajectory","[275] J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cremers, “A benchmark for the evaluation of rgb-d slam systems,” in Proc. IEEE/RSJ Int. Conf. Intell. Robot. Syst., 2012, pp. 573–580."
"For example, SplaTAM [112] achieves a trajectory error improvement of $\\sim 5 0 \\%$ , decreasing it from $0 . 5 2 \\mathrm { c m }$ to $\\mathbf { 0 . 3 6 c m }$ compared to the previous state-of-the-art (SOTA) [266]","[112] N. Keetha, J. Karhade, K. M. Jatavallabhula, G. Yang, S. Scherer, D. Ramanan, and J. Luiten, “Splatam: Splat, track & map 3d Pattern Recognit., 2024.;[266] E. Sandstr ¨om, Y. Li, L. Van Gool, and M. R. Oswald, “Point-slam: Dense neural point cloud-based slam,” in Proc. IEEE Int. Conf. Comput. Vis., 2023, pp. 18 433–18 444."
"Dataset: The same dataset as in Sec. 6.1, i.e., Replica [261], is used for comparison","[261] J. Straub, T. Whelan, L. Ma, Y. Chen, E. Wijmans, S. Green, J. J. Engel, R. Mur-Artal, C. Ren, S. Verma et al., “The replica dataset: A digital replica of indoor spaces,” arXiv preprint arXiv:1906.05797, 2019."
The testing views are the same as those collected by [262],"[262] E. Sucar, S. Liu, J. Ortiz, and A. J. Davison, “imap: Implicit mapping and positioning in real-time,” in Proc. IEEE Int. Conf. Comput. Vis., 2021, pp. 6229–6238."
"TABLE 3 Comparison of mapping methods (§6.2) on Replica [261] (static scenes), in terms of PSNR, SSIM, and LPIPS","[261] J. Straub, T. Whelan, L. Ma, Y. Chen, E. Wijmans, S. Green, J. J. Engel, R. Mur-Artal, C. Ren, S. Verma et al., “The replica dataset: A digital replica of indoor spaces,” arXiv preprint arXiv:1906.05797, 2019."
The results for FPS are taken from [113] using one 4090 GPU,"[113] H. Matsuki, R. Murai, P. H. Kelly, and A. J. Davison, “Gaussian splatting slam,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024."
"Benchmarking Algorithms: For performance comparison, we involve four recent papers which introduce 3D Gaussians into their systems [111]–[114], as well as three dense SLAM methods [263], [264], [266]","[111] C. Yan, D. Qu, D. Wang, D. Xu, Z. Wang, B. Zhao, and X. Li, “Gs-slam: Dense visual slam with 3d gaussian splatting,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[112] N. Keetha, J. Karhade, K. M. Jatavallabhula, G. Yang, S. Scherer, D. Ramanan, and J. Luiten, “Splatam: Splat, track & map 3d Pattern Recognit., 2024.;[113] H. Matsuki, R. Murai, P. H. Kelly, and A. J. Davison, “Gaussian splatting slam,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[114] V. Yugay, Y. Li, T. Gevers, and M. R. Oswald, “Gaussian-slam: Photo-realistic dense slam with gaussian splatting,” arXiv preprint arXiv:2312.10070, 2023.;[263] X. Yang, H. Li, H. Zhai, Y. Ming, Y. Liu, and G. Zhang, “Voxfusion: Dense tracking and mapping with voxel-based neural implicit representation,” in IEEE International Symposium on Mixed and Augmented Reality, 2022, pp. 499–507.;[264] Z. Zhu, S. Peng, V. Larsson, W. Xu, H. Bao, Z. Cui, M. R. Oswald, and M. Pollefeys, “Nice-slam: Neural implicit scalable encoding for slam,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2022, pp. 12 786–12 796.;[266] E. Sandstr ¨om, Y. Li, L. Van Gool, and M. R. Oswald, “Point-slam: Dense neural point cloud-based slam,” in Proc. IEEE Int. Conf. Comput. Vis., 2023, pp. 18 433–18 444."
"Evaluation Metric: Peak signal-to-noise ratio (PSNR), structural similarity (SSIM) [300], and learned perceptual image patch similarity (LPIPS) [301] are used for measuring RGB rendering performance","[300] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality assessment: from error visibility to structural similarity,” IEEE Trans. Image Process., vol. 13, no. 4, pp. 600–612, 2004.;[301] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The unreasonable effectiveness of deep features as a perceptual metric,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 586–595."
"For example, Gaussian-SLAM [114] establishes new SOTA and outperforms previous methods by a large margin","[114] V. Yugay, Y. Li, T. Gevers, and M. R. Oswald, “Gaussian-slam: Photo-realistic dense slam with gaussian splatting,” arXiv preprint arXiv:2312.10070, 2023."
"Compared to Point-SLAM [266], GSSLAM [113] is about 578 times faster in achieving very competitive accuracy","[113] H. Matsuki, R. Murai, P. H. Kelly, and A. J. Davison, “Gaussian splatting slam,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[266] E. Sandstr ¨om, Y. Li, L. Van Gool, and M. R. Oswald, “Point-slam: Dense neural point cloud-based slam,” in Proc. IEEE Int. Conf. Comput. Vis., 2023, pp. 18 433–18 444."
"In contrast to previous method [266] that relies on depth information, such as depth-guided ray sampling, for synthesizing novel views, 3D GS based system eliminates this need, allowing for high-fidelity rendering for any views","[266] E. Sandstr ¨om, Y. Li, L. Van Gool, and M. R. Oswald, “Point-slam: Dense neural point cloud-based slam,” in Proc. IEEE Int. Conf. Comput. Vis., 2023, pp. 18 433–18 444."
"Dataset: D-NeRF [184] dataset includes videos with 50 to 200 frames each, captured from unique viewpoints","[184] A. Pumarola, E. Corona, G. Pons-Moll, and F. Moreno-Noguer, “D-nerf: Neural radiance fields for dynamic scenes,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2021, pp. 10 318–10 327."
The testing views are the same as the original paper [184],"[184] A. Pumarola, E. Corona, G. Pons-Moll, and F. Moreno-Noguer, “D-nerf: Neural radiance fields for dynamic scenes,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2021, pp. 10 318–10 327."
"Benchmarking Algorithms: For performance comparison, we involve five recent papers that model dynamic scenes with 3D GS [93]–[95], [126], [132], as well as six NeRF based approaches [37], [184], [187], [302]–[304]","[37] S. Fridovich-Keil, G. Meanti, F. R. Warburg, B. Recht, and A. Kanazawa, “K-planes: Explicit radiance fields in space, time, and appearance,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2023, pp. 12 479–12 488.;[93] Z. Yang, H. Yang, Z. Pan, X. Zhu, and L. Zhang, “Real-time photorealistic dynamic scene representation and rendering with 4d gaussian splatting,” in Proc. Int. Conf. Learn. Represent., 2024.;[94] Z. Yang, X. Gao, W. Zhou, S. Jiao, Y. Zhang, and X. Jin, “Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[95] G. Wu, T. Yi, J. Fang, L. Xie, X. Zhang, W. Wei, W. Liu, Q. Tian, and X. Wang, $^ { \\prime \\prime } { 4 \\mathrm { d } }$ gaussian splatting for real-time dynamic scene rendering,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[126] H. Yu, J. Julin, Z. ´A. Milacski, K. Niinuma, and L. A. Jeni, “Cogs: Controllable gaussian splatting,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[132] Z. Lu, X. Guo, L. Hui, T. Chen, M. Yang, X. Tang, F. Zhu, and Y. Dai, $"" 3 \\mathrm { d }$ geometry-aware deformable gaussian splatting for dynamic view synthesis,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[184] A. Pumarola, E. Corona, G. Pons-Moll, and F. Moreno-Noguer, “D-nerf: Neural radiance fields for dynamic scenes,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2021, pp. 10 318–10 327.;[187] X. Guo, J. Sun, Y. Dai, G. Chen, X. Ye, X. Tan, E. Ding, Y. Zhang, and J. Wang, “Forward flow for novel view synthesis of dynamic scenes,” in Proc. IEEE Int. Conf. Comput. Vis., 2023, pp. 16 022– 16 033.;[302] J. Fang, T. Yi, X. Wang, L. Xie, X. Zhang, W. Liu, M. Nießner, and Q. Tian, “Fast dynamic radiance fields with time-aware neural voxels,” in SIGGRAPH Asia, 2022, pp. 1–9.;[303] A. Cao and J. Johnson, “Hexplane: A fast representation for dynamic scenes,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2023, pp. 130–141.;[304] F. Wang, Z. Chen, G. Wang, Y. Song, and H. Liu, “Masked spacetime hash encoding for efficient dynamic scene reconstruction,” in Proc. Adv. Neural Inf. Process. Syst., 2023."
"Evaluation Metric: The same metrics as in Sec. 6.2, i.e., PSNR, SSIM [300], and LPIPS [301], are used for evaluation","[300] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality assessment: from error visibility to structural similarity,” IEEE Trans. Image Process., vol. 13, no. 4, pp. 600–612, 2004.;[301] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The unreasonable effectiveness of deep features as a perceptual metric,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 586–595."
"The static version of 3D GS [10] fails to reconstruct dynamic scenes, resulting in a sharp drop in performance","[10] B. Kerbl, G. Kopanas, T. Leimk ¨uhler, and G. Drettakis, “3d gaussian splatting for real-time radiance field rendering,” ACM Trans. Graph., vol. 42, no. 4, 2023."
"By modeling the dynamics, D-3DGS [94] outperforms the SOTA method, FFDNeRF [187], by 6.83dB in terms of PSNR","[94] Z. Yang, X. Gao, W. Zhou, S. Jiao, Y. Zhang, and X. Jin, “Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[187] X. Guo, J. Sun, Y. Dai, G. Chen, X. Ye, X. Tan, E. Ding, Y. Zhang, and J. Wang, “Forward flow for novel view synthesis of dynamic scenes,” in Proc. IEEE Int. Conf. Comput. Vis., 2023, pp. 16 022– 16 033."
"TABLE 4 Comparison of reconstruction methods (§6.3) on D-NeRF [184] (dynamic scenes), in terms of PSNR, SSIM, and LPIPS. ∗ denotes results reported in [95]","[95] G. Wu, T. Yi, J. Fang, L. Xie, X. Zhang, W. Wei, W. Liu, Q. Tian, and X. Wang, $^ { \\prime \\prime } { 4 \\mathrm { d } }$ gaussian splatting for real-time dynamic scene rendering,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[184] A. Pumarola, E. Corona, G. Pons-Moll, and F. Moreno-Noguer, “D-nerf: Neural radiance fields for dynamic scenes,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2021, pp. 10 318–10 327."
"Human avatar modeling aims to create the model of human avatars from a given multi-view video.  
• Dataset: ZJU-MoCap [292] is a prevalent benchmark in human modeling from videos, captured with 23 synchronized cameras at a $1 0 2 4 \\times 1 0 2 4$ resolution","[292] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, “Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2021, pp. 9054– 9063."
"Six subjects (i.e., 377, 386, 387, 392, 393, and 394) are used for evaluation [305]","[305] C.-Y. Weng, B. Curless, P. P. Srinivasan, J. T. Barron, and I. Kemelmacher-Shlizerman, “Humannerf: Free-viewpoint rendering of moving people from monocular video,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2022, pp. 16 210–16 220."
The same testing views following [306] are adopted,"[306] C. Geng, S. Peng, Z. Xu, H. Bao, and X. Zhou, “Learning neural volumetric representations of dynamic humans in minutes,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2023, pp. 8759– 8770."
"Benchmarking Algorithms: For performance comparison, we involve three recent papers which model human avatar with 3D GS [145], [146], [249], as well as six human rendering approaches [292], [305]–[309]","[145] S. Hu and Z. Liu, “Gauhuman: Articulated gaussian splatting from monocular human videos,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[146] J. Lei, Y. Wang, G. Pavlakos, L. Liu, and K. Daniilidis, “Gart: Gaussian articulated template models,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[249] Z. Qian, S. Wang, M. Mihajlovic, A. Geiger, and S. Tang, “3dgsavatar: Animatable avatars via deformable 3d gaussian splatting,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[292] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, “Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2021, pp. 9054– 9063.;[305] C.-Y. Weng, B. Curless, P. P. Srinivasan, J. T. Barron, and I. Kemelmacher-Shlizerman, “Humannerf: Free-viewpoint rendering of moving people from monocular video,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2022, pp. 16 210–16 220.;[306] C. Geng, S. Peng, Z. Xu, H. Bao, and X. Zhou, “Learning neural volumetric representations of dynamic humans in minutes,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2023, pp. 8759– 8770.;[307] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, “Animatable neural radiance fields for modeling dynamic human bodies,” in Proc. IEEE Int. Conf. Comput. Vis., 2021, pp. 14 314–14 323.;[308] A. Yu, V. Ye, M. Tancik, and A. Kanazawa, “pixelnerf: Neural radiance fields from one or few images,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2021, pp. 4578–4587.;[309] Y. Kwon, D. Kim, D. Ceylan, and H. Fuchs, “Neural human performer: Learning generalizable radiance fields for human performance rendering,” in Proc. Adv. Neural Inf. Process. Syst., 2021, pp. 24 741–24 752."
"Evaluation Metric: PSNR, SSIM [300], and LPIPS\\* [301] are used for measuring RGB rendering performance","[300] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality assessment: from error visibility to structural similarity,” IEEE Trans. Image Process., vol. 13, no. 4, pp. 600–612, 2004.;[301] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The unreasonable effectiveness of deep features as a perceptual metric,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 586–595."
"TABLE 5 Comparison of reconstruction methods (§6.4) on ZJU-MoCap [292] (avatar), in terms of PSNR, SSIM, and LPIPS\\*","[292] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, “Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2021, pp. 9054– 9063."
The results for non-GS methods are taken from [146],"[146] J. Lei, Y. Wang, G. Pavlakos, L. Liu, and K. Daniilidis, “Gart: Gaussian articulated template models,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024."
"TABLE 6 Comparison of reconstruction methods (§6.5) on EndoNeRF [258] (surgical scenes), in terms of PSNR, SSIM, and LPIPS","[258] Y. Wang, Y. Long, S. H. Fan, and Q. Dou, “Neural rendering for stereo 3d reconstruction of deformable tissues in robotic surgery,” in Proc. Int. Conf. Med. Image Comput. Comput. Assist. Interv., 2022, pp. 431–441."
The results for non-GS methods are taken from [153],"[153] Y. Liu, C. Li, C. Yang, and Y. Yuan, “Endogaussian: Gaussian splatting for deformable surgical scene reconstruction,” arXiv preprint arXiv:2401.12561, 2024."
"FPS and GPU usage for training (Mem.) are measured using one 4090 GPU [153].   
  
• Result: Table 5 presents the numerical results of topleading solutions in human avatar modeling","[153] Y. Liu, C. Li, C. Yang, and Y. Yuan, “Endogaussian: Gaussian splatting for deformable surgical scene reconstruction,” arXiv preprint arXiv:2401.12561, 2024."
"For instance, GART [146] outperforms current SOTA, Instant-NVR [306], by 1.21dB in terms of PSNR","[146] J. Lei, Y. Wang, G. Pavlakos, L. Liu, and K. Daniilidis, “Gart: Gaussian articulated template models,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[306] C. Geng, S. Peng, Z. Xu, H. Bao, and X. Zhou, “Learning neural volumetric representations of dynamic humans in minutes,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2023, pp. 8759– 8770."
"Dataset: EndoNeRF [258] dataset presents a specialized collection of stereo camera captures, comprising two samples of in-vivo prostatectomy","[258] Y. Wang, Y. Long, S. H. Fan, and Q. Dou, “Neural rendering for stereo 3d reconstruction of deformable tissues in robotic surgery,” in Proc. Int. Conf. Med. Image Comput. Comput. Assist. Interv., 2022, pp. 431–441."
The same testing views as in [260] are used,"[260] R. Zha, X. Cheng, H. Li, M. Harandi, and Z. Ge, “Endosurf: Neural surface reconstruction of deformable tissues with stereo endoscope videos,” in Proc. Int. Conf. Med. Image Comput. Comput. Assist. Interv., 2023, pp. 13–23."
"Benchmarking Algorithms: For performance comparison, we involve three recent papers which reconstruct dynamic 3D endoscopic scenes with GS [152], [153], [155], as well as three NeRF-based surgical reconstruction approaches [258]– [260]","[152] Y. Huang, B. Cui, L. Bai, Z. Guo, M. ${ \\tt X u } ,$ and H. Ren, “Endo4dgs: Distilling depth ranking for endoscopic monocular scene reconstruction with 4d gaussian splatting,” in Proc. Int. Conf. Med. Image Comput. Comput. Assist. Interv., 2024.;[153] Y. Liu, C. Li, C. Yang, and Y. Yuan, “Endogaussian: Gaussian splatting for deformable surgical scene reconstruction,” arXiv preprint arXiv:2401.12561, 2024.;[155] H. Zhao, X. Zhao, L. Zhu, W. Zheng, and Y. Xu, “Hfgs: 4d gaussian splatting with emphasis on spatial and temporal highfrequency components for endoscopic scene reconstruction,” arXiv preprint arXiv:2405.17872, 2024.;[258] Y. Wang, Y. Long, S. H. Fan, and Q. Dou, “Neural rendering for stereo 3d reconstruction of deformable tissues in robotic surgery,” in Proc. Int. Conf. Med. Image Comput. Comput. Assist. Interv., 2022, pp. 431–441.;[259] C. Yang, K. Wang, Y. Wang, X. Yang, and W. Shen, “Neural lerplane representations for fast 4d reconstruction of deformable tissues,” in Proc. Int. Conf. Med. Image Comput. Comput. Assist. Interv., 2023, pp. 46–56.;[260] R. Zha, X. Cheng, H. Li, M. Harandi, and Z. Ge, “Endosurf: Neural surface reconstruction of deformable tissues with stereo endoscope videos,” in Proc. Int. Conf. Med. Image Comput. Comput. Assist. Interv., 2023, pp. 13–23."
"Evaluation Metric: PSNR, SSIM [300], and LPIPS [301] are adopted for evaluation","[300] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality assessment: from error visibility to structural similarity,” IEEE Trans. Image Process., vol. 13, no. 4, pp. 600–612, 2004.;[301] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The unreasonable effectiveness of deep features as a perceptual metric,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 586–595."
"For instance, EndoGaussian [153] outperforms a strong baseline, LerPlane- $\\boldsymbol { \\cdot } 3 2 \\mathbf { k }$ [259], among all metrics","[153] Y. Liu, C. Li, C. Yang, and Y. Yuan, “Endogaussian: Gaussian splatting for deformable surgical scene reconstruction,” arXiv preprint arXiv:2401.12561, 2024.;[259] C. Yang, K. Wang, Y. Wang, X. Yang, and W. Shen, “Neural lerplane representations for fast 4d reconstruction of deformable tissues,” in Proc. Int. Conf. Med. Image Comput. Comput. Assist. Interv., 2023, pp. 46–56."
"While significant progress has been made in physics (Sec. 5.7) and semantics [310]–[315] individually, there remains considerable untapped potential in their synergistic integration","[310] J. Wang, Z. Zhang, Q. Zhang, J. Li, J. Sun, M. Sun, J. He, and R. Xu, “Query-based semantic gaussian field for scene representation in reinforcement learning,” arXiv preprint arXiv:2406.02370, 2024.;[311] Y. Qu, S. Dai, X. Li, J. Lin, L. Cao, S. Zhang, and R. Ji, “Goi: Find 3d gaussians of interest with an optimizable open-vocabulary semantic-space hyperplane,” arXiv preprint arXiv:2405.17596, 2024.;[312] Y. Ji, H. Zhu, J. Tang, W. Liu, Z. Zhang, Y. Xie, L. Ma, and X. Tan, “Fastlgs: Speeding up language embedded gaussians with feature grid mapping,” arXiv preprint arXiv:2406.01916, 2024.;[313] G. Liao, J. Li, Z. Bao, X. Ye, J. Wang, Q. Li, and K. Liu, “Clip-gs: Clip-informed gaussian splatting for real-time and view-consistent 3d semantic understanding,” arXiv preprint arXiv:2404.14249, 2024.;[314] S. Choi, H. Song, J. Kim, T. Kim, and H. Do, “Click-gaussian: Interactive segmentation to any 3d gaussians,” arXiv preprint arXiv:2407.11793, 2024.;[315] S. Ji, G. Wu, J. Fang, J. Cen, T. Yi, W. Liu, Q. Tian, and X. Wang, “Segment any 4d gaussians,” arXiv preprint arXiv:2407.04504, 2024."
"For instance, incorporating prior knowledge such as the general shape of objects can reduce the need for extensive training viewpoints [47], [48] while improving geometry/surface reconstruction [77], [316]","[47] D. Charatan, S. Li, A. Tagliasacchi, and V. Sitzmann, “pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[48] S. Szymanowicz, C. Rupprecht, and A. Vedaldi, “Splatter image: Ultra-fast single-view 3d reconstruction,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[77] Y. Li, C. Lyu, Y. Di, G. Zhai, G. H. Lee, and F. Tombari, “Geogaussian: Geometry-aware gaussian splatting for scene rendering,” arXiv preprint arXiv:2403.11324, 2024.;[316] A. Gue´don and V. Lepetit, “Sugar: Surface-aligned gaussian splatting for efficient 3d mesh reconstruction and high-quality mesh rendering,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024."
"A critical metric for assessing scene representation is the quality of its generated scenes, which encompasses challenges in geometry, texture, and lighting fidelity [66], [128], [141]","[66] J. Gao, C. Gu, Y. Lin, H. Zhu, X. Cao, L. Zhang, and Y. Yao, “Relightable 3d gaussian: Real-time point cloud relighting with brdf decomposition and ray tracing,” arXiv preprint arXiv:2311.16043, 2023.;[128] Y.-H. Huang, Y.-T. Sun, Z. Yang, X. Lyu, Y.-P. Cao, and X. Qi, “Sc-gs: Sparse-controlled gaussian splatting for editable dynamic scenes,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[141] Y. Chen, Z. Chen, C. Zhang, F. Wang, X. Yang, Y. Wang, Z. Cai, L. Yang, H. Liu, and G. Lin, “Gaussianeditor: Swift and controllable 3d editing with gaussian splatting,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024."
"By merging physical principles and semantic information within the 3D GS framework, one can expect that the quality will be enhanced, thereby facilitating dynamics modeling [21], [166], editing [90], [92], generation [133], [134], and beyond","[21] Y. Jiang, C. Yu, T. Xie, X. Li, Y. Feng, H. Wang, M. Li, H. Lau, F. Gao, Y. Yang et al., “Vr-gs: A physical dynamics-aware interactive gaussian splatting system in virtual reality,” arXiv preprint arXiv:2401.16663, 2024.;[90] S. Zhou, H. Chang, S. Jiang, Z. Fan, Z. Zhu, D. Xu, P. Chari, S. You, Z. Wang, and A. Kadambi, “Feature 3dgs: Supercharging 3d gaussian splatting to enable distilled feature fields,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[92] J. Cen, J. Fang, C. Yang, L. Xie, X. Zhang, W. Shen, and Q. Tian, “Segment any 3d gaussians,” arXiv preprint arXiv:2312.00860, 2023.;[133] J. Tang, J. Ren, H. Zhou, Z. Liu, and G. Zeng, “Dreamgaussian: Generative gaussian splatting for efficient 3d content creation,” in Proc. Int. Conf. Learn. Represent., 2024.;[134] T. Yi, J. Fang, J. Wang, G. Wu, L. Xie, X. Zhang, W. Liu, Q. Tian, and X. Wang, “Gaussiandreamer: Fast generation from text to 3d gaussians by bridging 2d and 3d diffusion models,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[166] T. Xie, Z. Zong, Y. Qiu, X. Li, Y. Feng, Y. Yang, and C. Jiang, “Physgaussian: Physics-integrated 3d gaussians for generative dynamics,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024."
"Notable efforts in related area include the continuum mechanics based GS systems (Sec. 5.7), and the generalizable Gaussian representation based on multi-view stereo [317]","[317] T. Liu, G. Wang, S. Hu, L. Shen, X. Ye, Y. Zang, Z. Cao, W. Li, and Z. Liu, “Fast generalizable gaussian splatting reconstruction from multi-view stereo,” in Proc. Eur. Conf. Comput. Vis., 2024."
Li et al. [318] used 3D Gaussians with density control as the basis for the volumetric representation and did not involve the splatting process,"[318] Y. Li, X. Fu, S. Zhao, R. Jin, and S. K. Zhou, “Sparse-view ct reconstruction with 3d gaussian volumetric representation,” arXiv preprint arXiv:2312.15676, 2023."
X-Gaussian [319] involves the splatting process for fast training and inference but cannot generate volumetric representation,"[319] Y. Cai, Y. Liang, J. Wang, A. Wang, Y. Zhang, X. Yang, Z. Zhou, and A. Yuille, “Radiative gaussian splatting for efficient x-ray novel view synthesis,” in Proc. Eur. Conf. Comput. Vis., 2024."
"While early efforts [188]–[190] in reconstructing urban/street scenes with 3D GS have been encouraging, they are just the tip of the iceberg in terms of the full capabilities","[188] X. Zhou, Z. Lin, X. Shan, Y. Wang, D. Sun, and M.-H. Yang, “Drivinggaussian: Composite gaussian splatting for surrounding dynamic autonomous driving scenes,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.;[189] Y. Yan, H. Lin, C. Zhou, W. Wang, H. Sun, K. Zhan, X. Lang, X. Zhou, and S. Peng, “Street gaussians for modeling dynamic urban scenes,” in Proc. Eur. Conf. Comput. Vis., 2024.;[190] H. Zhou, J. Shao, L. Xu, D. Bai, W. Qiu, B. Liu, Y. Wang, A. Geiger, and Y. Liao, “Hugs: Holistic urban 3d scene understanding via gaussian splatting,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024, pp. 21 336–21 345."
"Moreover, recent studies have begun to unveil the capability of 3D GS in several domains, e.g., point cloud registration [320], image representation and compression [60], and fluid synthesis [171]","[60] X. Zhang, X. Ge, T. Xu, D. He, Y. Wang, H. Qin, G. Lu, J. Geng, and J. Zhang, “Gaussianimage: 1000 fps image representation and compression by 2d gaussian splatting,” in Proc. Eur. Conf. Comput. Vis., 2024.;[171] Y. Feng, X. Feng, Y. Shang, Y. Jiang, C. Yu, Z. Zong, T. Shao, H. Wu, K. Zhou, C. Jiang et al., “Gaussian splashing: Dynamic fluid synthesis with gaussian splatting,” arXiv preprint arXiv:2401.15318, 2024.;[320] J. Chang, Y. Xu, Y. Li, Y. Chen, and X. Han, “Gaussreg: Fast 3d registration with gaussian splatting,” in Proc. Eur. Conf. Comput. Vis., 2024."
