[
    "[1] S. J. Gortler, R. Grzeszczuk, R. Szeliski, and M. F. Cohen, “The lumigraph,” in Seminal Graphics Papers: Pushing the Boundaries, Volume 2, 2023, pp. 453–464.",
    "[2] M. Levoy and P. Hanrahan, “Light field rendering,” in Seminal Graphics Papers: Pushing the Boundaries, Volume 2, 2023, pp. 441– 452.",
    "[3] C. Buehler, M. Bosse, L. McMillan, S. Gortler, and M. Cohen, “Unstructured lumigraph rendering,” in Seminal Graphics Papers: Pushing the Boundaries, Volume 2, 2023, pp. 497–504.",
    "[4] N. Snavely, S. M. Seitz, and R. Szeliski, “Photo tourism: exploring photo collections in $3 \\mathrm { d } , \\prime \\prime$ in ACM Trans. Graph., 2006, pp. 835–846.",
    "[5] M. Goesele, N. Snavely, B. Curless, H. Hoppe, and S. M. Seitz, “Multi-view stereo for community photo collections,” in Proc. IEEE Int. Conf. Comput. Vis., 2007, pp. 1–8.",
    "[6] S. J. Garbin, M. Kowalski, M. Johnson, J. Shotton, and J. Valentin, “Fastnerf: High-fidelity neural rendering at 200fps,” in Proc. IEEE Int. Conf. Comput. Vis., 2021, pp. 14 346–14 355.",
    "[7] C. Reiser, S. Peng, Y. Liao, and A. Geiger, “Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps,” in Proc. IEEE Int. Conf. Comput. Vis., 2021, pp. 14 335–14 345.",
    "[8] J. T. Barron, B. Mildenhall, D. Verbin, P. P. Srinivasan, and P. Hedman, “Mip-nerf 360: Unbounded anti-aliased neural radiance fields,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2022, pp. 5470–5479.",
    "[9] T. M ¨uller, A. Evans, C. Schied, and A. Keller, “Instant neural graphics primitives with a multiresolution hash encoding,” ACM Trans. Graph., vol. 41, no. 4, pp. 1–15, 2022.",
    "[10] B. Kerbl, G. Kopanas, T. Leimk ¨uhler, and G. Drettakis, “3d gaussian splatting for real-time radiance field rendering,” ACM Trans. Graph., vol. 42, no. 4, 2023.",
    "[11] V. Sitzmann, J. Thies, F. Heide, M. Nießner, G. Wetzstein, and M. Zollhofer, “Deepvoxels: Learning persistent 3d feature embeddings,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019, pp. 2437–2446.",
    "[12] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng, “Nerf: Representing scenes as neural radiance fields for view synthesis,” in Proc. Eur. Conf. Comput. Vis., 2020, pp. 405–421.",
    "[13] H. Pfister, M. Zwicker, J. Van Baar, and M. Gross, “Surfels: Surface elements as rendering primitives,” in Proceedings of the 27th annual conference on Computer graphics and interactive techniques, 2000, pp. 335–342.",
    "[14] M. Zwicker, H. Pfister, J. Van Baar, and M. Gross, “Surface splatting,” in Proceedings of the 28th annual conference on Computer graphics and interactive techniques, 2001, pp. 371–378.",
    "[15] L. Ren, H. Pfister, and M. Zwicker, “Object space ewa surface splatting: A hardware accelerated approach to high quality point rendering,” in Comput. Graph. Forum, no. 3, 2002, pp. 461–470.",
    "[16] W. Yifan, F. Serena, S. Wu, C. O¨ ztireli, and O. Sorkine-Hornung, “Differentiable surface splatting for point-based geometry processing,” ACM Trans. Graph., vol. 38, no. 6, pp. 1–14, 2019.",
    "[17] O. Wiles, G. Gkioxari, R. Szeliski, and J. Johnson, “Synsin: Endto-end view synthesis from a single image,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2020, pp. 7467–7477.",
    "[18] D. Kalkofen, E. Mendez, and D. Schmalstieg, “Comprehensible visualization for augmented reality,” IEEE Trans. Vis. Comput. Graph., vol. 15, no. 2, pp. 193–204, 2008.",
    "[19] A. Patney, M. Salvi, J. Kim, A. Kaplanyan, C. Wyman, N. Benty, D. Luebke, and A. Lefohn, “Towards foveated rendering for gazetracked virtual reality,” ACM Trans. Graph., vol. 35, no. 6, pp. 1–12, 2016.",
    "[20] R. Albert, A. Patney, D. Luebke, and J. Kim, “Latency requirements for foveated rendering in virtual reality,” ACM Transactions on Applied Perception, vol. 14, no. 4, pp. 1–13, 2017.",
    "[21] Y. Jiang, C. Yu, T. Xie, X. Li, Y. Feng, H. Wang, M. Li, H. Lau, F. Gao, Y. Yang et al., “Vr-gs: A physical dynamics-aware interactive gaussian splatting system in virtual reality,” arXiv preprint arXiv:2401.16663, 2024.",
    "[22] T. Lu, M. Yu, L. Xu, Y. Xiangli, L. Wang, D. Lin, and B. Dai, “Scaffold-gs: Structured 3d gaussians for view-adaptive rendering,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[23] S. Saito, G. Schwartz, T. Simon, J. Li, and G. Nam, “Relightable gaussian codec avatars,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[24] T. Zhang, K. Huang, W. Zhi, and M. Johnson-Roberson, “Darkgs: Learning neural illumination and 3d gaussians relighting for robotic exploration in the dark,” in Proc. IEEE/RSJ Int. Conf. Intell. Robot. Syst., 2024.",
    "[25] B. Fei, J. Xu, R. Zhang, Q. Zhou, W. Yang, and Y. He, “3d gaussian splatting as new era: A survey,” IEEE Trans. Vis. Comput. Graph., 2024.",
    "[26] A. Dalal, D. Hagen, K. G. Robbersmyr, and K. M. Knausg˚ard, “Gaussian splatting: 3d reconstruction and novel view synthesis, a review,” IEEE Access, 2024.",
    "[27] Y. Bao, T. Ding, J. Huo, Y. Liu, Y. Li, W. Li, Y. Gao, and J. Luo, “3d gaussian splatting: Survey, technologies, challenges, and opportunities,” arXiv preprint arXiv:2407.17418, 2024.",
    "[28] T. Wu, Y.-J. Yuan, L.-X. Zhang, J. Yang, Y.-P. Cao, L.-Q. Yan, and L. Gao, “Recent advances in 3d gaussian splatting,” Comput. Vis. Media, pp. 1–30, 2024.",
    "[29] L. Kobbelt and M. Botsch, “A survey of point-based techniques in computer graphics,” Comput. Graph., vol. 28, no. 6, pp. 801–814, 2004.",
    "[30] Y. Xie, T. Takikawa, S. Saito, O. Litany, S. Yan, N. Khan, F. Tombari, J. Tompkin, V. Sitzmann, and S. Sridhar, “Neural fields in visual computing and beyond,” in Comput. Graph. Forum, no. 2, 2022, pp. 641–676.",
    "[31] W. Wang, Y. Yang, and Y. Pan, “Visual knowledge in the big model era: Retrospect and prospect,” arXiv preprint arXiv:2404.04308, 2024.",
    "[32] A. Tewari, J. Thies, B. Mildenhall, P. Srinivasan, E. Tretschk, W. Yifan, C. Lassner, V. Sitzmann, R. Martin-Brualla, S. Lombardi et al., “Advances in neural rendering,” in Comput. Graph. Forum, no. 2, 2022, pp. 703–735.",
    "[33] X.-F. Han, H. Laga, and M. Bennamoun, “Image-based 3d object reconstruction: State-of-the-art and trends in the deep learning era,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 43, no. 5, pp. 1578–1604, 2019.",
    "[34] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and A. Geiger, “Occupancy networks: Learning 3d reconstruction in function space,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019, pp. 4460–4470.",
    "[35] J. J. Park, P. Florence, J. Straub, R. Newcombe, and S. Lovegrove, “Deepsdf: Learning continuous signed distance functions for shape representation,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019, pp. 165–174.",
    "[36] C. Sun, M. Sun, and H.-T. Chen, “Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2022, pp. 5459– 5469.",
    "[37] S. Fridovich-Keil, G. Meanti, F. R. Warburg, B. Recht, and A. Kanazawa, “K-planes: Explicit radiance fields in space, time, and appearance,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2023, pp. 12 479–12 488.",
    "[38] J. P. Grossman and W. J. Dally, “Point sample rendering,” in Render. Tech., 1998, pp. 181–192.",
    "[39] M. Zwicker, H. Pfister, J. Van Baar, and M. Gross, “Ewa volume splatting,” in Proceedings Visualization, 2001. VIS’01., 2001, pp. 29– 538.",
    "[40] “Ewa splatting,” IEEE Trans. Vis. Comput. Graph., vol. 8, no. 3, pp. 223–238, 2002.",
    "[41] K.-A. Aliev, A. Sevastopolsky, M. Kolos, D. Ulyanov, and V. Lempitsky, “Neural point-based graphics,” in Proc. Eur. Conf. Comput. Vis., 2020, pp. 696–712.",
    "[42] D. R ¨uckert, L. Franke, and M. Stamminger, “Adop: Approximate differentiable one-pixel point rendering,” ACM Trans. Graph., vol. 41, no. 4, pp. 1–14, 2022.",
    "[43] C. Lassner and M. Zollhofer, “Pulsar: Efficient sphere-based neural rendering,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2021, pp. 1440–1449.",
    "[44] K. Cheng, X. Long, K. Yang, Y. Yao, W. Yin, Y. Ma, W. Wang, and X. Chen, “Gaussianpro: 3d gaussian splatting with progressive propagation,” in Proc. ACM Int. Conf. Mach. Learn., 2024.",
    "[45] H. Xiong, S. Muttukuru, R. Upadhyay, P. Chari, and A. Kadambi, “Sparsegs: Real-time 360 {\\deg} sparse view synthesis using gaussian splatting,” arXiv preprint arXiv:2312.00206, 2023.",
    "[46] Z. Zhu, Z. Fan, Y. Jiang, and Z. Wang, “Fsgs: Real-time fewshot view synthesis using gaussian splatting,” in Proc. Eur. Conf. Comput. Vis., 2024.",
    "[47] D. Charatan, S. Li, A. Tagliasacchi, and V. Sitzmann, “pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[48] S. Szymanowicz, C. Rupprecht, and A. Vedaldi, “Splatter image: Ultra-fast single-view 3d reconstruction,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[49] J. Li, J. Zhang, X. Bai, J. Zheng, X. Ning, J. Zhou, and L. Gu, “Dngaussian: Optimizing sparse-view 3d gaussian radiance fields with global-local depth normalization,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[50] A. Swann, M. Strong, W. K. Do, G. S. Camps, M. Schwager, and M. Kennedy III, “Touch-gs: Visual-tactile supervised 3d gaussian splatting,” arXiv preprint arXiv:2403.09875, 2024.",
    "[51] Y. Chen, H. Xu, C. Zheng, B. Zhuang, M. Pollefeys, A. Geiger, T.-J. Cham, and J. Cai, “Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images,” in Proc. Eur. Conf. Comput. Vis., 2024.",
    "[52] C. Wewer, K. Raj, E. Ilg, B. Schiele, and J. E. Lenssen, “latentsplat: Autoencoding variational gaussians for fast generalizable 3d reconstruction,” arXiv preprint arXiv:2403.16292, 2024.",
    "[53] Y. Xu, Z. Shi, W. Yifan, H. Chen, C. Yang, S. Peng, Y. Shen, and G. Wetzstein, “Grm: Large gaussian reconstruction model for efficient 3d reconstruction and generation,” arXiv preprint arXiv:2403.14621, 2024.",
    "[54] Q. Shen, X. Yi, Z. Wu, P. Zhou, H. Zhang, S. Yan, and X. Wang, “Gamba: Marry gaussian splatting with mamba for single view 3d reconstruction,” arXiv preprint arXiv:2403.18795, 2024.",
    "[55] J. Zhang, J. Li, X. Yu, L. Huang, L. Gu, J. Zheng, and X. Bai, “Corgs: Sparse-view 3d gaussian splatting via co-regularization,” in Proc. Eur. Conf. Comput. Vis., 2024.",
    "[56] Z. Fan, K. Wang, K. Wen, Z. Zhu, D. ${ \\tt X u } ,$ and Z. Wang, “Lightgaussian: Unbounded 3d gaussian compression with 15x reduction and $2 0 0 +$ fps,” arXiv preprint arXiv:2311.17245, 2023.",
    "[57] K. Navaneet, K. P. Meibodi, S. A. Koohpayegani, and H. Pirsiavash, “Compact3d: Compressing gaussian splat radiance field models with vector quantization,” arXiv preprint arXiv:2311.18159, 2023.",
    "[58] J. C. Lee, D. Rho, X. Sun, J. H. Ko, and E. Park, “Compact 3d gaussian representation for radiance field,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[59] W. Morgenstern, F. Barthel, A. Hilsmann, and P. Eisert, “Compact 3d scene representation via self-organizing gaussian grids,” arXiv preprint arXiv:2312.13299, 2023.",
    "[60] X. Zhang, X. Ge, T. Xu, D. He, Y. Wang, H. Qin, G. Lu, J. Geng, and J. Zhang, “Gaussianimage: 1000 fps image representation and compression by 2d gaussian splatting,” in Proc. Eur. Conf. Comput. Vis., 2024.",
    "[61] S. Niedermayr, J. Stumpfegger, and R. Westermann, “Compressed 3d gaussian splatting for accelerated novel view synthesis,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[62] Y. Chen, Q. Wu, J. Cai, M. Harandi, and W. Lin, “Hac: Hash-grid assisted context for 3d gaussian splatting compression,” in Proc. Eur. Conf. Comput. Vis., 2024.",
    "[63] P. Papantonakis, G. Kopanas, B. Kerbl, A. Lanvin, and G. Drettakis, “Reducing the memory footprint of 3d gaussian splatting,” in I3D, 2024, pp. 1–17.",
    "[64] G. Fang and B. Wang, “Mini-splatting: Representing scenes with a constrained number of gaussians,” arXiv preprint arXiv:2403.14166, 2024",
    "[65] Z. Yu, A. Chen, B. Huang, T. Sattler, and A. Geiger, “Mipsplatting: Alias-free 3d gaussian splatting,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024, pp. 19 447–19 456.",
    "[66] J. Gao, C. Gu, Y. Lin, H. Zhu, X. Cao, L. Zhang, and Y. Yao, “Relightable 3d gaussian: Real-time point cloud relighting with brdf decomposition and ray tracing,” arXiv preprint arXiv:2311.16043, 2023.",
    "[67] Z. Yan, W. F. Low, Y. Chen, and G. H. Lee, “Multi-scale 3d gaussian splatting for anti-aliased rendering,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[68] Y. Jiang, J. Tu, Y. Liu, X. Gao, X. Long, W. Wang, and Y. Ma, “Gaussianshader: 3d gaussian splatting with shading functions for reflective surfaces,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[69] B. Lee, H. Lee, X. Sun, U. Ali, and E. Park, “Deblurring 3d gaussian splatting,” arXiv preprint arXiv:2401.00834, 2024.",
    "[70] D. Malarz, W. Smolak, J. Tabor, S. Tadeja, and P. Spurek, “Gaussian splitting algorithm with color and opacity depended on viewing direction,” arXiv preprint arXiv:2312.13729, 2023.",
    "[71] L. Bolanos, S.-Y. Su, and H. Rhodin, “Gaussian shadow casting for neural characters,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[72] L. Radl, M. Steiner, M. Parger, A. Weinrauch, B. Kerbl, and M. Steinberger, “Stopthepop: Sorted gaussian splatting for viewconsistent real-time rendering,” ACM Trans. Graph., 2024.",
    "[73] Z. Yang, X. Gao, Y. Sun, Y. Huang, X. Lyu, W. Zhou, S. Jiao, X. $\\mathrm { \\ Q i , }$ and X. Jin, “Spec-gaussian: Anisotropic view-dependent appearance for 3d gaussian splatting,” arXiv preprint arXiv:2402.15870, 2024.",
    "[74] C. Peng, Y. Tang, Y. Zhou, N. Wang, X. Liu, D. Li, and R. Chellappa, “Bags: Blur agnostic gaussian splatting through multiscale kernel modeling,” arXiv preprint arXiv:2403.04926, 2024.",
    "[75] L. Zhao, P. Wang, and P. Liu, “Bad-gaussians: Bundle adjusted deblur gaussian splatting,” arXiv preprint arXiv:2403.11831, 2024.",
    "[76] H. Dahmani, M. Bennehar, N. Piasco, L. Roldao, and D. Tsishkou, “Swag: Splatting in the wild images with appearanceconditioned gaussians,” arXiv preprint arXiv:2403.10427, 2024.",
    "[77] Y. Li, C. Lyu, Y. Di, G. Zhai, G. H. Lee, and F. Tombari, “Geogaussian: Geometry-aware gaussian splatting for scene rendering,” arXiv preprint arXiv:2403.11324, 2024.",
    "[78] Z. Liang, Q. Zhang, W. Hu, Y. Feng, L. Zhu, and K. Jia, “Analyticsplatting: Anti-aliased 3d gaussian splatting via analytic integration,” arXiv preprint arXiv:2403.11056, 2024.",
    "[79] O. Seiskari, J. Ylilammi, V. Kaatrasalo, P. Rantalankila, M. Turkulainen, J. Kannala, E. Rahtu, and A. Solin, “Gaussian splatting on the move: Blur and rolling shutter compensation for natural camera motion,” arXiv preprint arXiv:2403.13327, 2024.",
    "[80] X. Song, J. Zheng, S. Yuan, H.-a. Gao, J. Zhao, X. He, W. Gu, and H. Zhao, “Sa-gs: Scale-adaptive gaussian splatting for trainingfree anti-aliasing,” arXiv preprint arXiv:2403.19615, 2024.",
    "[81] Y. Fu, S. Liu, A. Kulkarni, J. Kautz, A. A. Efros, and X. Wang, “Colmap-free 3d gaussian splatting,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[82] J. Jung, J. Han, H. An, J. Kang, S. Park, and S. Kim, “Relaxing accurate initialization constraint for 3d gaussian splatting,” arXiv preprint arXiv:2403.09413, 2024.",
    "[83] M. Yu, T. Lu, L. Xu, L. Jiang, Y. Xiangli, and B. Dai, “Gsdf: 3dgs meets sdf for improved rendering and reconstruction,” arXiv preprint arXiv:2403.16964, 2024.",
    "[84] J. Zhang, F. Zhan, M. Xu, S. Lu, and E. Xing, “Fregs: 3d gaussian splatting with progressive frequency regularization,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[85] L. Huang, J. Bai, J. Guo, and Y. Guo, $^ { \\prime \\prime } G s { + + }$ : Error analyzing and optimal gaussian splatting,” arXiv preprint arXiv:2402.00752, 2024.",
    "[86] J. Li, L. Cheng, Z. Wang, T. Mu, and J. He, “Loopgaussian: Creating 3d cinemagraph with multi-view images via eulerian motion field,” arXiv preprint arXiv:2404.08966, 2024.",
    "[87] J.-C. Shi, M. Wang, H.-B. Duan, and S.-H. Guan, “Language embedded 3d gaussians for open-vocabulary scene understanding,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[88] M. Qin, W. Li, J. Zhou, H. Wang, and H. Pfister, “Langsplat: 3d language gaussian splatting,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[89] X. Zuo, P. Samangouei, Y. Zhou, Y. Di, and M. Li, “Fmgs: Foundation model embedded 3d gaussian splatting for holistic 3d scene understanding,” arXiv preprint arXiv:2401.01970, 2024.",
    "[90] S. Zhou, H. Chang, S. Jiang, Z. Fan, Z. Zhu, D. Xu, P. Chari, S. You, Z. Wang, and A. Kadambi, “Feature 3dgs: Supercharging 3d gaussian splatting to enable distilled feature fields,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[91] M. Ye, M. Danelljan, F. Yu, and L. Ke, “Gaussian grouping: Segment and edit anything in 3d scenes,” in Proc. Eur. Conf. Comput. Vis., 2024.",
    "[92] J. Cen, J. Fang, C. Yang, L. Xie, X. Zhang, W. Shen, and Q. Tian, “Segment any 3d gaussians,” arXiv preprint arXiv:2312.00860, 2023.",
    "[93] Z. Yang, H. Yang, Z. Pan, X. Zhu, and L. Zhang, “Real-time photorealistic dynamic scene representation and rendering with 4d gaussian splatting,” in Proc. Int. Conf. Learn. Represent., 2024.",
    "[94] Z. Yang, X. Gao, W. Zhou, S. Jiao, Y. Zhang, and X. Jin, “Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[95] G. Wu, T. Yi, J. Fang, L. Xie, X. Zhang, W. Wei, W. Liu, Q. Tian, and X. Wang, $^ { \\prime \\prime } { 4 \\mathrm { d } }$ gaussian splatting for real-time dynamic scene rendering,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[96] Y. Xu, B. Chen, Z. Li, H. Zhang, L. Wang, Z. Zheng, and Y. Liu, “Gaussian head avatar: Ultra high-fidelity head avatar via dynamic gaussians,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[97] S. Szymanowicz, E. Insafutdinov, C. Zheng, D. Campbell, J. F. Henriques, C. Rupprecht, and A. Vedaldi, “Flash3d: Feedforward generalisable 3d scene reconstruction from a single image,” arXiv preprint arXiv:2406.04343, 2024.",
    "[98] K. Sargent, Z. Li, T. Shah, C. Herrmann, H.-X. Yu, Y. Zhang, E. R. Chan, D. Lagun, L. Fei-Fei, D. Sun et al., “Zeronvs: Zero-shot 360- degree view synthesis from a single image,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024, pp. 9420–9429.",
    "[99] J. Meng, H. Li, Y. Wu, Q. Gao, S. Yang, J. Zhang, and S. Ma, “Mirror-3dgs: Incorporating mirror reflections into 3d gaussian splatting,” arXiv preprint arXiv:2404.01168, 2024.",
    "[100] H. Chen, C. Li, and G. H. Lee, “Neusg: Neural implicit surface reconstruction with 3d gaussian splatting guidance,” arXiv preprint arXiv:2312.00846, 2023.",
    "[101] Z. Yu, T. Sattler, and A. Geiger, “Gaussian opacity fields: Efficient and compact surface reconstruction in unbounded scenes,” arXiv preprint arXiv:2404.10772, 2024.",
    "[102] B. Zhang, C. Fang, R. Shrestha, Y. Liang, X. Long, and P. Tan, “Rade-gs: Rasterizing depth in gaussian splatting,” arXiv preprint arXiv:2406.01467, 2024.",
    "[103] A. Chen, H. Xu, S. Esposito, S. Tang, and A. Geiger, “Lara: Efficient large-baseline radiance fields,” arXiv preprint arXiv:2407.04699, 2024.",
    "[104] E. Ververas, R. A. Potamias, J. Song, J. Deng, and S. Zafeiriou, “Sags: Structure-aware 3d gaussian splatting,” in Proc. Eur. Conf. Comput. Vis., 2024, pp. 221–238.",
    "[105] C. Smith, D. Charatan, A. Tewari, and V. Sitzmann, “Flowmap: High-quality camera poses, intrinsics, and depth via gradient descent,” arXiv preprint arXiv:2404.15259, 2024.",
    "[106] Y. Lin, Z. Dai, S. Zhu, and Y. Yao, “Gaussian-flow: 4d reconstruction with dynamic 3d gaussian particle,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[107] A. Saroha, M. Gladkova, C. Curreli, T. Yenamandra, and D. Cremers, “Gaussian splatting in style,” arXiv preprint arXiv:2403.08498, 2024.",
    "[108] N. Moenne-Loccoz, A. Mirzaei, O. Perel, R. de Lutio, J. Martinez Esturo, G. State, S. Fidler, N. Sharp, and Z. Gojcic, “3d gaussian ray tracing: Fast tracing of particle scenes,” ACM Trans. Graph., vol. 43, no. 6, pp. 1–19, 2024.",
    "[109] A. Mai, P. Hedman, G. Kopanas, D. Verbin, D. Futschik, Q. Xu, F. Kuester, J. T. Barron, and Y. Zhang, “Ever: Exact volumetric ellipsoid rendering for real-time view synthesis,” arXiv preprint arXiv:2410.01804, 2024.",
    "[110] J. Condor, S. Speierer, L. Bode, A. Bozic, S. Green, P. Didyk, and A. Jarabo, “Don’t splat your gaussians: Volumetric ray-traced primitives for modeling and rendering scattering and emissive media,” ACM Trans. Graph., 2025.",
    "[111] C. Yan, D. Qu, D. Wang, D. Xu, Z. Wang, B. Zhao, and X. Li, “Gs-slam: Dense visual slam with 3d gaussian splatting,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[112] N. Keetha, J. Karhade, K. M. Jatavallabhula, G. Yang, S. Scherer, D. Ramanan, and J. Luiten, “Splatam: Splat, track & map 3d Pattern Recognit., 2024.",
    "[113] H. Matsuki, R. Murai, P. H. Kelly, and A. J. Davison, “Gaussian splatting slam,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[114] V. Yugay, Y. Li, T. Gevers, and M. R. Oswald, “Gaussian-slam: Photo-realistic dense slam with gaussian splatting,” arXiv preprint arXiv:2312.10070, 2023.",
    "[115] H. Huang, L. Li, H. Cheng, and S.-K. Yeung, “Photo-slam: Realtime simultaneous localization and photorealistic mapping for monocular, stereo, and rgb-d cameras,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[116] M. Li, S. Liu, and H. Zhou, “Sgs-slam: Semantic gaussian splatting for neural dense slam,” in Proc. Eur. Conf. Comput. Vis., 2024.",
    "[117] Y. Ji, Y. Liu, G. Xie, B. Ma, and Z. Xie, “Neds-slam: A novel neural explicit dense semantic slam framework using 3d gaussian splatting,” arXiv preprint arXiv:2403.11679, 2024.",
    "[118] G. Lu, S. Zhang, Z. Wang, C. Liu, J. Lu, and Y. Tang, “Manigaussian: Dynamic gaussian splatting for multi-task robotic manipulation,” arXiv preprint arXiv:2403.08321, 2024.",
    "[119] J. Abou-Chakra, K. Rana, F. Dayoub, and N. S ¨underhauf, “Physically embodied gaussian splatting: A realtime correctable world model for robotics,” in Proc. Annu. Conf. Robot Learn., 2024.",
    "[120] O. Shorinwa, J. Tucker, A. Smith, A. Swann, T. Chen, R. Firoozi, M. D. Kennedy, and M. Schwager, “Splat-mover: Multi-stage, open-vocabulary robotic manipulation via editable gaussian splatting,” in Proc. Annu. Conf. Robot Learn., 2024.",
    "[121] M. Ji, R.-Z. Qiu, X. Zou, and X. Wang, “Graspsplats: Efficient manipulation with 3d feature splatting,” arXiv preprint arXiv:2409.02084, 2024.",
    "[122] Y. Zheng, X. Chen, Y. Zheng, S. Gu, R. Yang, B. Jin, P. Li, C. Zhong, Z. Wang, L. Liu et al., “Gaussiangrasper: 3d language gaussian splatting for open-vocabulary robotic grasping,” arXiv preprint arXiv:2403.09637, 2024.",
    "[123] S. Zhu, R. Qin, G. Wang, J. Liu, and H. Wang, “Semgaussslam: Dense semantic gaussian splatting slam,” arXiv preprint arXiv:2403.07494, 2024.",
    "[124] Z. Peng, T. Shao, Y. Liu, J. Zhou, Y. Yang, J. Wang, and K. Zhou, “Rtg-slam: Real-time 3d reconstruction at scale using gaussian splatting,” ACM Trans. Graph., 2024.",
    "[125] J. Luiten, G. Kopanas, B. Leibe, and D. Ramanan, “Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis,” in Proc. Int. Conf. 3D Vis., 2024.",
    "[126] H. Yu, J. Julin, Z. ´A. Milacski, K. Niinuma, and L. A. Jeni, “Cogs: Controllable gaussian splatting,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[127] R. Shao, J. Sun, C. Peng, Z. Zheng, B. Zhou, H. Zhang, and Y. Liu, “Control4d: Efficient 4d portrait editing with text,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[128] Y.-H. Huang, Y.-T. Sun, Z. Yang, X. Lyu, Y.-P. Cao, and X. Qi, “Sc-gs: Sparse-controlled gaussian splatting for editable dynamic scenes,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[129] D. Das, C. Wewer, R. Yunus, E. Ilg, and J. E. Lenssen, “Neural parametric gaussians for monocular non-rigid object reconstruction,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[130] Z. Li, Z. Chen, Z. Li, and Y. Xu, “Spacetime gaussian feature splatting for real-time dynamic view synthesis,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[131] J. Sun, H. Jiao, G. Li, Z. Zhang, L. Zhao, and W. Xing, “3dgstream: On-the-fly training of 3d gaussians for efficient streaming of photo-realistic free-viewpoint videos,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[132] Z. Lu, X. Guo, L. Hui, T. Chen, M. Yang, X. Tang, F. Zhu, and Y. Dai, $\" 3 \\mathrm { d }$ geometry-aware deformable gaussian splatting for dynamic view synthesis,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[133] J. Tang, J. Ren, H. Zhou, Z. Liu, and G. Zeng, “Dreamgaussian: Generative gaussian splatting for efficient 3d content creation,” in Proc. Int. Conf. Learn. Represent., 2024.",
    "[134] T. Yi, J. Fang, J. Wang, G. Wu, L. Xie, X. Zhang, W. Liu, Q. Tian, and X. Wang, “Gaussiandreamer: Fast generation from text to 3d gaussians by bridging 2d and 3d diffusion models,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[135] J. Tang, Z. Chen, X. Chen, T. Wang, G. Zeng, and Z. Liu, “Lgm: Large multi-view gaussian model for high-resolution 3d content creation,” in Proc. Eur. Conf. Comput. Vis., 2024.",
    "[136] S. Zhou, Z. Fan, D. Xu, H. Chang, P. Chari, T. Bharadwaj, S. You, Z. Wang, and A. Kadambi, “Dreamscene360: Unconstrained textto-3d scene generation with panoramic gaussian splatting,” in Proc. Eur. Conf. Comput. Vis., 2024.",
    "[137] Z. Li, Y. Chen, L. Zhao, and P. Liu, “Controllable text-to-3d generation via surface-aligned gaussian splatting,” arXiv preprint arXiv:2403.09981, 2024.",
    "[138] Y. Mu, X. Zuo, C. Guo, Y. Wang, J. Lu, X. Wu, S. Xu, P. Dai, Y. Yan, and L. Cheng, “Gsd: View-guided gaussian splatting diffusion for 3d reconstruction,” in Proc. Eur. Conf. Comput. Vis., 2024.",
    "[139] Y. Jiang, Z. Shen, P. Wang, Z. Su, Y. Hong, Y. Zhang, J. Yu, and L. Xu, “Hifi4g: High-fidelity human performance rendering via compact gaussian splatting,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[140] Y. Wang, Q. Wu, G. Zhang, and D. Xu, “Gscream: Learning 3d geometry and feature consistent gaussian splatting for object removal,” in Proc. Eur. Conf. Comput. Vis., 2024.",
    "[141] Y. Chen, Z. Chen, C. Zhang, F. Wang, X. Yang, Y. Wang, Z. Cai, L. Yang, H. Liu, and G. Lin, “Gaussianeditor: Swift and controllable 3d editing with gaussian splatting,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[142] J. Fang, J. Wang, X. Zhang, L. Xie, and Q. Tian, “Gaussianeditor: Editing 3d gaussians delicately with text instructions,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[143] R.-Z. Qiu, G. Yang, W. Zeng, and X. Wang, “Feature splatting: Language-driven physics-based scene synthesis and editing,” arXiv preprint arXiv:2404.01223, 2024.",
    "[144] Z. Li, Z. Zheng, L. Wang, and Y. Liu, “Animatable gaussians: Learning pose-dependent gaussian maps for high-fidelity human avatar modeling,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[145] S. Hu and Z. Liu, “Gauhuman: Articulated gaussian splatting from monocular human videos,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[146] J. Lei, Y. Wang, G. Pavlakos, L. Liu, and K. Daniilidis, “Gart: Gaussian articulated template models,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[147] Y. Yuan, X. Li, Y. Huang, S. De Mello, K. Nagano, J. Kautz, and U. Iqbal, “Gavatar: Animatable 3d gaussian avatars with implicit mesh learning,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[148] Z. Zhou, F. Ma, H. Fan, and Y. Yang, “Headstudio: Text to animatable head avatars with 3d gaussian splatting,” in Proc. Eur. Conf. Comput. Vis., 2024.",
    "[149] S. Qian, T. Kirschstein, L. Schoneveld, D. Davoli, S. Giebenhain, and M. Nießner, “Gaussianavatars: Photorealistic head avatars with rigged 3d gaussians,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[150] H. Dhamo, Y. Nie, A. Moreau, J. Song, R. Shaw, Y. Zhou, and E. Pe´rez-Pellitero, “Headgas: Real-time animatable head avatars via 3d gaussian splatting,” arXiv preprint arXiv:2312.02902, 2023.",
    "[151] J. Li, J. Zhang, X. Bai, J. Zheng, X. Ning, J. Zhou, and L. Gu, “Talkinggaussian: Structure-persistent 3d talking head synthesis via gaussian splatting,” in Proc. Eur. Conf. Comput. Vis., 2024.",
    "[152] Y. Huang, B. Cui, L. Bai, Z. Guo, M. ${ \\tt X u } ,$ and H. Ren, “Endo4dgs: Distilling depth ranking for endoscopic monocular scene reconstruction with 4d gaussian splatting,” in Proc. Int. Conf. Med. Image Comput. Comput. Assist. Interv., 2024.",
    "[153] Y. Liu, C. Li, C. Yang, and Y. Yuan, “Endogaussian: Gaussian splatting for deformable surgical scene reconstruction,” arXiv preprint arXiv:2401.12561, 2024.",
    "[154] L. Zhu, Z. Wang, Z. Jin, G. Lin, and L. Yu, “Deformable endoscopic tissues reconstruction with gaussian splatting,” arXiv preprint arXiv:2401.11535, 2024.",
    "[155] H. Zhao, X. Zhao, L. Zhu, W. Zheng, and Y. Xu, “Hfgs: 4d gaussian splatting with emphasis on spatial and temporal highfrequency components for endoscopic scene reconstruction,” arXiv preprint arXiv:2405.17872, 2024.",
    "[156] K. Wang, C. Yang, Y. Wang, S. Li, Y. Wang, Q. Dou, X. Yang, and W. Shen, “Endogslam: Real-time dense reconstruction and tracking in endoscopic surgeries using gaussian splatting,” arXiv preprint arXiv:2403.15124, 2024.",
    "[157] S. Bonilla, S. Zhang, D. Psychogyios, D. Stoyanov, F. Vasconcelos, and S. Bano, “Gaussian pancakes: Geometrically-regularized 3d gaussian splatting for realistic endoscopic reconstruction,” arXiv preprint arXiv:2404.06128, 2024. Z. Gan, and W. Ding, “Hgs-mapping: Online dense mapping using hybrid gaussian representation in urban scenes,” arXiv preprint arXiv:2403.20159, 2024.",
    "[159] C. Wu, Y. Duan, X. Zhang, Y. Sheng, J. Ji, and Y. Zhang, “Mmgaussian: 3d gaussian-based multi-modal fusion for localization and reconstruction in unbounded scenes,” in Proc. IEEE/RSJ Int. Conf. Intell. Robot. Syst., 2024.",
    "[160] B. Xiong, Z. Li, and Z. Li, “Gauu-scene: A scene reconstruction benchmark on large scale 3d reconstruction dataset using gaussian splatting,” arXiv preprint arXiv:2401.14032, 2024.",
    "[161] H. Zhao, H. Weng, D. Lu, A. Li, J. Li, A. Panda, and S. Xie, “On scaling up 3d gaussian splatting training,” arXiv preprint arXiv:2406.18533, 2024.",
    "[162] B. Kerbl, A. Meuleman, G. Kopanas, M. Wimmer, A. Lanvin, and G. Drettakis, “A hierarchical 3d gaussian representation for realtime rendering of very large datasets,” ACM Trans. Graph., vol. 44, no. 3, 2024.",
    "[163] Y. Liu, H. Guan, C. Luo, L. Fan, J. Peng, and Z. Zhang, “Citygaussian: Real-time high-quality large-scale scene rendering with gaussians,” in Proc. Eur. Conf. Comput. Vis., 2024.",
    "[164] J. Lin, Z. Li, X. Tang, J. Liu, S. Liu, J. Liu, Y. Lu, X. Wu, S. Xu, Y. Yan et al., “Vastgaussian: Vast 3d gaussians for large scene reconstruction,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[165] K. Ren, L. Jiang, T. Lu, M. Yu, L. Xu, Z. Ni, and B. Dai, “Octreegs: Towards consistent real-time rendering with lod-structured 3d gaussians,” arXiv preprint arXiv:2403.17898, 2024.",
    "[166] T. Xie, Z. Zong, Y. Qiu, X. Li, Y. Feng, Y. Yang, and C. Jiang, “Physgaussian: Physics-integrated 3d gaussians for generative dynamics,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[167] F. Liu, H. Wang, S. Yao, S. Zhang, J. Zhou, and Y. Duan, “Physics3d: Learning physical properties of 3d gaussians via video diffusion,” arXiv preprint arXiv:2406.04338, 2024.",
    "[168] P. Borycki, W. Smolak, J. Waczy´nska, M. Mazur, S. Tadeja, and P. Spurek, “Gasp: Gaussian splatting for physic-based simulations,” arXiv preprint arXiv:2409.05819, 2024.",
    "[169] T. Huang, Y. Zeng, H. Li, W. Zuo, and R. W. Lau, “Dreamphysics: Learning physical properties of dynamic 3d gaussians with video diffusion priors,” in Proc. AAAI Conf. Artif. Intell., 2025.",
    "[170] T. Zhang, H.-X. Yu, R. Wu, B. Y. Feng, C. Zheng, N. Snavely, J. Wu, and W. T. Freeman, “Physdreamer: Physics-based interaction with 3d objects via video generation,” in Proc. Eur. Conf. Comput. Vis., 2024, pp. 388–406.",
    "[171] Y. Feng, X. Feng, Y. Shang, Y. Jiang, C. Yu, Z. Zong, T. Shao, H. Wu, K. Zhou, C. Jiang et al., “Gaussian splashing: Dynamic fluid synthesis with gaussian splatting,” arXiv preprint arXiv:2401.15318, 2024.",
    "[172] L. Zhong, H.-X. Yu, J. Wu, and Y. Li, “Reconstruction and simulation of elastic objects with spring-mass 3d gaussians,” in Proc. Eur. Conf. Comput. Vis., 2024.",
    "[173] Y. Shao, M. Huang, C. C. Loy, and B. Dai, “Gausim: Registering elastic objects into digital world by gaussian simulator,” arXiv preprint arXiv:2412.17804, 2024.",
    "[174] S. Zhang, H. Zhao, Z. Zhou, G. Wu, C. Zheng, X. Wang, and W. Liu, “Togs: Gaussian splatting with temporal opacity offset for real-time 4d dsa rendering,” arXiv preprint arXiv:2403.19586, 2024.",
    "[175] R. Wu, Z. Zhang, Y. Yang, and W. Zuo, “Dual-camera smooth zoom on mobile phones,” arXiv preprint arXiv:2404.04908, 2024.",
    "[176] H. Li, Y. Gao, D. Zhang, C. Wu, Y. Dai, C. Zhao, H. Feng, E. Ding, J. Wang, and J. Han, “Ggrt: Towards generalizable 3d gaussians without pose priors in real-time,” arXiv preprint arXiv:2403.10147, 2024.",
    "[177] S. Hong, J. He, X. Zheng, H. Wang, H. Fang, K. Liu, C. Zheng, and S. Shen, “Liv-gaussmap: Lidar-inertial-visual fusion for real-time 3d radiance field map rendering,” arXiv preprint arXiv:2401.14857, 2024.",
    "[178] S. Sun, M. Mielle, A. J. Lilienthal, and M. Magnusson, “Highfidelity slam using gaussian splatting with rendering-guided densification and regularized optimization,” in Proc. IEEE/RSJ Int. Conf. Intell. Robot. Syst., 2024.",
    "[179] F. Tosi, Y. Zhang, Z. Gong, E. Sandstro¨m, S. Mattoccia, M. R. Oswald, and M. Poggi, “How nerfs and 3d gaussian splatting are reshaping slam: a survey,” arXiv preprint arXiv:2402.13255, 2024. W. Chen, “Compact 3d gaussian splatting for dense visual slam,” arXiv preprint arXiv:2403.11247, 2024.",
    "[181] J. Hu, X. Chen, B. Feng, G. Li, L. Yang, H. Bao, G. Zhang, and Z. Cui, “Cg-slam: Efficient dense rgb-d slam in a consistent uncertainty-aware 3d gaussian field,” arXiv preprint arXiv:2403.16095, 2024.",
    "[182] X. Lang, L. Li, H. Zhang, F. Xiong, M. Xu, Y. Liu, X. Zuo, and J. Lv, “Gaussian-lic: Photo-realistic lidar-inertial-camera slam with 3d gaussian splatting,” arXiv preprint arXiv:2404.06926, 2024.",
    "[183] E. Sandstr¨om, K. Tateno, M. Oechsle, M. Niemeyer, L. Van Gool, M. R. Oswald, and F. Tombari, “Splat-slam: Globally optimized rgb-only slam with 3d gaussians,” arXiv preprint arXiv:2405.16544, 2024.",
    "[184] A. Pumarola, E. Corona, G. Pons-Moll, and F. Moreno-Noguer, “D-nerf: Neural radiance fields for dynamic scenes,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2021, pp. 10 318–10 327.",
    "[185] K. Park, U. Sinha, P. Hedman, J. T. Barron, S. Bouaziz, D. B. Goldman, R. Martin-Brualla, and S. M. Seitz, “Hypernerf: a higher-dimensional representation for topologically varying neural radiance fields,” ACM Trans. Graph., vol. 40, no. 6, pp. 1–12, 2021.",
    "[186] K. Park, U. Sinha, J. T. Barron, S. Bouaziz, D. B. Goldman, S. M. Seitz, and R. Martin-Brualla, “Nerfies: Deformable neural radiance fields,” in Proc. IEEE Int. Conf. Comput. Vis., 2021, pp. 5865–5874.",
    "[187] X. Guo, J. Sun, Y. Dai, G. Chen, X. Ye, X. Tan, E. Ding, Y. Zhang, and J. Wang, “Forward flow for novel view synthesis of dynamic scenes,” in Proc. IEEE Int. Conf. Comput. Vis., 2023, pp. 16 022– 16 033.",
    "[188] X. Zhou, Z. Lin, X. Shan, Y. Wang, D. Sun, and M.-H. Yang, “Drivinggaussian: Composite gaussian splatting for surrounding dynamic autonomous driving scenes,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[189] Y. Yan, H. Lin, C. Zhou, W. Wang, H. Sun, K. Zhan, X. Lang, X. Zhou, and S. Peng, “Street gaussians for modeling dynamic urban scenes,” in Proc. Eur. Conf. Comput. Vis., 2024.",
    "[190] H. Zhou, J. Shao, L. Xu, D. Bai, W. Qiu, B. Liu, Y. Wang, A. Geiger, and Y. Liao, “Hugs: Holistic urban 3d scene understanding via gaussian splatting,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024, pp. 21 336–21 345.",
    "[191] A. Kratimenos, J. Lei, and K. Daniilidis, “Dynmf: Neural motion factorization for real-time dynamic view synthesis with 3d gaussian splatting,” arXiv preprint arXiv:2312.00112, 2023.",
    "[192] R. Shaw, J. Song, A. Moreau, M. Nazarczuk, S. Catley-Chandar, H. Dhamo, and E. Perez-Pellitero, “Swags: Sampling windows adaptively for dynamic 3d gaussian splatting,” arXiv preprint arXiv:2312.13308, 2023.",
    "[193] Y. Liang, N. Khan, Z. Li, T. Nguyen-Phuoc, D. Lanman, J. Tompkin, and L. Xiao, “Gaufre: Gaussian deformation fields for real-time dynamic novel view synthesis,” arXiv preprint arXiv:2312.11458, 2023.",
    "[194] K. Katsumata, D. M. Vo, and H. Nakayama, “An efficient 3d gaussian representation for monocular/multi-view dynamic scenes,” arXiv preprint arXiv:2311.12897, 2023.",
    "[195] Z. Guo, W. Zhou, L. Li, M. Wang, and H. Li, “Motion-aware 3d gaussian splatting for efficient dynamic scene reconstruction,” arXiv preprint arXiv:2403.11447, 2024.",
    "[196] J. Bae, S. Kim, Y. Yun, H. Lee, G. Bang, and Y. Uh, “Per-gaussian embedding-based deformation for deformable 3d gaussian splatting,” arXiv preprint arXiv:2404.03613, 2024.",
    "[197] J. Lei, Y. Weng, A. Harley, L. Guibas, and K. Daniilidis, “Mosca: Dynamic gaussian fusion from casual videos via 4d motion scaffolds,” arXiv preprint arXiv:2405.17421, 2024.",
    "[198] Q. Wang, V. Ye, H. Gao, J. Austin, Z. Li, and A. Kanazawa, “Shape of motion: 4d reconstruction from a single video,” arXiv preprint arXiv:2407.13764, 2024.",
    "[199] Y. Duan, F. Wei, Q. Dai, Y. He, W. Chen, and B. Chen, “4drotor gaussian splatting: towards efficient novel view synthesis for dynamic scenes,” in Proc. ACM Spec. Interest Group Comput. Graph. Interact. Tech., 2024, pp. 1–11.",
    "[200] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. WardeFarley, S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial networks,” Communications of the ACM, vol. 63, no. 11, pp. 139–144, 2020. models,” in Proc. Adv. Neural Inf. Process. Syst., 2020, pp. 6840– 6851.",
    "[202] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “High-resolution image synthesis with latent diffusion models,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2022, pp. 10 684– 10 695.",
    "[203] L. Zhang, A. Rao, and M. Agrawala, “Adding conditional control to text-to-image diffusion models,” in Proc. IEEE Int. Conf. Comput. Vis., 2023, pp. 3836–3847.",
    "[204] Z. Chen, F. Wang, and H. Liu, “Text-to-3d using gaussian splatting,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[205] Y. Liang, X. Yang, J. Lin, H. Li, X. Xu, and Y. Chen, “Luciddreamer: Towards high-fidelity text-to-3d generation via interval score matching,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[206] X. Liu, X. Zhan, J. Tang, Y. Shan, G. Zeng, D. Lin, X. Liu, and Z. Liu, “Humangaussian: Text-driven 3d human generation with gaussian splatting,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[207] X. Yang, Y. Chen, C. Chen, C. Zhang, Y. Xu, X. Yang, F. Liu, and G. Lin, “Learn to optimize denoising scores for 3d generation: A unified and improved diffusion prior on nerf and 3d gaussian splatting,” arXiv preprint arXiv:2312.04820, 2023.",
    "[208] Z.-X. Zou, Z. Yu, Y.-C. Guo, Y. Li, D. Liang, Y.-P. Cao, and S.-H. Zhang, “Triplane meets gaussian splatting: Fast and generalizable single-view 3d reconstruction with transformers,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[209] H. Ling, S. W. Kim, A. Torralba, S. Fidler, and K. Kreis, “Align your gaussians: Text-to-4d with dynamic 3d gaussians and composed diffusion models,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[210] J. Ren, L. Pan, J. Tang, C. Zhang, A. Cao, G. Zeng, and Z. Liu, “Dreamgaussian4d: Generative 4d gaussian splatting,” arXiv preprint arXiv:2312.17142, 2023.",
    "[211] Y. Yin, D. Xu, Z. Wang, Y. Zhao, and Y. Wei, “4dgen: Grounded 4d content generation with spatial-temporal consistency,” arXiv preprint arXiv:2312.17225, 2023.",
    "[212] J. Zhang, Z. Tang, Y. Pang, X. Cheng, P. Jin, Y. Wei, W. Yu, M. Ning, and L. Yuan, “Repaint123: Fast and high-quality one image to 3d generation with progressive controllable 2d repainting,” arXiv preprint arXiv:2312.13271, 2023.",
    "[213] Z. Pan, Z. Yang, X. Zhu, and L. Zhang, “Fast dynamic 3d object generation from a single-view video,” arXiv preprint arXiv:2401.08742, 2024.",
    "[214] D. Xu, Y. Yuan, M. Mardani, S. Liu, J. Song, Z. Wang, and A. Vahdat, “Agg: Amortized generative 3d gaussians for single image to 3d,” arXiv preprint arXiv:2401.04099, 2024.",
    "[215] C. Yang, S. Li, J. Fang, R. Liang, L. Xie, X. Zhang, W. Shen, and Q. Tian, “Gaussianobject: Just taking four images to get a high-quality 3d object with gaussian splatting,” arXiv preprint arXiv:2402.10259, 2024.",
    "[216] F. Barthel, A. Beckmann, W. Morgenstern, A. Hilsmann, and P. Eisert, “Gaussian splatting decoder for 3d-aware generative adversarial networks,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Worksh., 2024.",
    "[217] L. Jiang and L. Wang, “Brightdreamer: Generic 3d gaussian generative framework for fast text-to-3d synthesis,” arXiv preprint arXiv:2403.11273, 2024.",
    "[218] W. Zhuo, F. Ma, H. Fan, and Y. Yang, “Vividdreamer: Invariant score distillation for hyper-realistic text-to-3d generation,” in Proc. Eur. Conf. Comput. Vis., 2024.",
    "[219] Z. Wu, C. Yu, Y. Jiang, C. Cao, F. Wang, and X. Bai, “Sc4d: Sparse-controlled video-to-4d generation and motion transfer,” arXiv preprint arXiv:2404.03736, 2024.",
    "[220] X. He, J. Chen, S. Peng, D. Huang, Y. Li, X. Huang, C. Yuan, W. Ouyang, and T. He, “Gvgen: Text-to-3d generation with volumetric representation,” in Proc. Eur. Conf. Comput. Vis., 2024.",
    "[221] X. Yang and X. Wang, “Hash3d: Training-free acceleration for 3d generation,” arXiv preprint arXiv:2404.06091, 2024.",
    "[222] J. Kim, J. Koo, K. Yeo, and M. Sung, “Synctweedies: A general generative framework based on synchronized diffusions,” arXiv preprint arXiv:2403.14370, 2024.",
    "[223] Q. Feng, Z. Xing, Z. Wu, and Y.-G. Jiang, “Fdgaussian: Fast gaussian splatting from single image via geometric-aware diffusion model,” arXiv preprint arXiv:2403.10242, 2024. Lee, and P. Zhou, “Dreamscene: 3d gaussian-based text-to-3d scene generation via formation pattern sampling,” arXiv preprint arXiv:2404.03575, 2024.",
    "[225] L. Melas-Kyriazi, I. Laina, C. Rupprecht, N. Neverova, A. Vedaldi, O. Gafni, and F. Kokkinos, “Im-3d: Iterative multiview diffusion and reconstruction for high-quality 3d generation,” in Proc. ACM Int. Conf. Mach. Learn., 2024.",
    "[226] B. Zhang, Y. Cheng, J. Yang, C. Wang, F. Zhao, Y. Tang, D. Chen, and B. Guo, “Gaussiancube: Structuring gaussian splatting using optimal transport for 3d generative modeling,” arXiv preprint arXiv:2403.19655, 2024.",
    "[227] Y.-C. Lee, Y.-T. Chen, A. Wang, T.-H. Liao, B. Y. Feng, and J.-B. Huang, “Vividdream: Generating 3d scene with ambient dynamics,” arXiv preprint arXiv:2405.20334, 2024.",
    "[228] J. Huang and H. Yu, “Point’n move: Interactive scene object manipulation on gaussian splatting radiance fields,” arXiv preprint arXiv:2311.16737, 2023.",
    "[229] K. Lan, H. Li, H. Shi, W. Wu , Y. Liao, L. Wang, and P. Zhou, “2d-guided 3d gaussian segmentation,” arXiv preprint arXiv:2312.16047, 2023.",
    "[230] J. Zhuang, D. Kang, Y.-P. Cao, G. Li, L. Lin, and Y. Shan, “Tipeditor: An accurate 3d editor following both text-prompts and image-prompts,” arXiv preprint arXiv:2401.14828, 2024.",
    "[231] B. Dou, T. Zhang, Y. Ma, Z. Wang, and Z. Yuan, “Cosseggaussians: Compact and swift scene segmenting 3d gaussians,” arXiv preprint arXiv:2401.05925, 2024.",
    "[232] X. Hu, Y. Wang, L. Fan, J. Fan, J. Peng, Z. Lei, Q. Li, and Z. Zhang, “Semantic anything in 3d gaussians,” arXiv preprint arXiv:2401.17857, 2024.",
    "[233] F. Palandra, A. Sanchietti, D. Baieri, and E. Rodola\\`, “Gsedit: Efficient text-guided editing of 3d objects via gaussian splatting,” arXiv preprint arXiv:2403.05154, 2024.",
    "[234] Q. Gu, Z. Lv, D. Frost, S. Green, J. Straub, and C. Sweeney, “Egolifter: Open-world 3d segmentation for egocentric perception,” arXiv preprint arXiv:2403.18118, 2024.",
    "[235] W. Lyu, X. Li, A. Kundu, Y.-H. Tsai, and M.-H. Yang, “Gaga: Group any gaussians via 3d-aware memory bank,” arXiv preprint arXiv:2404.07977, 2024.",
    "[236] Z. Liu, H. Ouyang, Q. Wang, K. L. Cheng, J. Xiao, K. Zhu, N. Xue, Y. Liu, Y. Shen, and Y. Cao, “Infusion: Inpainting 3d gaussians via learning depth completion from diffusion prior,” arXiv preprint arXiv:2404.11613, 2024.",
    "[237] D. Zhang, Z. Chen, Y.-J. Yuan, F.-L. Zhang, Z. He, S. Shan, and L. Gao, “Stylizedgs: Controllable stylization for 3d gaussian splatting,” arXiv preprint arXiv:2404.05220, 2024.",
    "[238] Q. Zhang, Y. Xu, C. Wang, H.-Y. Lee, G. Wetzstein, B. Zhou, and C. Yang, “3ditscene: Editing any scene via language-guided disentangled gaussian splatting,” arXiv preprint arXiv:2405.18424, 2024.",
    "[239] J. Wu, J.-W. Bian, X. Li, G. Wang, I. Reid, P. Torr, and V. A. Prisacariu, “Gaussctrl: Multi-view consistent text-driven 3d gaussian splatting editing,” in Proc. Eur. Conf. Comput. Vis., 2024, pp. 55–71.",
    "[240] Y. Wang, X. Yi, Z. Wu, N. Zhao, L. Chen, and H. Zhang, “Viewconsistent 3d editing with gaussian splatting,” in Proc. Eur. Conf. Comput. Vis., 2024, pp. 404–420.",
    "[241] R. Jena, G. S. Iyer, S. Choudhary, B. Smith, P. Chaudhari, and J. Gee, “Splatarmor: Articulated gaussian splatting for animatable humans from monocular rgb videos,” arXiv preprint arXiv:2311.10812, 2023.",
    "[242] K. Ye, T. Shao, and K. Zhou, “Animatable 3d gaussians for high-fidelity synthesis of human motions,” arXiv preprint arXiv:2311.13404, 2023.",
    "[243] A. Moreau, J. Song, H. Dhamo, R. Shaw, Y. Zhou, and E. P´erezPellitero, “Human gaussian splatting: Real-time rendering of animatable avatars,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[244] M. Kocabas, J.-H. R. Chang, J. Gabriel, O. Tuzel, and A. Ranjan, “Hugs: Human gaussian splats,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[245] R. Abdal, W. Yifan, Z. Shi, Y. Xu, R. Po, Z. Kuang, Q. Chen, D.- Y. Yeung, and G. Wetzstein, “Gaussian shell maps for efficient 3d human generation,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[246] S. Zheng, B. Zhou, R. Shao, B. Liu, S. Zhang, L. Nie, and Y. Liu, “Gps-gaussian: Generalizable pixel-wise 3d gaussian splatting",
    "[247] L. Hu, H. Zhang, Y. Zhang, B. Zhou, B. Liu, S. Zhang, and L. Nie, “Gaussianavatar: Towards realistic human avatar modeling from a single video via animatable 3d gaussians,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[248] H. Pang, H. Zhu, A. Kortylewski, C. Theobalt, and M. Habermann, “Ash: Animatable gaussian splats for efficient and photoreal human rendering,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[249] Z. Qian, S. Wang, M. Mihajlovic, A. Geiger, and S. Tang, “3dgsavatar: Animatable avatars via deformable 3d gaussian splatting,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[250] H. Jung, N. Brasch, J. Song, E. Perez-Pellitero, Y. Zhou, Z. Li, N. Navab, and B. Busam, “Deformable 3d gaussian splatting for animatable human avatars,” arXiv preprint arXiv:2312.15059, 2023.",
    "[251] M. Li, J. Tao, Z. Yang, and Y. Yang, “Human101: Training $^ { 1 0 0 + }$ fps human gaussians in 100s from 1 view,” arXiv preprint arXiv:2312.15258, 2023.",
    "[252] M. Li, S. Yao, Z. Xie, K. Chen, and Y.-G. Jiang, “Gaussianbody: Clothed human reconstruction via 3d gaussian splatting,” arXiv preprint arXiv:2401.09720, 2024.",
    "[253] J. Xiang, X. Gao, Y. Guo, and J. Zhang, “Flashavatar: Highfidelity digital avatar rendering at 300fps,” arXiv preprint arXiv:2312.02214, 2023.",
    "[254] Y. Chen, L. Wang, Q. Li, H. Xiao, S. Zhang, H. Yao, and Y. Liu, “Monogaussianavatar: Monocular gaussian point-based head avatar,” arXiv preprint arXiv:2312.04558, 2023.",
    "[255] Z. Zhao, Z. Bao, Q. Li, G. Qiu, and K. Liu, “Psavatar: A pointbased morphable shape model for real-time head avatar creation with 3d gaussian splatting,” arXiv preprint arXiv:2401.12900, 2024.",
    "[256] A. Rivero, S. Athar, Z. Shu, and D. Samaras, “Rig3dgs: Creating controllable portraits from casual monocular videos,” arXiv preprint arXiv:2402.03723, 2024.",
    "[257] H. Luo, M. Ouyang, Z. Zhao, S. Jiang, L. Zhang, Q. Zhang, W. Yang, L. Xu, and J. Yu, “Gaussianhair: Hair modeling and rendering with light-aware gaussians,” arXiv preprint arXiv:2402.10483, 2024.",
    "[258] Y. Wang, Y. Long, S. H. Fan, and Q. Dou, “Neural rendering for stereo 3d reconstruction of deformable tissues in robotic surgery,” in Proc. Int. Conf. Med. Image Comput. Comput. Assist. Interv., 2022, pp. 431–441.",
    "[259] C. Yang, K. Wang, Y. Wang, X. Yang, and W. Shen, “Neural lerplane representations for fast 4d reconstruction of deformable tissues,” in Proc. Int. Conf. Med. Image Comput. Comput. Assist. Interv., 2023, pp. 46–56.",
    "[260] R. Zha, X. Cheng, H. Li, M. Harandi, and Z. Ge, “Endosurf: Neural surface reconstruction of deformable tissues with stereo endoscope videos,” in Proc. Int. Conf. Med. Image Comput. Comput. Assist. Interv., 2023, pp. 13–23.",
    "[261] J. Straub, T. Whelan, L. Ma, Y. Chen, E. Wijmans, S. Green, J. J. Engel, R. Mur-Artal, C. Ren, S. Verma et al., “The replica dataset: A digital replica of indoor spaces,” arXiv preprint arXiv:1906.05797, 2019.",
    "[262] E. Sucar, S. Liu, J. Ortiz, and A. J. Davison, “imap: Implicit mapping and positioning in real-time,” in Proc. IEEE Int. Conf. Comput. Vis., 2021, pp. 6229–6238.",
    "[263] X. Yang, H. Li, H. Zhai, Y. Ming, Y. Liu, and G. Zhang, “Voxfusion: Dense tracking and mapping with voxel-based neural implicit representation,” in IEEE International Symposium on Mixed and Augmented Reality, 2022, pp. 499–507.",
    "[264] Z. Zhu, S. Peng, V. Larsson, W. Xu, H. Bao, Z. Cui, M. R. Oswald, and M. Pollefeys, “Nice-slam: Neural implicit scalable encoding for slam,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2022, pp. 12 786–12 796.",
    "[265] M. M. Johari, C. Carta, and F. Fleuret, “Eslam: Efficient dense slam system based on hybrid representation of signed distance fields,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2023, pp. 17 408–17 419.",
    "[266] E. Sandstr ¨om, Y. Li, L. Van Gool, and M. R. Oswald, “Point-slam: Dense neural point cloud-based slam,” in Proc. IEEE Int. Conf. Comput. Vis., 2023, pp. 18 433–18 444.",
    "[267] H. Wang, J. Wang, and L. Agapito, “Co-slam: Joint coordinate and sparse parametric encodings for neural real-time slam,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2023, pp. 13 293–13 302. “A moving least squares material point method with displacement discontinuity and two-way rigid body coupling,” ACM Trans. Graph., vol. 37, no. 4, pp. 1–14, 2018.",
    "[269] M. M ¨uller, B. Heidelberger, M. Hennix, and J. Ratcliff, “Position based dynamics,” Journal of Visual Communication and Image Representation, vol. 18, no. 2, pp. 109–118, 2007.",
    "[270] A. Knapitsch, J. Park, Q.-Y. Zhou, and V. Koltun, “Tanks and temples: Benchmarking large-scale scene reconstruction,” ACM Trans. Graph., vol. 36, no. 4, pp. 1–13, 2017.",
    "[271] T. Zhou, R. Tucker, J. Flynn, G. Fyffe, and N. Snavely, “Stereo magnification: learning view synthesis using multiplane images,” ACM Trans. Graph., vol. 37, no. 4, pp. 1–12, 2018.",
    "[272] P. Hedman, J. Philip, T. Price, J.-M. Frahm, G. Drettakis, and G. Brostow, “Deep blending for free-viewpoint image-based rendering,” ACM Trans. Graph., vol. 37, no. 6, pp. 1–15, 2018.",
    "[273] B. Mildenhall, P. P. Srinivasan, R. Ortiz-Cayon, N. K. Kalantari, R. Ramamoorthi, R. Ng, and A. Kar, “Local light field fusion: Practical view synthesis with prescriptive sampling guidelines,” ACM Trans. Graph., vol. 38, no. 4, pp. 1–14, 2019.",
    "[274] A. Liu, R. Tucker, V. Jampani, A. Makadia, N. Snavely, and A. Kanazawa, “Infinite nature: Perpetual view generation of natural scenes from a single image,” in Proc. IEEE Int. Conf. Comput. Vis., 2021, pp. 14 458–14 467.",
    "[275] J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cremers, “A benchmark for the evaluation of rgb-d slam systems,” in Proc. IEEE/RSJ Int. Conf. Intell. Robot. Syst., 2012, pp. 573–580.",
    "[276] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving? the kitti vision benchmark suite,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2012, pp. 3354–3361.",
    "[277] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner, “Scannet: Richly-annotated 3d reconstructions of indoor scenes,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2017, pp. 5828–5839.",
    "[278] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui, J. Guo, Y. Zhou, Y. Chai, B. Caine et al., “Scalability in perception for autonomous driving: Waymo open dataset,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2020, pp. 2446– 2454.",
    "[279] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, “nuscenes: A multimodal dataset for autonomous driving,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2020, pp. 11 621–11 631.",
    "[280] S. James, Z. Ma, D. R. Arrojo, and A. J. Davison, “Rlbench: The robot learning benchmark & learning environment,” IEEE Robotics and Automation Letters, vol. 5, no. 2, pp. 3019–3026, 2020.",
    "[281] A. Mandlekar, D. Xu, J. Wong, S. Nasiriany, C. Wang, R. Kulkarni, L. Fei-Fei, S. Savarese, Y. Zhu, and R. Martı´n-Mart´ın, “What matters in learning from offline human demonstrations for robot manipulation,” in Proc. Annu. Conf. Robot Learn., 2022, pp. 1678– 1690.",
    "[282] Z. Yan, C. Li, and G. H. Lee, “Nerf-ds: Neural radiance fields for dynamic specular objects,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2023, pp. 8285–8295.",
    "[283] K. Kania, K. M. Yi, M. Kowalski, T. Trzci ´nski, and A. Tagliasacchi, “Conerf: Controllable neural radiance fields,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2022, pp. 18 623–18 632.",
    "[284] A. Mirzaei, T. Aumentado-Armstrong, K. G. Derpanis, J. Kelly, M. A. Brubaker, I. Gilitschenski, and A. Levinshtein, “Spin-nerf: Multiview segmentation and perceptual inpainting with neural radiance fields,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2023, pp. 20 669–20 679.",
    "[285] R. Shao, Z. Zheng, H. Tu, B. Liu, H. Zhang, and Y. Liu, “Tensor4d: Efficient neural 4d decomposition for high-fidelity dynamic reconstruction and rendering,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2023, pp. 16 632–16 642.",
    "[286] T. Wu, J. Zhang, X. Fu, Y. Wang, J. Ren, L. Pan, W. Wu, L. Yang, J. Wang, C. Qian et al., “Omniobject3d: Large-vocabulary 3d object dataset for realistic perception, reconstruction and generation,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2023, pp. 803–814.",
    "[287] M. Deitke, D. Schwenk, J. Salvador, L. Weihs, O. Michel, E. VanderBilt, L. Schmidt, K. Ehsani, A. Kembhavi, and A. Farhadi, “Objaverse: A universe of annotated 3d objects,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2023, pp. 13 142–13 153.",
    "[288] T. Alldieck, M. Magnor, W. Xu, C. Theobalt, and G. Pons-Moll, Conf. Comput. Vis. Pattern Recognit., 2018, pp. 8387–8397.",
    "[289] D. Cudeiro, T. Bolkart, C. Laidlaw, A. Ranjan, and M. J. Black, “Capture, learning, and synthesis of 3d speaking styles,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019, pp. 10 101–10 111.",
    "[290] Z. Zheng, T. Yu, Y. Wei, Q. Dai, and Y. Liu, “Deephuman: 3d human reconstruction from a single image,” in Proc. IEEE Int. Conf. Comput. Vis., 2019, pp. 7739–7749.",
    "[291] T. Yu, Z. Zheng, K. Guo, P. Liu, Q. Dai, and Y. Liu, “Function4d: Real-time human volumetric capture from very sparse consumer rgbd sensors,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2021, pp. 5746–5756.",
    "[292] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, “Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2021, pp. 9054– 9063.",
    "[293] E. Ramon, G. Triginer, J. Escur, A. Pumarola, J. Garcia, X. Giro-i Nieto, and F. Moreno-Noguer, “H3d-net: Few-shot high-fidelity 3d head reconstruction,” in Proc. IEEE Int. Conf. Comput. Vis., 2021, pp. 5620–5629.",
    "[294] Z. Su, T. Yu, Y. Wang, and Y. Liu, “Deepcloth: Neural garment representation for shape and style editing,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 45, no. 2, pp. 1581–1593, 2022.",
    "[295] M. Allan, J. Mcleod, C. Wang, J. C. Rosenthal, Z. Hu, N. Gard, P. Eisert, K. X. Fu, T. Zeffiro, W. Xia et al., “Stereo correspondence and reconstruction of endoscopic data challenge,” arXiv preprint arXiv:2101.01133, 2021.",
    "[296] Y. Cai, J. Wang, A. Yuille, Z. Zhou, and A. Wang, “Structureaware sparse-view x-ray 3d reconstruction,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024, pp. 11 174–11 183.",
    "[297] Y. Xiangli, L. Xu, X. Pan, N. Zhao, A. Rao, C. Theobalt, B. Dai, and D. Lin, “Bungeenerf: Progressive neural radiance field for extreme multi-scale scene rendering,” in Proc. Eur. Conf. Comput. Vis. Springer, 2022, pp. 106–122.",
    "[298] M. Tancik, V. Casser, X. Yan, S. Pradhan, B. Mildenhall, P. P. Srinivasan, J. T. Barron, and H. Kretzschmar, “Block-nerf: Scalable large scene neural view synthesis,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2022, pp. 8248–8258.",
    "[299] G. Yang, F. Xue, Q. Zhang, K. Xie, C.-W. Fu, and H. Huang, “Urbanbis: a large-scale benchmark for fine-grained urban building instance segmentation,” in Proc. ACM Spec. Interest Group Comput. Graph. Interact. Tech., 2023, pp. 1–11.",
    "[300] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality assessment: from error visibility to structural similarity,” IEEE Trans. Image Process., vol. 13, no. 4, pp. 600–612, 2004.",
    "[301] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The unreasonable effectiveness of deep features as a perceptual metric,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 586–595.",
    "[302] J. Fang, T. Yi, X. Wang, L. Xie, X. Zhang, W. Liu, M. Nießner, and Q. Tian, “Fast dynamic radiance fields with time-aware neural voxels,” in SIGGRAPH Asia, 2022, pp. 1–9.",
    "[303] A. Cao and J. Johnson, “Hexplane: A fast representation for dynamic scenes,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2023, pp. 130–141.",
    "[304] F. Wang, Z. Chen, G. Wang, Y. Song, and H. Liu, “Masked spacetime hash encoding for efficient dynamic scene reconstruction,” in Proc. Adv. Neural Inf. Process. Syst., 2023.",
    "[305] C.-Y. Weng, B. Curless, P. P. Srinivasan, J. T. Barron, and I. Kemelmacher-Shlizerman, “Humannerf: Free-viewpoint rendering of moving people from monocular video,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2022, pp. 16 210–16 220.",
    "[306] C. Geng, S. Peng, Z. Xu, H. Bao, and X. Zhou, “Learning neural volumetric representations of dynamic humans in minutes,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2023, pp. 8759– 8770.",
    "[307] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, “Animatable neural radiance fields for modeling dynamic human bodies,” in Proc. IEEE Int. Conf. Comput. Vis., 2021, pp. 14 314–14 323.",
    "[308] A. Yu, V. Ye, M. Tancik, and A. Kanazawa, “pixelnerf: Neural radiance fields from one or few images,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2021, pp. 4578–4587.",
    "[309] Y. Kwon, D. Kim, D. Ceylan, and H. Fuchs, “Neural human performer: Learning generalizable radiance fields for human performance rendering,” in Proc. Adv. Neural Inf. Process. Syst., 2021, pp. 24 741–24 752.",
    "[310] J. Wang, Z. Zhang, Q. Zhang, J. Li, J. Sun, M. Sun, J. He, and R. Xu, “Query-based semantic gaussian field for scene representation in reinforcement learning,” arXiv preprint arXiv:2406.02370, 2024.",
    "[311] Y. Qu, S. Dai, X. Li, J. Lin, L. Cao, S. Zhang, and R. Ji, “Goi: Find 3d gaussians of interest with an optimizable open-vocabulary semantic-space hyperplane,” arXiv preprint arXiv:2405.17596, 2024.",
    "[312] Y. Ji, H. Zhu, J. Tang, W. Liu, Z. Zhang, Y. Xie, L. Ma, and X. Tan, “Fastlgs: Speeding up language embedded gaussians with feature grid mapping,” arXiv preprint arXiv:2406.01916, 2024.",
    "[313] G. Liao, J. Li, Z. Bao, X. Ye, J. Wang, Q. Li, and K. Liu, “Clip-gs: Clip-informed gaussian splatting for real-time and view-consistent 3d semantic understanding,” arXiv preprint arXiv:2404.14249, 2024.",
    "[314] S. Choi, H. Song, J. Kim, T. Kim, and H. Do, “Click-gaussian: Interactive segmentation to any 3d gaussians,” arXiv preprint arXiv:2407.11793, 2024.",
    "[315] S. Ji, G. Wu, J. Fang, J. Cen, T. Yi, W. Liu, Q. Tian, and X. Wang, “Segment any 4d gaussians,” arXiv preprint arXiv:2407.04504, 2024.",
    "[316] A. Gue´don and V. Lepetit, “Sugar: Surface-aligned gaussian splatting for efficient 3d mesh reconstruction and high-quality mesh rendering,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024.",
    "[317] T. Liu, G. Wang, S. Hu, L. Shen, X. Ye, Y. Zang, Z. Cao, W. Li, and Z. Liu, “Fast generalizable gaussian splatting reconstruction from multi-view stereo,” in Proc. Eur. Conf. Comput. Vis., 2024.",
    "[318] Y. Li, X. Fu, S. Zhao, R. Jin, and S. K. Zhou, “Sparse-view ct reconstruction with 3d gaussian volumetric representation,” arXiv preprint arXiv:2312.15676, 2023.",
    "[319] Y. Cai, Y. Liang, J. Wang, A. Wang, Y. Zhang, X. Yang, Z. Zhou, and A. Yuille, “Radiative gaussian splatting for efficient x-ray novel view synthesis,” in Proc. Eur. Conf. Comput. Vis., 2024.",
    "[320] J. Chang, Y. Xu, Y. Li, Y. Chen, and X. Han, “Gaussreg: Fast 3d registration with gaussian splatting,” in Proc. Eur. Conf. Comput. Vis., 2024."
]