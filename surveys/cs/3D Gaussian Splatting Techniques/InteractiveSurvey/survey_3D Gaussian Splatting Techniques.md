# A Survey of 3D Gaussian Splatting Techniques

# 1 Abstract


The field of 3D scene reconstruction and novel view synthesis has witnessed significant advancements, driven by the increasing demand for realistic and interactive 3D content in applications such as virtual reality (VR), augmented reality (AR), and computer graphics. This survey paper focuses on the recent advancements in 3D Gaussian Splatting (3DGS) techniques, particularly in semantic optimization, feature propagation, deformation, and uncertainty estimation. The paper provides a comprehensive overview of the latest research, highlighting key techniques and methodologies that have contributed to the evolution of 3DGS. Notable contributions include enhancements in semantic optimization and feature propagation, such as Decoupled Semantic Optimization (DSO) and Contextual Feature Propagation (CFP), which improve the accuracy and robustness of 3D scene representations. Additionally, the survey explores advanced deformation techniques, including Cage-based Deformation, ARAP deformation with a diffusion prior, and NeuralCage Learning, which enable intuitive and high-quality manipulations of 3D scenes. The paper also covers advancements in uncertainty and efficiency improvements, such as uncertainty estimation with spherical harmonics and precomputation for efficient training, which enhance the reliability and practicality of 3DGS models. Finally, the survey discusses advanced 3D scene representation and inpainting techniques, including Gaussian-Enhanced Surfels, opacity modulation, and hash grids for fast rendering, which address the challenges of high-frequency texture representation and real-time rendering. By consolidating the current state of the art, this survey aims to inspire further innovations and advancements in 3D Gaussian Splatting techniques.

# 2 Introduction
The field of 3D scene reconstruction and novel view synthesis has seen significant advancements in recent years, driven by the increasing demand for realistic and interactive 3D content in applications such as virtual reality (VR), augmented reality (AR), and computer graphics [1]. Traditional methods, such as mesh-based and voxel-based representations, have limitations in capturing fine details and handling complex scenes with dynamic elements. The advent of 3D Gaussian Splatting (3DGS) has emerged as a promising alternative, offering a balance between computational efficiency and high-fidelity representation [2]. 3DGS represents 3D scenes using a collection of Gaussian primitives, each defined by parameters such as position, scale, rotation, and opacity [3]. This representation allows for efficient rendering and optimization, making it suitable for real-time applications and large-scale scenes.

This survey paper focuses on the recent advancements in 3D Gaussian Splatting techniques, particularly in the areas of semantic optimization, feature propagation, deformation, and uncertainty estimation [4]. The paper aims to provide a comprehensive overview of the latest research, highlighting the key techniques and methodologies that have contributed to the evolution of 3DGS [5]. We begin by exploring enhancements in semantic optimization and feature propagation, which are crucial for improving the accuracy and robustness of 3D scene representations [6]. Decoupled Semantic Optimization (DSO) and Contextual Feature Propagation (CFP) are discussed in detail, emphasizing their roles in addressing the challenges of cross-view granularity inconsistency and semantic reconstruction.

Next, we delve into the techniques for deformation and transformation, which are essential for creating interactive and dynamic 3D scenes. Cage-based deformation, ARAP deformation with a diffusion prior, and NeuralCage Learning are examined, showcasing their contributions to structure-aware and automated 3D scene manipulation [7]. These methods enable intuitive and high-quality deformations, making them valuable tools for applications such as character animation and 3D content creation.

The paper also covers advancements in uncertainty and efficiency improvements, which are vital for enhancing the reliability and practicality of 3DGS models [2]. Techniques such as uncertainty estimation with spherical harmonics, precomputation for efficient training, and direct initialization strategies are discussed, highlighting their roles in improving the robustness and computational efficiency of 3DGS. These methods ensure that the models can handle large-scale scenes and real-time applications with high accuracy and performance.

Finally, the survey explores advanced 3D scene representation and inpainting techniques, including Gaussian-Enhanced Surfels, opacity modulation, and hash grids for fast rendering [8]. These methods address the challenges of high-frequency texture representation and real-time rendering, making 3DGS a versatile and powerful tool for a wide range of applications. The paper concludes by discussing future directions and potential areas for further research, emphasizing the ongoing need for innovation in 3D scene reconstruction and novel view synthesis [9].

The contributions of this survey paper are multifaceted. Firstly, it provides a comprehensive and structured overview of the latest advancements in 3D Gaussian Splatting techniques, synthesizing insights from a wide range of research papers and studies [3]. Secondly, it highlights the practical implications of these techniques, demonstrating their potential in real-world applications such as VR, AR, and 3D content creation. Lastly, it identifies key challenges and future research directions, serving as a valuable resource for researchers and practitioners in the field. By consolidating the current state of the art, this survey aims to inspire further innovations and advancements in 3D Gaussian Splatting techniques [3].

# 3 Enhancements in 3D Gaussian Splatting Techniques

## 3.1 Semantic Optimization and Feature Propagation

### 3.1.1 Decoupled Semantic Optimization
Decoupled Semantic Optimization (DSO) is a novel approach designed to enhance the efficiency and accuracy of semantic reconstruction in 3D Gaussian Splatting (3DGS) frameworks [4]. Unlike traditional methods that intertwine geometric and semantic optimization, DSO separates these processes, allowing for more targeted and efficient refinement of semantic information. This decoupling is achieved by first reconstructing the geometric representation of the scene to a high level of detail, followed by an independent optimization step that focuses on refining the semantic embeddings [6]. This separation ensures that the geometric model is stable and well-optimized before introducing the complexity of semantic optimization, which can often lead to overfitting and degraded performance in sparse-view scenarios.

In the DSO framework, the initial geometric reconstruction phase leverages advanced techniques such as epipolar geometry and triangulation to ensure that the initial spatial relationships are geometrically constrained. This helps in reducing initialization ambiguities and provides a robust foundation for subsequent semantic optimization. Once the geometric model is established, the semantic optimization phase begins. This phase involves the use of a graph learning module that dynamically aggregates features from multiple views, ensuring that the semantic information is consistent and accurate across the entire scene. The graph learning module helps in addressing the common issue of visibility problems during individual block optimization, where the content of a training view may be distributed across multiple blocks, leading to artifacts and degraded reconstruction quality.

The effectiveness of DSO is further enhanced by the introduction of a growth-and-dropout mechanism that dynamically adjusts the number of Gaussians during training. This mechanism helps in reducing overfitting by periodically dropping out Gaussians, which prevents the model from becoming overly specialized to the training data. Additionally, the growth aspect of the mechanism allows the model to adapt to the complexity of the scene by adding more Gaussians where necessary. This dynamic adjustment ensures that the model remains computationally efficient while maintaining high reconstruction quality. Extensive experiments on various datasets and 3DGS frameworks have shown that DSO consistently improves rendering quality and reduces model storage, making it a valuable addition to the toolkit of 3D scene reconstruction methods [10].

### 3.1.2 Contextual Feature Propagation
Contextual Feature Propagation (CFP) is a critical component in enhancing the accuracy and robustness of 3D Gaussian Splatting (3DGS) by addressing the issue of cross-view granularity inconsistency [11]. Traditional 3DGS methods often struggle with maintaining consistent feature granularity across different views, leading to fragmented and noisy 3D object representations [12]. CFP mitigates this problem by leveraging spatial context to ensure that each Gaussian primitive can effectively aggregate information from its neighbors, thereby improving the overall coherence and quality of the 3D reconstruction.

In CFP, a localized graph is constructed for each Gaussian primitive, where the nodes represent the Gaussians and the edges encode the spatial relationships between them. This graph structure enables lightweight feature propagation, allowing each Gaussian to dynamically update its features based on the information from its immediate neighbors. The propagation process is guided by a set of learned attention weights, which help in prioritizing the most relevant features from the neighboring Gaussians. This mechanism ensures that the feature representation of each Gaussian is enriched with context from its surroundings, leading to more accurate and consistent 3D reconstructions.

To further enhance the effectiveness of CFP, a Mask-Aware Contrastive Learning (MA-CL) strategy is employed. MA-CL supervises the centroids of the Gaussians by aligning them with the corresponding mask centroids in the input images. This alignment helps in reducing noise and inconsistencies that arise from varying granularities across different views. Additionally, a precomputation step is introduced to reduce the computational cost of the training process. By precomputing the neighborhood relationships and attention weights, the model can efficiently update the features of each Gaussian during training, leading to faster convergence and improved performance. Overall, CFP significantly enhances the spatial awareness and feature consistency in 3DGS, making it a powerful tool for high-quality 3D scene reconstruction.

### 3.1.3 Mask-Aware Contrastive Learning
Mask-Aware Contrastive Learning (MA-CL) is a novel approach designed to address the challenges of cross-view granularity inconsistency in 3D Gaussian Splatting (3DGS) [11]. Unlike traditional methods that rely on per-pixel InfoNCE losses, which are susceptible to granularity noise, MA-CL computes feature centroids within segmentation masks derived from the Segment Anything Model (SAM) and applies contrastive supervision to these centroids across different views [11]. This method significantly reduces the impact of inconsistent segmentation, leading to more stable and accurate training. By focusing on mask centroids, MA-CL ensures that the learned representations are more robust and less affected by the noise and variability inherent in per-pixel losses.

The core of MA-CL lies in its ability to compute and supervise feature centroids within masks, which are generated using SAM. These masks provide a more reliable and consistent segmentation of the scene, which is crucial for maintaining cross-view consistency. The contrastive loss is then applied to these centroids, ensuring that the learned features are aligned across different views. This approach not only stabilizes the training process but also improves the quality of the reconstructed 3D scenes [13]. By reducing the noise from inconsistent segmentation, MA-CL enables more efficient and effective learning, leading to higher-quality 3D reconstructions and more accurate novel view synthesis. The use of mask centroids also allows for better handling of complex scenes with varying levels of detail and texture.

To further enhance the efficiency and performance of MA-CL, a precomputation strategy is employed to reduce computational costs. This strategy involves precomputing the neighborhood relationships between Gaussians, which are then used during the training process to speed up the optimization. By precomputing these relationships, the method can quickly identify and update the relevant Gaussians, leading to faster convergence and reduced training times. This precomputation step is particularly beneficial for large-scale scenes, where the number of Gaussians can be very high. Overall, MA-CL represents a significant advancement in 3DGS by addressing key challenges in cross-view consistency and computational efficiency, making it a valuable tool for high-quality 3D scene reconstruction and novel view synthesis [7].

## 3.2 Deformation and Transformation Techniques

### 3.2.1 Cage-Based Deformation
Cage-based deformation is a powerful technique in computer graphics that facilitates intuitive and structure-aware manipulation of 3D models while preserving fine details [7]. Unlike direct vertex-based deformation, which can result in unstructured distortions, a cage provides a high-level control structure that smoothly influences the deformation of the enclosed model [7]. This method is particularly useful for applications requiring precise and controlled shape modifications, such as character animation, architectural design, and medical imaging. Traditional cage-based deformation methods primarily focus on mesh or implicit representations, leveraging the cage to guide the deformation of the underlying geometry. However, these methods are not directly applicable to the complexity and discreteness of 3D Gaussian Splatting (3DGS) [14].

To address the limitations of traditional cage-based deformation methods in the context of 3DGS, we introduce a novel cage-based 3DGS deformation technique [7]. This method allows for automated, structure-aware transformations without the need for manual control point adjustments. The key innovation lies in utilizing the Jacobian matrix to drive the deformation of Gaussian parameters. By computing the Jacobian matrix, which captures the local derivatives of the Gaussian parameters with respect to the cage vertices, we can efficiently propagate the deformation from the cage to the 3DGS points. This approach ensures that the deformation is smooth and coherent, maintaining the fine details of the original 3DGS representation [5].

Our cage-based 3DGS deformation method is designed to be flexible and compatible with various 3DGS optimization techniques. It can be seamlessly integrated into existing 3DGS pipelines, enhancing their capabilities for interactive and real-time applications. Experimental results demonstrate that our method significantly improves the quality and efficiency of 3DGS deformations, making it a valuable tool for a wide range of applications, from virtual reality and augmented reality to 3D content creation and simulation [15]. The ability to perform structure-aware deformations without manual intervention opens up new possibilities for automating complex 3D modeling tasks, thereby streamlining the workflow and reducing the overall computational burden.

### 3.2.2 ARAP Deformation with Diffusion Prior
ARAP-GS, an innovative method for 3D Gaussian Splatting (3DGS) editing, leverages As-Rigid-As-Possible (ARAP) deformation combined with a diffusion prior to achieve intuitive and high-quality 3DGS manipulation [5]. The method is designed to enable users to perform free-form deformations on 3DGS models using simple drag operations, while maintaining the structural integrity and visual fidelity of the scene [7]. The process begins with transforming the initial 3D Gaussians into a representative subset, which is then deformed according to the user's input [15]. This subset deformation is propagated to the remaining Gaussians through interpolation, ensuring a smooth and consistent transformation across the entire model.

However, the initial deformation stage may introduce artifacts, such as misaligned colors and opacities, which can degrade the visual quality of the rendered images. To address this, ARAP-GS incorporates a fine-tuning stage that utilizes a 2D diffusion prior to refine the deformed 3DGS. The diffusion prior helps to smooth out inconsistencies and align the Gaussian properties, such as color and opacity, with the deformed geometry. This iterative optimization process ensures that the final 3DGS model not only maintains the intended deformation but also preserves the fine details and textures, leading to a more realistic and visually coherent result [7].

The integration of the diffusion prior in the fine-tuning stage is a key contribution of ARAP-GS, as it enables the method to handle complex deformations without requiring additional supervision data. This makes ARAP-GS particularly suitable for applications where precise and interactive 3DGS editing is essential, such as in virtual reality, 3D modeling, and digital content creation. The method's ability to perform these tasks efficiently and with high fidelity opens up new possibilities for real-time 3D scene manipulation and interactive 3D content generation.

### 3.2.3 NeuralCage Learning
NeuralCage Learning represents a significant advancement in the field of 3D scene deformation, addressing the limitations of traditional cage-based methods that often require manual manipulation of cage vertices [7]. This manual process can be time-consuming and unintuitive, especially for complex shapes. NeuralCage Learning automates the deformation mapping between a source and target model, leveraging deep learning to infer the deformation parameters without the need for manual adjustments. By training on a large dataset of paired 3D models, NeuralCage Learning can generalize to a wide range of shapes and deformations, making it a versatile tool for 3D content creation and editing.

Our proposed method, CAGE-GS, integrates NeuralCage Learning with 3D Gaussian Splatting (3DGS) to create a cage-based 3DGS deformation framework [7]. This integration allows for the automatic geometric transfer from a target model to a 3DGS representation, ensuring that the deformation is both structurally coherent and visually consistent [5]. CAGE-GS leverages the structured deformation space provided by NeuralCage Learning to optimize the parameters of the 3D Gaussians, resulting in high-fidelity texture preservation and smooth surface transitions [7]. This approach not only simplifies the deformation process but also enhances the quality of the deformed 3D scenes, making it suitable for applications such as animation, virtual reality, and interactive 3D content creation [5].

To further enhance the usability and flexibility of CAGE-GS, we support multiple input formats, including texts, images, point clouds, meshes, and 3DGS models. This versatility allows users to specify deformations using intuitive inputs, such as text descriptions or image sketches, which are then translated into precise 3D deformations. Extensive experiments on both public datasets and self-collected scenes demonstrate that CAGE-GS outperforms existing methods in terms of deformation accuracy, texture preservation, and computational efficiency. The results highlight the potential of NeuralCage Learning in advancing the state of the art in 3D scene deformation and editing.

## 3.3 Uncertainty and Efficiency Improvements

### 3.3.1 Uncertainty Estimation with Spherical Harmonics
Uncertainty estimation in 3D Gaussian Splatting (3DGS) is a critical aspect that enhances the reliability and interpretability of 3D reconstructions [15]. Unlike traditional methods that often treat uncertainty as a scalar value, 3DGS leverages its explicit representation of 3D Gaussians to model uncertainty in a more nuanced manner [2]. By representing uncertainty using spherical harmonics, we can capture the view-dependent nature of uncertainty, which is essential for applications such as object completion and active view selection. This approach allows us to distinguish between well-defined and ambiguous regions of the scene from different viewpoints, providing a more comprehensive understanding of the reconstruction's reliability.

The use of spherical harmonics for uncertainty estimation in 3DGS builds upon the existing view-dependent color representation of Gaussians [2]. Each Gaussian in the 3DGS model is characterized by its position, scale, rotation, color, and opacity [3]. By extending this representation to include spherical harmonics coefficients for uncertainty, we can model how the uncertainty varies with the viewing angle. This is particularly useful in high-frequency regions where the gradient norm of the photometric loss might fail to provide accurate guidance. The spherical harmonics coefficients are optimized alongside the other Gaussian parameters during the training process, ensuring that the uncertainty estimates are consistent with the observed data and the underlying 3D structure.

In practice, this method of uncertainty estimation offers several advantages. First, it provides a natural way to integrate uncertainty into the 3DGS pipeline, making it easier to implement and maintain. Second, the view-dependent uncertainty metrics can be used to guide the refinement of the 3D model, helping to identify and correct under-reconstructed regions. Finally, the uncertainty estimates can be used to inform decision-making processes in downstream applications, such as robotic navigation and augmented reality, where understanding the confidence in the 3D reconstruction is crucial. Overall, the integration of spherical harmonics for uncertainty estimation in 3DGS represents a significant step forward in enhancing the robustness and interpretability of 3D scene representations [2].

### 3.3.2 Precomputation for Efficient Training
Precomputation for efficient training is a critical strategy in 3D Gaussian Splatting (3DGS) to manage the computational demands of large-scale scenes [10]. By precomputing certain aspects of the training process, particularly the neighborhood relationships among Gaussians, we can significantly reduce the computational overhead during the actual training phase. This approach involves freezing the positions of the Gaussians after an initial optimization step, allowing us to construct a static graph that represents the spatial relationships between the Gaussians. This static graph is then used to propagate features and perform neighborhood-based operations, such as feature aggregation and message passing, without the need for repeated graph construction.

The precomputation of neighborhood relationships is particularly beneficial in scenarios where the number of Gaussians is very large, such as in large-scale urban or natural environments. By precomputing these relationships, we eliminate the need for dynamic graph updates during training, which can be computationally expensive and time-consuming. This static graph can be efficiently stored and reused across multiple training iterations, leading to a substantial reduction in the overall training time. Moreover, this approach ensures that the spatial relationships between Gaussians remain consistent throughout the training process, which is crucial for maintaining the structural integrity of the 3D scene representation [15].

In addition to reducing computational costs, precomputation also helps in managing memory usage, especially in multi-GPU settings. By precomputing and storing the neighborhood relationships, we can distribute the computational load more evenly across multiple GPUs, ensuring that each GPU processes a balanced subset of the scene [16]. This balanced distribution not only accelerates the training process but also improves the scalability of 3DGS to handle increasingly complex and large-scale scenes. Overall, precomputation for efficient training is a key technique that enhances the practicality and efficiency of 3DGS, making it a viable solution for real-world applications involving large and intricate 3D environments.

### 3.3.3 Direct Initialization Strategy
The direct initialization strategy in 3D Gaussian Splatting (3DGS) fundamentally alters the traditional incremental refinement process by precomputing a dense set of 3D Gaussians from 2D correspondences across multiple input views [10]. This approach leverages the known viewing rays for each correspondence pixel and the camera poses to triangulate the initial positions of the Gaussians [10]. By doing so, it eliminates the need for the model to gradually fill in missing details, which is a common practice in the original 3DGS framework. This direct initialization not only accelerates the convergence of the optimization process but also guides the model toward a more accurate and stable convergence point.

The direct initialization strategy significantly reduces the path each Gaussian must travel in the parameter space during optimization. Traditional methods often start with a sparse set of Gaussians and incrementally densify the model as the optimization progresses, which can be computationally expensive and time-consuming. In contrast, the direct initialization strategy initializes a dense set of Gaussians from the outset, ensuring that the model has a comprehensive initial representation of the scene. This dense initialization allows the optimization process to focus on refining the positions and attributes of the Gaussians rather than discovering where to add new details. Consequently, the model converges faster and with higher accuracy, leading to more efficient and effective 3D scene reconstruction.

Moreover, the direct initialization strategy enhances the robustness of the 3DGS model, particularly in scenarios with noisy or sparse input data. By leveraging dense 2D correspondences, the method can better handle ambiguities and inconsistencies in the input views, which are common in real-world applications. This robust initialization helps the model to converge to a more globally consistent representation of the scene, reducing the risk of local minima and improving the overall quality of the reconstruction. The direct initialization strategy thus provides a more principled and efficient approach to 3D scene representation, making it a valuable advancement in the field of 3D Gaussian Splatting [10].

# 4 Advanced 3D Scene Representation and Inpainting

## 4.1 Coarse-to-Fine Optimization and Rendering Efficiency

### 4.1.1 Gaussian-Enhanced Surfels
Gaussian-Enhanced Surfels (GESs) represent a significant advancement in the field of 3D scene representation and novel view synthesis [14]. GESs combine the strengths of surfels and Gaussian primitives to achieve high-quality, real-time rendering [14]. Surfels, which are 2D points with associated surface properties, provide a coarse representation of the scene's geometry and appearance. This coarse representation is essential for capturing the overall structure and color distribution of the scene. However, surfels alone often lack the fine details necessary for photorealistic rendering, especially in regions with complex textures and high-frequency variations.

To address these limitations, GESs integrate 3D Gaussian primitives, which are capable of encoding both geometric and material properties at a finer scale. These Gaussian primitives are defined by a set of parameters, including position, scale, rotation, and opacity, allowing them to model fine details and smooth transitions in the scene [3]. The combination of surfels and Gaussians enables GESs to capture both the macro and micro structures of the scene, resulting in a more comprehensive and accurate representation. The Gaussian primitives are particularly effective in representing areas with sparse initial point clouds, where surfels alone might struggle to provide sufficient detail.

The construction of GESs involves a two-stage optimization process. In the first stage, surfels are optimized to create a geometrically accurate representation of the scene. This stage leverages normal maps and de-lighted images to ensure that the surfels accurately capture the surface normals and diffuse colors of the scene. Once the surfels are optimized, the second stage focuses on refining the Gaussian primitives. This stage involves joint optimization of both surfels and Gaussians, where the Gaussians are used to supplement the fine-scale details and enhance the overall visual quality. The coarse-to-fine approach ensures that the optimization process is efficient and robust, leading to a high-quality GES representation that can be used for real-time rendering and novel view synthesis [14].

### 4.1.2 Opacity Modulation and Hessian Pruning
Opacity modulation and Hessian pruning are critical techniques in enhancing the efficiency and visual fidelity of 3D Gaussian Splatting (3DGS) models [10]. Opacity modulation addresses the issue of local minima and visual artifacts by dynamically adjusting the transparency of Gaussian primitives. In traditional 3DGS, the abrupt changes in light conditions across different view directions can lead to color degradation and loss of detail. By introducing an opacity modulating parameter, the model can gradually evolve translucent surfels into opaque ones, allowing for smoother transitions and more accurate representation of surfaces [14]. When the opacity parameter is below 1, the surfel remains translucent with a Gaussian-distributed opacity, which helps in capturing subtle color variations and gradients [14]. As the parameter increases, the surfel becomes more opaque from the center outward, ensuring that the color-based gradients are effectively propagated during optimization [14].

Hessian pruning, on the other hand, focuses on reducing the computational overhead by identifying and removing redundant or less significant Gaussians. This technique leverages the Hessian matrix to evaluate the importance of each Gaussian in the overall scene representation. The Hessian matrix captures the second-order derivatives of the loss function with respect to the Gaussian parameters, providing insights into the curvature of the loss landscape. Gaussians with low Hessian scores are considered less influential and can be pruned without significant loss of visual quality. This pruning process not only reduces the number of Gaussians, leading to faster rendering times, but also helps in maintaining the essential details of the scene. By combining opacity modulation and Hessian pruning, 3DGS models can achieve a balance between computational efficiency and visual fidelity, making them suitable for real-time applications and large-scale scenes.

In practice, the integration of opacity modulation and Hessian pruning has been shown to significantly improve the performance of 3DGS models. Opacity modulation ensures that the model can handle complex lighting conditions and surface textures more effectively, while Hessian pruning optimizes the Gaussian distribution to enhance rendering speed and reduce memory usage. Together, these techniques address the challenges of multi-layer transparency and high-frequency surface textures, making 3DGS a robust and versatile representation for novel view synthesis and 3D reconstruction tasks. The combination of these methods not only improves the visual quality of the rendered scenes but also ensures that the models can be efficiently deployed in real-world applications, such as virtual reality, augmented reality, and real-time graphics.

### 4.1.3 Hash Grids for Fast Rendering
Hash grids have emerged as a powerful technique for encoding view-dependent features in fast rendering systems, particularly inspired by the advancements in Instant-NGP. By leveraging a hash grid, the system can efficiently store and retrieve directional information, which is crucial for accurate color decoding from various perspectives [3]. The hash grid encodes the view direction vector, replacing the traditional MLP input, thereby enhancing the precision of view-dependent color decoding [3]. This approach not only improves the accuracy of color representation but also addresses the challenge of smoothing gradients across rendered pixels, which can lead to better reconstruction of fine details and color variations in the scene [3].

Incorporating lighting conditions as a global attribute within the hash grid further enriches the representation, allowing for more nuanced and realistic rendering. The directional information stored in the anchor embeddings is integrated into the hash grid, ensuring that the system can dynamically adjust to changes in lighting and viewing angles. This method emphasizes color differences rather than just positional gradients, which is particularly beneficial in areas that are difficult to reconstruct, such as those with complex lighting or high-frequency color variations. By focusing on these aspects, the hash grid approach can significantly enhance the overall quality and realism of the rendered images.

Moreover, the use of hash grids in fast rendering systems facilitates efficient handling of large sets of semi-transparent primitives, a common challenge in real-time rendering. Traditional methods, such as rasterization and sequential ray tracing, often struggle with accurately and efficiently rendering multiple layers of transparent objects [17]. The hash grid method, combined with stochastic ray tracing, allows for a single traversal of the acceleration structure, accepting and shading only a single intersection per ray. This not only reduces computational overhead but also minimizes the introduction of visual artifacts, making it a robust solution for rendering complex scenes with high levels of transparency and dynamic lighting conditions.

## 4.2 Transparent and Reflective Object Handling

### 4.2.1 TransparentGS Framework
The TransparentGS framework is a significant advancement in the field of 3D Gaussian Splatting (3DGS), specifically designed to handle the challenges posed by transparent objects [18]. Traditional 3DGS methods, while effective for opaque and reflective surfaces, struggle with transparent materials due to their reliance on visual appearance optimization, which often leads to inaccuracies in geometric representation [18]. TransparentGS addresses this issue by introducing a novel approach that balances the conflicting goals of visual transparency and geometric accuracy. The framework achieves this by modifying the opacity of Gaussian primitives to better represent transparent surfaces while maintaining the integrity of the underlying 3D structure [18].

In TransparentGS, the key innovation lies in the stochastic ray tracing method, which efficiently renders large sets of semi-transparent primitives [17]. Unlike traditional methods that process all ray-Gaussian intersections sequentially, TransparentGS traverses the acceleration structure only once per ray, accepting and shading a single intersection [17]. This approach not only reduces computational complexity but also minimizes the visual artifacts that arise from approximations in multi-layer transparency. The framework further enhances this method by allowing the shading of multiple intersections within a single traversal, thereby capturing the complex interplay of light and transparency more accurately. This stochastic approach is particularly beneficial for dynamic scenes where real-time rendering is crucial.

Additionally, TransparentGS incorporates a dual-branch decomposition model that separates the scene into intrinsic and transient components. The intrinsic Gaussian primitives capture stable properties such as reflectance and illumination, while the transient Gaussians model unstable content like noise and color shifts. This decomposition allows for more robust and accurate representation of transparent objects, especially under varying lighting conditions. The framework also employs an opacity modulating parameter during optimization, which gradually evolves translucent surfels into opaque ones, ensuring that the geometry is optimized and stabilized over time [14]. This parameterized approach enables the framework to handle the nondifferentiable nature of surfel geometry, allowing for fine-tuned adjustments that improve the overall quality of the rendered scenes.

### 4.2.2 Reflective Gaussian Surfel with Deferred Rendering
Reflective Gaussian Surfel with Deferred Rendering (RGS-DR) represents a significant advancement in the rendering of complex, reflective surfaces by integrating the benefits of Gaussian splatting with deferred rendering techniques. In RGS-DR, each surfel is represented as a Gaussian primitive that encodes both geometric and material properties, including reflectance and transparency. This approach allows for the efficient and accurate representation of high-frequency surface details, which are crucial for realistic rendering of materials like metals and glass. The deferred rendering pipeline is particularly advantageous in this context, as it separates the rendering process into multiple passes, enabling the application of complex shading and lighting effects after the geometry has been rasterized. This separation mitigates issues related to depth precision and ensures that each pixel receives the correct shading based on its material properties.

The core of RGS-DR lies in its ability to handle the intricate interplay between light and surface properties, especially in scenarios involving multiple layers of semi-transparent primitives. By using anisotropic spherical Gaussian (ASG) components, RGS-DR can accurately model the anisotropic behavior of reflective surfaces, which is essential for achieving photorealistic results. The deferred rendering stage applies physically-based shading models to each pixel, taking into account factors such as Fresnel effects, microfacet distributions, and inter-reflections. This stage is crucial for capturing the subtle nuances of light interaction, which are often lost in traditional forward rendering pipelines. Moreover, the use of Gaussian light field probes (GaussProbes) further enhances the realism by capturing the local light field around transparent objects, thereby providing accurate lighting information for secondary ray effects such as shadows and reflections [19].

Despite the computational challenges associated with rendering large sets of semi-transparent primitives, RGS-DR offers a practical solution by leveraging the efficiency of the deferred rendering pipeline and the compact representation of Gaussian primitives. The method's ability to balance accuracy and performance makes it suitable for real-time applications, such as virtual reality and augmented reality, where high-fidelity rendering is essential. Additionally, the framework's modular design allows for easy integration with existing rendering engines, facilitating its adoption in both research and industry settings. By addressing the limitations of previous methods in handling complex lighting and material properties, RGS-DR sets a new standard for the rendering of reflective and transparent surfaces in 3D scenes [19].

### 4.2.3 Stochastic Ray Tracing for Transparency
Stochastic ray tracing for transparency addresses the challenges of efficiently and accurately rendering 3D clouds of transparent primitives [17]. Traditional methods, such as those based on α-blending, often suffer from issues like color degradation and loss of detail, especially in areas with sharp light condition changes. These methods typically derive depth maps by averaging the depths of Gaussians along the ray, weighted by their appearance-optimized weights. However, this blending inherently mixes contributions from the first surface and transmitted background details, leading to artifacts and noise. To overcome these limitations, a stochastic approach has been proposed, where each ray traverses the acceleration structure only once, accepting and shading just a single intersection. This method not only simplifies the rendering process but also enhances the efficiency and quality of the rendered output.

The stochastic ray tracing method for transparency leverages a variance-guided densification technique to identify and refine areas that require more detailed representation. By analyzing the variance of color gradients, this technique pinpoints regions with high color variation but low gradient variation in position, indicating areas where the Gaussian primitives need to be denser [3]. This approach ensures that the model can accurately capture the intricate details of transparent objects, such as their surface reflections and refractions, without getting stuck in local minima or producing blurred and needle-shaped artifacts. The method's ability to handle sharp changes in light conditions across different view directions is particularly significant, as it maintains the visual fidelity of the rendered scenes.

Furthermore, the stochastic ray tracing method can be extended to shade multiple intersections within a single traversal, providing a balance between computational efficiency and rendering quality [17]. This extension is particularly beneficial for practical scenes, where a low-noise estimator is often more advantageous than a slow noise-free one. The simplicity and effectiveness of this approach have enabled its integration into commercial Monte Carlo renderers, facilitating seamless rendering of 3D Gaussians alongside conventional 3D assets [17]. By addressing the transparency-depth dilemma and improving the handling of lighting effects such as shadows, reflections, and global illumination, stochastic ray tracing for transparency represents a significant advancement in the field of 3D rendering and novel view synthesis.

## 4.3 High-Frequency Texture and Material Representation

### 4.3.1 3D Gabor Splatting
3D Gabor Splatting (3DGS) builds upon the foundational concept of 3D Gaussian Splatting by introducing the Gabor kernel to enhance the color variation and detail in the reconstructed 3D scenes [3]. The Gabor kernel, a combination of a Gaussian function and a sinusoidal wave, is well-known for its ability to capture high-frequency textures and patterns, making it particularly suitable for representing detailed and intricate surface properties. By integrating the Gabor kernel into the 3D Gaussian primitives, 3D Gabor Splatting can efficiently model high-frequency color variations, which are essential for realistic rendering of materials such as textiles and other surfaces with fine textures [20].

In traditional 3D Gaussian Splatting, each Gaussian primitive is defined by a set of parameters including position, scale, rotation, opacity, and color [3]. These primitives are used to approximate the 3D scene, and their parameters are optimized to minimize the difference between the rendered and observed images. However, this approach often struggles to capture high-frequency color variations, leading to a loss of detail in the final render. 3D Gabor Splatting addresses this limitation by augmenting each Gaussian primitive with a Gabor kernel, allowing each primitive to represent a wider range of colors and textures [20]. This augmentation is achieved by modifying the color component of the Gaussian primitive to include a sinusoidal term, which introduces high-frequency variations in the color space.

The implementation of 3D Gabor Splatting involves a deferred rendering pipeline, where the properties of the Gabor-enhanced Gaussian primitives are rasterized to generate a material property buffer in image space. This buffer is then used to apply shading functions to each pixel, determining the final color of the rendered image. The use of a deferred rendering approach helps mitigate the impact of depth precision on rendering and ensures that the high-frequency color variations are accurately represented. Additionally, the method leverages standard triangle-mesh acceleration structures for efficient Gaussian ray tracing, making it suitable for real-time rendering applications [17]. The resulting 3D Gabor Splatting method not only improves the visual fidelity of the rendered scenes but also maintains the computational efficiency required for practical applications in computer graphics and vision [20].

### 4.3.2 Spatially-Varying Gaussian Representation
Spatially-Varying Gaussian Representation (SVG) is a significant advancement in 3D scene representation, extending the capabilities of traditional Constant Gaussian models [21]. Unlike Constant Gaussians, which assume uniform material and normal properties across the entire primitive, SVGs allow for these properties to vary spatially within a single Gaussian [21]. This flexibility is crucial for accurately modeling complex materials and surfaces, such as those found in natural scenes with varying reflectance properties. Each SVG can have different BRDF lobes at various points, enabling a more nuanced and realistic representation of surfaces. For instance, a single SVG can capture the smooth, glossy finish of a car's paint alongside the rough, matte texture of its tires, all within the same primitive.

The implementation of SVGs involves defining a set of parameters that control the spatial variation of material properties. These parameters can include normal vectors, albedo, and specular coefficients, which are interpolated across the Gaussian primitive based on predefined rules or learned from data. The interpolation ensures that the transition between different material properties is smooth and physically plausible, avoiding abrupt changes that could lead to visual artifacts. The use of SVGs in 3D scene representation not only enhances the visual fidelity of the rendered scenes but also improves the efficiency of the rendering process by reducing the number of primitives needed to achieve high detail. This is particularly advantageous in real-time applications where computational resources are limited.

Moreover, the integration of SVGs into existing 3D rendering pipelines is facilitated by their compatibility with deferred rendering techniques. In a deferred rendering setup, the properties of SVGs are rasterized into a material property buffer, which is then used to apply shading functions to each pixel. This approach allows for the precise control of lighting and shading, even in complex scenes with multiple light sources and dynamic environments. The ability to handle spatially-varying material properties within a single primitive also simplifies the management of large 3D scenes, making it easier to perform tasks such as object removal and scene editing. Overall, the Spatially-Varying Gaussian Representation offers a powerful tool for achieving high-fidelity 3D scene reconstruction and novel view synthesis, bridging the gap between theoretical models and practical applications [12].

### 4.3.3 Variance-Guided Densification
Variance-guided densification is a technique designed to enhance the quality of 3D Gaussian splatting (3DGS) reconstructions by strategically increasing the density of Gaussians in areas that require more detail [22]. This method leverages the variance of color gradients to identify regions where the initial point cloud is sparse, and thus, the representation is insufficient. By focusing on areas with high color variation but low gradient variation in position, the technique can pinpoint Gaussians that need to be densified, ensuring that these regions are better represented in the final model. This approach is particularly effective in addressing the shortcomings of traditional methods that rely solely on position gradients, which often fail to capture the fine details and variations in color that are crucial for high-quality reconstructions [3].

The key advantage of variance-guided densification lies in its ability to emphasize color differences rather than just position gradients. This is particularly important in scenes with complex textures and thin structures, where the failure to adequately represent these details can lead to noisy and unreliable depth estimations. By analyzing the variance of color gradients, the technique can identify and densify areas that are critical for the overall quality of the reconstruction. This not only improves the visual fidelity of the rendered images but also enhances the robustness of the model, making it more suitable for applications that require high accuracy, such as virtual reality and augmented reality. Moreover, the method's emphasis on color differences ensures that the model can better capture the nuances of the scene, leading to more coherent and realistic reconstructions.

To implement variance-guided densification, a maximum-weight window aggregation algorithm is used to analyze the color gradients in the scene [3]. This algorithm efficiently identifies the regions with high color variance and low position gradient variation, allowing for targeted densification of the Gaussian primitives [3]. The process is iterative, with each iteration refining the density of the Gaussians to better match the underlying structure of the scene. This iterative refinement ensures that the model converges to a high-quality representation, even in challenging scenarios with complex geometries and textures. The result is a more accurate and detailed 3D reconstruction that can be seamlessly integrated into various rendering pipelines, including those used in commercial Monte Carlo renderers. The simplicity and effectiveness of this approach make it a valuable addition to the toolkit of 3D reconstruction and novel view synthesis techniques.

# 5 3D Gaussian Splatting for Dynamic and Interactive Scenes

## 5.1 Single-Image and Video-Based Avatars

### 5.1.1 SEGA for 3D Face Avatars
SEGA, a pioneering method for single-image-based 3D face avatar creation, addresses the limitations of existing approaches by integrating 2D and 3D priors to achieve high-quality, drivable avatars [23]. The method leverages a pre-trained network on large-scale 2D face datasets to encode identity information from a single input image [23]. This 2D prior extraction ensures that the avatar retains the unique facial features and characteristics of the input image, even when the input is a single, potentially low-resolution image.

To ensure multi-view and expression consistency, SEGA incorporates 3D priors during the training process. These 3D priors help maintain the structural integrity and realism of the avatar across different expressions and viewpoints. By disentangling identity and expression information, SEGA can generate avatars that not only look like the input subject but also animate naturally and consistently in 3D space. This combination of 2D and 3D priors allows SEGA to generalize well to unseen identities and expressions, making it a versatile tool for real-world applications.

Extensive experiments have demonstrated that SEGA outperforms existing methods in terms of visual fidelity, generalization, and computational efficiency. The method's ability to create high-quality avatars from a single image opens up new possibilities for applications in virtual reality (VR), augmented reality (AR), and digital entertainment, where rapid and personalized avatar creation is crucial [24]. SEGA's robustness and efficiency make it a promising solution for integrating realistic 3D face avatars into interactive environments, enhancing user engagement and immersion [23].

### 5.1.2 Monocular Video-Based Avatar Creation
Monocular video-based avatar creation represents a significant advancement in the field of 3D reconstruction and novel view synthesis, particularly in scenarios where multi-view data is not readily available [23]. Unlike traditional methods that rely on multiple cameras or calibrated setups, monocular video-based approaches leverage a single video stream to reconstruct detailed 3D models of humans and their environments. This capability is crucial for applications such as virtual reality (VR), augmented reality (AR), and telepresence, where the ability to create realistic avatars from everyday video recordings can greatly enhance user experience and interaction [24].

One of the key challenges in monocular video-based avatar creation is the accurate estimation of camera poses and human poses, as well as the dense reconstruction of the scene. Recent methods have addressed this by integrating 3D Gaussian Splatting (3DGS) with continuous-time trajectory optimization [10]. By representing camera motion as a continuous function, these methods can jointly optimize camera poses, exposure times, and the 3D scene representation, leading to more accurate and consistent reconstructions. Additionally, the use of a camera response function (CRF) module allows for the conversion of accumulated irradiance into high dynamic range (HDR) images, further enhancing the quality of the reconstructed scenes [25].

Another notable advancement is the ability to handle dynamic scenes and fast-moving objects, which are common in real-world videos. Techniques such as motion blur loss and event-based data fusion have been introduced to improve the robustness and accuracy of the reconstruction process. These methods enable the creation of photorealistic avatars and scenes from monocular videos, even in challenging conditions such as low light and high-speed motion [23]. The resulting systems are not only faster and more efficient than previous approaches but also capable of real-time processing, making them suitable for online applications and interactive environments.

### 5.1.3 InteractAvatar for Hand-Face Interactions
InteractAvatar represents a significant advancement in the modeling of hand-face interactions, addressing the limitations of previous methods that often relied on pre-scanned templates or were constrained to single-hand interactions. This method introduces a novel approach to capturing the dynamic and subtle changes in facial geometry and appearance that occur during hand-face interactions. By modeling these interactions with high fidelity, InteractAvatar enables more realistic and immersive digital experiences, particularly in applications such as virtual reality (VR), augmented reality (AR), and telepresence.

The core of InteractAvatar's innovation lies in its photorealistic hand-face interaction module, which goes beyond the simple mesh recovery of previous works. This module captures fine-grained changes in facial geometry, such as skin deformations and shadows, that result from hand contact [26]. The module achieves this by integrating detailed physical constraints and real-time rendering techniques, ensuring that the interactions are not only visually accurate but also temporally consistent. This level of detail is crucial for applications where the realism of the avatar is paramount, such as in social VR environments where nonverbal communication cues play a critical role in user engagement and interaction [26].

To achieve these results, InteractAvatar employs a sophisticated optimization framework that jointly tracks camera poses and human poses, while also constructing a consistent dense scene map with both geometric and appearance information [27]. The method uses 3D Gaussians in canonical space, guided by SMPL-based deformations, to model the human avatar [27]. This approach allows for the decomposition of deformations into rigid and non-rigid parts, effectively handling the dynamic nature of hand-face interactions. Through extensive experiments, InteractAvatar has demonstrated superior performance in both novel view synthesis and self-reenactment tasks, setting a new standard for the realistic modeling of hand-face interactions in digital avatars [26].

## 5.2 Motion Deblurring and Real-Time SLAM

### 5.2.1 MoBGS for Motion Deblurring
In the context of motion deblurring, the integration of 3D Gaussian Splatting (3DGS) into the deblurring process, as proposed in the MoBGS method, represents a significant advancement [15]. MoBGS addresses the inherent challenges of motion blur and varying image brightness by incorporating a unified differentiable rendering framework that models the physical imaging process, including camera motion and exposure time [25]. This approach allows for the joint optimization of the camera trajectory, color response function (CRF), and exposure time, effectively decoupling the effects of camera motion and irradiance accumulation [25].

The key innovation in MoBGS lies in its ability to handle both global camera motion and local object motion within a monocular video setup. By accurately predicting latent camera poses and estimating exposure timestamps, MoBGS can simulate the physical formation of motion blur and correct it during the reconstruction process [28]. This is achieved through a Latent Camera-induced Exposure Estimation (LCEE) method, which ensures that the deblurring process is consistent across the entire image, leading to sharper and more realistic reconstructions. The LCEE method is particularly effective in scenarios where the camera and objects are in motion, as it aligns the motion trajectories and exposure times to produce a coherent and deblurred image.

Empirical evaluations on the Stereo Blur dataset and real-world video sequences have demonstrated the superiority of MoBGS over existing state-of-the-art methods in terms of deblurring quality and novel view synthesis (NVS) [28]. The method's ability to handle diverse and complex motion patterns, coupled with its efficiency in processing monocular videos, makes it a promising solution for applications requiring high-quality, real-time deblurring and NVS [28]. The robustness and accuracy of MoBGS in these evaluations underscore its potential for practical deployment in various domains, including virtual reality, augmented reality, and autonomous systems.

### 5.2.2 EBAD-Gaussian for Event-Driven Deblurring
The EBAD-Gaussian method is a novel approach designed to address the challenges of event-driven deblurring in dynamic scenes. This method leverages the high dynamic range and temporal resolution of event cameras to complement traditional RGB data, thereby enhancing the reconstruction of scenes with complex motion and varying illumination. By fusing event streams and RGB data, EBAD-Gaussian can effectively model the cumulative effect of relative camera and object motion over the exposure time. The method optimizes Gaussian parameters and camera poses simultaneously, ensuring that the reconstructed 3D scene is both accurate and free from motion blur [29].

To achieve this, EBAD-Gaussian introduces a motion blur loss function that penalizes inconsistencies between the reconstructed scene and the observed events [29]. This loss function is designed to capture the temporal and spatial characteristics of the event data, which are particularly sensitive to motion. By integrating this loss with the traditional photometric loss used for RGB data, the method can jointly optimize the Gaussian parameters and camera poses in a way that is robust to both camera and object motion. This joint optimization is crucial for handling dynamic scenes where both the camera and objects are moving, a scenario that is often challenging for methods that only consider static or camera-induced motion.

The effectiveness of EBAD-Gaussian is demonstrated through extensive experiments on both synthetic and real datasets. The results show that the method can achieve high-fidelity real-time reconstruction under complex motion and low-light conditions, outperforming existing state-of-the-art deblurring techniques. The use of event data not only helps in reducing motion blur but also improves the reconstruction of underexposed regions, making EBAD-Gaussian a versatile solution for event-driven deblurring in a wide range of applications, from robotics to augmented reality [29].

### 5.2.3 ODHSR for Real-Time SLAM
ODHSR (Online Dense Human Scene Reconstruction) represents a significant advancement in the field of real-time Simultaneous Localization and Mapping (SLAM) by integrating camera tracking, human pose estimation, and dense photorealistic reconstruction into a unified framework [27]. Unlike previous methods that often require multi-view images or specialized hardware, ODHSR operates solely on monocular RGB video, making it highly accessible and practical for consumer-grade devices. The system leverages a continuous-time trajectory model to represent camera motion, allowing for joint optimization of camera poses and exposure times, which is crucial for handling dynamic scenes and ensuring geometric consistency in the reconstructed models [25].

One of the key contributions of ODHSR is its ability to achieve real-time performance while maintaining high reconstruction quality. This is accomplished through an efficient online algorithm that processes video frames incrementally, thereby reducing the computational overhead associated with batch processing. The system's robustness is further enhanced by its capability to handle challenging in-the-wild sequences, as demonstrated through extensive evaluations on datasets like Neuman and EMDB. These datasets feature complex scenes with significant motion blur and varying lighting conditions, which are common in real-world applications. ODHSR's performance in these scenarios underscores its potential for deployment in VR/AR environments, autonomous systems, and other real-time applications.

To address the limitations of traditional 3D Gaussian Splatting (3DGS) methods, particularly in low-light and high-speed scenarios, ODHSR introduces a novel framework that fuses event streams with RGB data [29]. This approach, known as EBAD-Gaussian, compensates for high-frequency details in underexposed regions by leveraging the high dynamic range characteristics of event streams. The motion blur loss incorporated in the framework helps in refining camera poses and Gaussian parameters, leading to more accurate and detailed reconstructions [29]. By optimizing these parameters within the exposure time, ODHSR achieves superior results in global human pose estimation and camera tracking, setting a new benchmark for real-time SLAM systems [27].

## 5.3 Manipulation and Interaction Scenarios

### 5.3.1 RoboSplat for Visuomotor Policies
RoboSplat represents a significant advancement in the development of visuomotor policies for robotic manipulation, addressing the critical issue of data scarcity and domain shift between training and deployment environments [22]. Unlike traditional methods that rely heavily on expert demonstrations, RoboSplat leverages 3D Gaussian Splatting (3DGS) to augment the training data, thereby enhancing the robustness of the learned policies. By segmenting 3D Gaussians of different scene components using off-the-shelf segmentation models and the robot's URDF, RoboSplat can generate a diverse set of training data from a single expert trajectory [22]. This approach not only increases the volume of training data but also ensures that the data is spatially accurate and representative of the real-world conditions in which the robot will operate.

The core innovation of RoboSplat lies in its ability to simulate a wide range of visual domains, including variations in lighting, texture, and object configurations, which are crucial for the robustness of visuomotor policies [22]. This is achieved through the strategic manipulation of 3D Gaussians, allowing the system to generate novel demonstrations that cover a broad spectrum of possible scenarios. For instance, RoboSplat can simulate different lighting conditions, occlusions, and object deformations, thereby preparing the policy to handle unexpected variations during deployment. This is particularly important in tasks involving pick and place, tool use, and articulated object manipulation, where the environment can be highly dynamic and unpredictable.

Extensive real-world experiments have demonstrated the effectiveness of RoboSplat in enhancing the robustness of visuomotor policies. The method has been tested across a variety of tasks, including those with significant disturbances and varying environmental conditions. The results show that policies trained with RoboSplat exhibit superior performance and adaptability compared to those trained with conventional data augmentation techniques. This improvement is attributed to the high fidelity and diversity of the generated data, which better prepares the policy for the complexities of real-world interactions. Overall, RoboSplat represents a significant step forward in the development of robust and adaptable visuomotor policies for robotic manipulation.

### 5.3.2 BIGS for Bimanual Interaction
In the realm of 3D reconstruction, the challenge of accurately capturing bimanual interactions, particularly those involving an unknown object, has garnered significant attention. Traditional methods often struggle with the complexity and variability of such interactions, leading to inaccuracies in the reconstructed 3D models. To address these limitations, the Bimanual Interaction 3D Gaussian Splatting (BIGS) pipeline was introduced [30]. BIGS leverages 3D Gaussian splatting to build detailed 3D models of both hands and an unknown object from monocular video inputs [30]. This approach is particularly advantageous in scenarios where the object's category is unknown, as it does not rely on pre-defined object models or categories.

The BIGS pipeline is structured into two main optimization stages to ensure robust and accurate reconstruction. In the first stage, the pipeline optimizes the 3D Gaussians for the hands and the object separately. This step is crucial for establishing reliable initial 3D shapes, as it allows the algorithm to focus on the distinct features and movements of each component without interference. Once the initial shapes are established, the second stage involves joint optimization of the hand and object Gaussians. This ensures that the interactions between the hands and the object are accurately represented, capturing the fine details and dynamics of the interaction. The separation of these stages helps mitigate the complexities associated with simultaneous optimization, leading to more precise and realistic 3D reconstructions.

By integrating these optimization stages, BIGS demonstrates superior performance in reconstructing bimanual interactions compared to existing methods. The use of 3D Gaussian splatting enables the pipeline to handle the high variability and dynamic nature of hand-object interactions, making it a powerful tool for applications such as virtual reality, robotics, and human-computer interaction [30]. The ability to reconstruct these interactions from monocular videos also makes BIGS highly versatile, as it reduces the need for specialized equipment and complex setup procedures. Overall, BIGS represents a significant advancement in the field of 3D reconstruction, offering a robust solution for category-agnostic bimanual interaction modeling.

### 5.3.3 CasualHDRSplat for 3D HDR Reconstruction
CasualHDRSplat is a pioneering one-stage method that addresses the challenges of 3D HDR reconstruction by integrating the physical imaging model with camera motion, exposure time, and camera response function (CRF) in a unified framework [25]. This method leverages the continuous-time camera trajectory on the special Euclidean group SE(3) to model camera movement accurately. By jointly optimizing these parameters, CasualHDRSplat enhances the robustness and flexibility of the reconstruction process, enabling it to handle dynamic scenes with varying lighting conditions and complex camera motions [25]. The integration of the physical imaging model ensures that the reconstructed 3D HDR scenes are not only visually appealing but also physically plausible, which is crucial for downstream applications such as virtual reality and augmented reality.

The key innovation of CasualHDRSplat lies in its ability to handle the inherent challenges of reconstructing 3D HDR scenes from consumer-grade cameras, which often use autoexposure during video recording. This autoexposure feature dynamically adjusts the exposure time based on ambient lighting, expanding the dynamic range of the captured video. However, this also introduces inconsistencies in the exposure across frames, making it difficult to apply traditional 3D reconstruction methods. CasualHDRSplat overcomes this by explicitly modeling the exposure time and CRF in its optimization process, allowing it to accurately reconstruct the HDR content while maintaining temporal consistency. This is particularly important for scenes with rapid changes in lighting, where the exposure adjustments can lead to significant variations in the captured images.

Furthermore, CasualHDRSplat employs a 3D Gaussian Splatting (3DGS) representation to achieve efficient and high-fidelity 3D reconstruction [7]. The 3DGS model represents the scene using a set of Gaussian distributions, which can be optimized directly and efficiently [31]. This explicit representation not only accelerates the reconstruction process but also facilitates the handling of complex scenes with intricate details. By combining the 3DGS model with the unified imaging model, CasualHDRSplat can reconstruct detailed 3D HDR scenes from monocular RGB videos, even under challenging conditions such as fast camera motion and low light [25]. The method's ability to handle these scenarios makes it a significant advancement in the field of 3D HDR reconstruction, paving the way for more practical and versatile applications.

# 6 Future Directions


Despite the significant advancements in 3D Gaussian Splatting (3DGS) techniques, several limitations and gaps remain. One major limitation is the handling of dynamic scenes with complex lighting and occlusions, where current methods often struggle to maintain high-fidelity reconstructions and real-time performance. Additionally, the integration of semantic information into 3DGS models is still in its early stages, with many methods failing to accurately capture and propagate semantic labels across different views and scales. Another gap is the robustness of 3DGS models in the presence of noise and outliers, which can significantly degrade the quality of the reconstructions. Furthermore, the computational efficiency of 3DGS, particularly for large-scale scenes, remains a challenge, as the optimization and rendering processes can be computationally intensive.

To address these limitations, several directions for future research are proposed. First, developing advanced techniques for dynamic scene reconstruction is crucial. This could involve integrating physics-based models to better handle lighting and occlusions, as well as incorporating temporal consistency constraints to ensure smooth and coherent reconstructions over time. Additionally, exploring the use of machine learning, particularly deep learning, to predict and correct for dynamic changes in the scene could enhance the robustness and accuracy of 3DGS models.

Second, enhancing the integration of semantic information into 3DGS is another important area for future research. This could be achieved by developing more sophisticated semantic optimization algorithms that can effectively propagate and refine semantic labels across different views and scales. Additionally, incorporating multi-modal data, such as LiDAR and thermal imagery, could provide richer semantic information and improve the overall quality of the reconstructions. Furthermore, exploring the use of unsupervised and weakly supervised learning methods to automatically infer semantic labels from the data could reduce the reliance on annotated datasets and make 3DGS more scalable and practical.

Third, improving the robustness of 3DGS models in the presence of noise and outliers is essential. This could involve developing more advanced outlier detection and rejection techniques, as well as robust optimization algorithms that can handle noisy and incomplete data. Additionally, incorporating uncertainty estimation into the 3DGS pipeline could help in identifying and mitigating the impact of uncertain regions, leading to more reliable and accurate reconstructions.

Finally, enhancing the computational efficiency of 3DGS is crucial for real-time applications and large-scale scenes. This could be achieved by developing more efficient optimization algorithms, such as parallel and distributed computing methods, to speed up the training and rendering processes. Additionally, exploring hardware acceleration, such as GPU and FPGA implementations, could significantly reduce the computational overhead and make 3DGS more practical for real-world applications.

The potential impact of the proposed future work is significant. Advancements in dynamic scene reconstruction and semantic integration could revolutionize applications in virtual and augmented reality, enabling more realistic and interactive 3D environments. Improved robustness and computational efficiency would make 3DGS more accessible and practical for a wide range of applications, from autonomous systems and robotics to digital content creation and telepresence. Overall, these research directions have the potential to push the boundaries of 3D scene reconstruction and novel view synthesis, opening up new possibilities for innovation and practical deployment.

# 7 Conclusion



In this survey, we have comprehensively explored the recent advancements in 3D Gaussian Splatting (3DGS) techniques, focusing on areas such as semantic optimization, feature propagation, deformation, uncertainty estimation, and efficiency improvements. Key techniques discussed include Decoupled Semantic Optimization (DSO), Contextual Feature Propagation (CFP), and Mask-Aware Contrastive Learning (MA-CL), which significantly enhance the accuracy and robustness of 3D scene representations. The paper also delved into deformation and transformation techniques, such as cage-based deformation, ARAP deformation with a diffusion prior, and NeuralCage Learning, which enable intuitive and high-quality manipulations of 3D scenes. Additionally, advancements in uncertainty and efficiency, including uncertainty estimation with spherical harmonics, precomputation for efficient training, and direct initialization strategies, were highlighted for their roles in improving the reliability and practicality of 3DGS models. The survey further explored advanced 3D scene representation and inpainting techniques, such as Gaussian-Enhanced Surfels, opacity modulation, and hash grids for fast rendering, which address high-frequency texture representation and real-time rendering challenges.

The significance of this survey lies in its comprehensive overview and synthesis of the latest research in 3D Gaussian Splatting. By consolidating insights from a wide range of studies, this paper provides a structured and detailed understanding of the current state of the art in 3DGS techniques. The practical implications of these advancements are substantial, as they enhance the accuracy, efficiency, and robustness of 3D scene reconstruction and novel view synthesis, making them highly valuable for applications in virtual reality, augmented reality, and 3D content creation. The survey also identifies key challenges and future research directions, serving as a valuable resource for researchers and practitioners in the field. It highlights the ongoing need for innovation in handling complex scenes, dynamic environments, and real-time applications, thereby inspiring further advancements and explorations in 3D Gaussian Splatting.

In conclusion, this survey aims to inspire further research and development in 3D Gaussian Splatting. The techniques and methodologies discussed offer a solid foundation for future innovations, and the identified challenges provide clear directions for ongoing and future work. We encourage researchers and practitioners to build upon these advancements, pushing the boundaries of 3D scene reconstruction and novel view synthesis. By continuing to refine and expand these techniques, the potential for creating more realistic, interactive, and efficient 3D content will be significantly enhanced, driving the field forward and opening up new possibilities in a variety of applications.

# References
[1] You Need a Transition Plane  Bridging Continuous Panoramic 3D  Reconstruction with Perspective Gauss  
[2] View-Dependent Uncertainty Estimation of 3D Gaussian Splatting  
[3] Metamon-GS  Enhancing Representability with Variance-Guided  Densification and Light Encoding  
[4] Training-Free Hierarchical Scene Understanding for Gaussian Splatting  with Superpoint Graphs  
[5] ARAP-GS  Drag-driven As-Rigid-As-Possible 3D Gaussian Splatting Editing  with Diffusion Prior  
[6] GSFF-SLAM  3D Semantic Gaussian Splatting SLAM via Feature Field  
[7] CAGE-GS  High-fidelity Cage Based 3D Gaussian Splatting Deformation  
[8] Visibility-Uncertainty-guided 3D Gaussian Inpainting via Scene  Conceptional Learning  
[9] DropoutGS  Dropping Out Gaussians for Better Sparse-view Rendering  
[10] EDGS  Eliminating Densification for Efficient Convergence of 3DGS  
[11] CAGS  Open-Vocabulary 3D Scene Understanding with Context-Aware Gaussian  Splatting  
[12] VGNC  Reducing the Overfitting of Sparse-view 3DGS via Validation-guided  Gaussian Number Control  
[13] HUG  Hierarchical Urban Gaussian Splatting with Block-Based  Reconstruction  
[14] When Gaussian Meets Surfel  Ultra-fast High-fidelity Radiance Field  Rendering  
[15] AAA-Gaussians  Anti-Aliased and Artifact-Free 3D Gaussian Rendering  
[16] BlockGaussian  Efficient Large-Scale Scene Novel View Synthesis via  Adaptive Block-Based Gaussian S  
[17] Stochastic Ray Tracing of 3D Transparent Gaussians  
[18] TSGS  Improving Gaussian Splatting for Transparent Surface  Reconstruction via Normal and De-lightin  
[19] TransparentGS  Fast Inverse Rendering of Transparent Objects with  Gaussians  
[20] 3D Gabor Splatting  Reconstruction of High-frequency Surface Texture  using Gabor Noise  
[21] SVG-IR  Spatially-Varying Gaussian Splatting for Inverse Rendering  
[22] Novel Demonstration Generation with Gaussian Splatting Enables Robust  One-Shot Manipulation  
[23] SEGA  Drivable 3D Gaussian Head Avatar from a Single Image  
[24] GSAC  Leveraging Gaussian Splatting for Photorealistic Avatar Creation  with Unity Integration  
[25] CasualHDRSplat  Robust High Dynamic Range 3D Gaussian Splatting from  Casually Captured Videos  
[26] InteractAvatar  Modeling Hand-Face Interaction in Photorealistic Avatars  with Deformable Gaussians  
[27] ODHSR  Online Dense 3D Reconstruction of Humans and Scenes from  Monocular Videos  
[28] MoBGS  Motion Deblurring Dynamic 3D Gaussian Splatting for Blurry  Monocular Video  
[29] EBAD-Gaussian  Event-driven Bundle Adjusted Deblur Gaussian Splatting  
[30] BIGS  Bimanual Category-agnostic Interaction Reconstruction from  Monocular Videos via 3D Gaussian S  
[31] Volume Encoding Gaussians  Transfer Function-Agnostic 3D Gaussians for  Volume Rendering  